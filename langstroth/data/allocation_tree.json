[
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 400.0,
                                    "instanceQuota": 400.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "CoEPP-Tier3",
                                    "usagePatterns": "We expect that well over half the instance allocation will be in continuous use. At this stage we anticipate that the majority of our storage needs will be provided via the RDSI ReDS allocations we have received. ",
                                    "useCase": "We have successfully completed the first phase of our project to build our NeCTAR-funded eResearch Tool \"High throughput computing for globally connected science\". We have successfully integrated grid and cloud resources and have used these to enhance our Tier-2 facility (connected to the World Large Hadron Collider Computing Grid, WLCG) as well as our local cluster for data analysis and theoretical calculations (Tier-3). We require our initial allocation of 400 cores to meet the needs of the ATLAS experiment at Large Hadron Collider and for the baseline Tier-3 for the 100 Physicists involved in the ARC Center of Excellence for Paricle Physics at the Terascale (CoEPP). Since our initial deployment of cloud resources we have seen the use of our Tier-3 grow to over 400% of our previous (physical) configuration. The use of the Tier-3 continues its strong growth with demand from both Physics experimentals and theorists within the CoEPP.  We need an additional 100 cores satisfy this. If demand growth continues we will make further requests once the INTERSECT node comes online. We also need to satisfy the needs of a second international experiment, Belle II, based in Japan which also employs the eResearch Tools developed in RT07.  The initial deployment of 28 cores using dynamic Torque has performed extremely well. We now wish to move to full production and another 200 cores are required for this. To summerize, we intend to make use of the tools developed by RT07 and go into full production. The additional 300 cores of this request will enable this."
                                },
                                {
                                    "coreQuota": 400.0,
                                    "instanceQuota": 400.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CoEPP-Tier3",
                                    "usagePatterns": "Initally we expect a small number of users with large data sets access over the network filesystem.",
                                    "useCase": "This will be used to implement of the first phase of our project to build our NeCTAR-funded eResearch Tool \"High throughput computing for globally connected science\". As such the NeCTAR cloud will be employed to provide worker nodes to PBS-queue. The worker nodes will use cvmfs and secure NFS V.4 to mount network file systems for scientific software and user home directories respectively. Other network filesystems may also be investigated."
                                }
                            ],
                            "name": "020203"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "csiro.au",
                                    "name": "CloudTest",
                                    "usagePatterns": "1 user. Small data sets.",
                                    "useCase": "We are currently investigating the possibly of running large simulation models in the Cloud. We are doing a bit of benchmarking to decide which way we may go."
                                }
                            ],
                            "name": "020299"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "griffith.edu.au",
                                    "name": "HPC Workflows",
                                    "usagePatterns": "It's a HPC workflows Project so it's more likely to have large data sets, and at least initially, a small number of users.",
                                    "useCase": "We (eResearch Services, Griffith Uni) are currently developing a user-friendly HPC job submission web portal for researchers. Currently we have NAMD (and shortly MatLab) jobs running from a web portal through NIMROD to our local cluster. Nimrod is able to use cloud bursting and we would like to have more than 2 CPU's available to test and improve this workflow."
                                }
                            ],
                            "name": "0202"
                        }
                    ],
                    "name": "0202"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Astronomy Data Visualization",
                                    "usagePatterns": "Initially there will be four sites involved, with individual users at each site.  After establishing the service, I will be engaging research groups at the sites to test the service.  It is expected that there will be users in the 10s, using the services for several hours a week each, with extremely large datasets (starting with many gigabytes but hopefully testing terabyte datasets as well).",
                                    "useCase": "My masters thesis is about using cloud VMs to enable remote access to HPC facilities for astronomy research groups.  I plan to establish an XXL portal service which will provide both access to facilities as well as distribute results around Australia to research groups.  I will also create several small \"templates\" with preconfigured access to data stores and HPC processing, for the purpose of testing the viability of rapid repurposing of VMs."
                                }
                            ],
                            "name": "020199"
                        }
                    ],
                    "name": "0201"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "rmit.edu.au",
                                    "name": "RMITCQPHRMC",
                                    "usagePatterns": "We will have several users and small to moderate data sets.",
                                    "useCase": "c/o Nigel Ward We want to match a theoretical model of a porous structure to an experimental pore size distribution - enabling fundamental understanding of the structure at the atomic level.  This requires the performance of many Hybrid Reverse Monte Carlo simulations, as currently there is no straightforward method to determine the one from the other. This project is also being used as a realistic test case for Steve Androulakis' BDP Research Tool, which relates to cloud-enabling the underlying software. "
                                }
                            ],
                            "name": "020406"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                }
                            ],
                            "name": "020405"
                        }
                    ],
                    "name": "0204"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Electromagnetic and photonic simulation",
                                    "usagePatterns": "Launch octave/matlab scripts to perform simulation. Very low disk access (typically 20MB written at the end of the simulation) but very high CPU need (full use for all the length of the simulation + post processing). High memory usage as well, depending on the system simulated (currently 2GB, but increases  with the square of the size of the system I am simulating). I also need to access my storage disk on storage2.eng.unimelb.edu.au from the VM to easily transfer results back on my desktop. ",
                                    "useCase": "I am running simulations of Electromagnetic systems via FDTD method to compare to experimental results. These simulations take several hours and a few GB of memory. "
                                }
                            ],
                            "name": "020504"
                        }
                    ],
                    "name": "0205"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uwa.edu.au",
                                    "name": "Swanserv",
                                    "usagePatterns": "We expect there to be no more than 10 usesr trying to access swanserv concurrently. The datasets that have to be uploaded and downloaded to swanserv range from about 150mb to 600mb ",
                                    "useCase": "Swanserv (along with our java application swan) is used as a tool to support data collection, quality verification and data analysis for radiation therapy trials.  Users go to our swanserv website (running on Tomcat) and are able to download an instance of our swan application which will allow them to view, manipulate, upload and download radiation therapy plans from the the swanserv server. Swanser in turn stores plans and trial information in a MySql database stored on the server. Users may also create and run scripts for validating plans via the swanserv website without having to load the plans into the swan application.  We already have this setup in place (along with a fair bit of data) on a server hosted by UWA but they are going to cease offering this service so we need to find somewhere else to host it. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uwa.edu.au",
                                    "name": "UWA Swanserv",
                                    "usagePatterns": "Radiotherapy plans tend to be a few hundred mb to 1gb. One of the main services of swanserv is to allow these to be uploaded and downloaded. We don't expect to have more than 10 to 15 concurrent users.",
                                    "useCase": "Swanserv is a tool to support radiotherapy research trials. It is a server with storage/access and some processing services for Radiotherapy plans and patient outcomes data. Currently we already have about 80gb of radiotherapy plans and outcomes data, which needs to be accessed by researchers nationally and internationally.  "
                                }
                            ],
                            "name": "029903"
                        }
                    ],
                    "name": "0299"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 18.0,
                                    "instanceQuota": 18.0,
                                    "institution": "versi.edu.au",
                                    "name": "RLI",
                                    "usagePatterns": "* many users (so far 118, but this is a growing number) * many small data sets",
                                    "useCase": "Hi, We would like to move the RLI (Remote Laboratory Instrumentation) to the cloud and see if it works just as well. More info on the project here: http://versi.edu.au/versi-projects/remote-instrument-access/remote-lab-instrumentation Feel free to contact me on michael.dsilva@versi.edu.au or 0403191855 for more info. PS: I'm not exactly sure what you mean by core hours. The idea is the VMs stay online and users connect to it and interact with it. It isn't a cluster job."
                                }
                            ],
                            "name": "02"
                        }
                    ],
                    "name": "02"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "VeRSI_AMSPP",
                                    "usagePatterns": "Large number of users in small access groups",
                                    "useCase": "This will run web, video, and databases for remote access to physical physics experiments at multiple institutes to encourage the uptake for physics in HED."
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 10.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "VeRSI_AMSPP",
                                    "usagePatterns": "Large number of users in small access groups",
                                    "useCase": "This will run web, video, and databases for remote access to physical physics experiments at multiple institutes to encourage the uptake for physics in HED."
                                }
                            ],
                            "name": "0203"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 2.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 0.5,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 0.5,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 0.5,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                }
                            ],
                            "name": "020302"
                        }
                    ],
                    "name": "0203"
                }
            ],
            "name": "02"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "uwa.edu.au",
                                    "name": "RICEDB",
                                    "usagePatterns": "Development phase will consist of a few users and large datasets.",
                                    "useCase": "The instance will be used for the development of a sub-cellular localisation database for rise, an important project in unifying the disparate datasets on protein localisation."
                                }
                            ],
                            "name": "060702"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "utas.edu.au",
                                    "name": "LegumeSeq1",
                                    "usagePatterns": "large data sets and small number of usersLegume",
                                    "useCase": "Analysing DNA Next Generation Sequencing data sets from several legumes transcriptomics experiments."
                                }
                            ],
                            "name": "060703"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.6,
                                    "institution": "murdoch.edu.au",
                                    "name": "Murdoch_Agri_Bio_Data_Integration",
                                    "usagePatterns": "This is a low level computing project will require the upload/process large datasets (500Mb per data source). There will be extensive testing (of random subsets of a data source) to optimise which (and how many data-sources) to be used before large process will occur.",
                                    "useCase": "As more datasets become available, researchers need to be smarter in how they integrate this information into a structure that will enable the evaluation of how cereal (wheat/barley) plants respond to changing environmental conditions to meet international grain markets. This research activity will utilise research in data ontology standards and statistical/bioinformatic tools to research data-pipelines for data mining of agricultural data."
                                }
                            ],
                            "name": "060799"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "utas.edu.au",
                                    "name": "LegumeSeq1",
                                    "usagePatterns": "large data sets and small number of usersLegume",
                                    "useCase": "Analysing DNA Next Generation Sequencing data sets from several legumes transcriptomics experiments."
                                }
                            ],
                            "name": "060705"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Chickpea GBrowse",
                                    "usagePatterns": "For this VM, the data set will not be big but there will be many users.",
                                    "useCase": "The Chickpea genome browsing system is a web-based applications with Apache2, CGI and Bioperl at the backend. The system mainly contains two components: 1.A generic Genome Browser (GBrowse) which has a lot of customisable features. 2.An in-house developed application called GBrowseBlast which allows users to submit a query DNA sequence to search for similar sequences in the genome and visualise the matching region directly in the GBrowse. Besides the frame of the system, the data is equally important. The Chickpea genome in NCBI contains the raw sequences which will be the reference sequence in the GBrowse. Other required genomic features will need to be extracted from the raw sequences and loaded to GBrowse as different tracks for visualisation. The final Chickpea browsing system will contain tracks with important features, such as genes, exons, assembled contigs, reads sequences, restriction sites and GC content. With these capabilities, the Chickpea research community will be able to easily search and visualise the Chickpea genome with the provided genomic features enabled. Users can also query their interested sequences to find homolog sequences in the Chickpea genome that is especially important to the plant research community.  "
                                }
                            ],
                            "name": "0607"
                        }
                    ],
                    "name": "0607"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 60.0,
                                    "instanceQuota": 60.0,
                                    "institution": "monash.edu",
                                    "name": "Characterisation VL Development and Testing",
                                    "usagePatterns": "Mixed interactive and scheduled usage.",
                                    "useCase": "This will be a first development environment for the Characterisation VL (CVL). The numbers I gave are a first estimate. Happy to refine this as we learn more about the system and develop our requirements. I may put in further requests on behalf of users of the CVL. "
                                },
                                {
                                    "coreQuota": 28.8,
                                    "instanceQuota": 28.8,
                                    "institution": "monash.edu",
                                    "name": "CharacterisationVL - [Additional resource]",
                                    "usagePatterns": "Large data sets",
                                    "useCase": "Characterisation VM"
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.4,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_Bisulphite_Adipose",
                                    "usagePatterns": "Large data sets with small amount of users.",
                                    "useCase": "The project aims to understand the Epigenetic modification and differences of several tissue types in Human and Sheep model. For this we are using WGBS. And the data size is enourmous. For the moment NICTA cloud would be the perfect solution for analysing these king of dataset as I am struggling with PBS jobs at our local CSIRO servers.   "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-CIAP",
                                    "usagePatterns": "At least initially we envision a small number or users (&lt; 1000 total, < 20 concurrent ) and relatively big data sets (e.g.28GB) We also need about 250 GB of volume storage.",
                                    "useCase": "This is an extension of the existing allocation for the NeCTAR Image Processing and Analysis toolkit. Some instances would be used to host online web interface and   database for the toolkit. (~ 3 instances ~ 4 cores) Other instances will be used as workers to preform actual computations. These my be provisioned on demand depending on the load. The worker instances may be either medium or large - this is still uncertain at the moment."
                                }
                            ],
                            "name": "06"
                        }
                    ],
                    "name": "06"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 400.0,
                                    "instanceQuota": 400.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "BIG SA",
                                    "usagePatterns": "A small number of medium sized data sets (a few GBytes each) which may be attached to each VM.",
                                    "useCase": "The Bioinformatics Interest Group of South Australia (BIG SA) is a group of people from across multiple disciplines and institutions, of which I am the president. I am currently involved with the development and delivery of a Next Generation Sequencing workshop in collaboration with national (BPA, CSIRO, ABN) and international groups (EBI). I am also involved with the delivery of several bioinformatic related workshops in Adelaide and am in the process of developing a Bioinformatics Training Platform for use on the cloud in these scenarios. Im involved in setting up and running of several hands-on bioinformatics training workshops later in the year. Here are some details:            Im currently organising two 3-day Software Carpentry (SWC) bootcamps for bioinformaticians - one to be held in Adelaide (24th-26th Sept 2013) and one to be held in Brisbane (30th Sept-2nd Oct 2013); each for 40 attendees  there is likely to be a need to instantiate 80 VMs concurrently so that there is sufficient time for the host institutions to update firewall rules.          The Australian Centre for Ancient DNA (ACAD) Bioinformatics Early Career Researcher Workshop is being run between 11-15th Nov 2013 for around 30 attendees: http://www.adelaide.edu.au/acad/events/bioinformatics13/. There will be morning lectures and afternoon hands-on sessions using VMs on the NeCTAR Research Cloud.          The annual BioInfoSummer symposium/conference will be held in Adelaide again this year (http://www.amsi.org.au/index.php/research-a-higher-education-mainmenu/workshop-programs/forthcoming-workshops/165-events/science-events-2013/1103-bis13) and we will again be using NeCTAR Cloud VMs for the afternoon hands-on sessions. Details are still to be worked out regarding numbers."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "murdoch.edu.au",
                                    "name": "Wheat Proteogenomics",
                                    "usagePatterns": "I will have one large dataset, namely the 6-frame translation, with reversed sequences of the Wheat genome, which will be accessed by 1 user, myself.",
                                    "useCase": "I am undertaking a Proteogenomics project on Wheat. I am using the MS/MS database search tool, MS-GFPlus to index the database (which takes a lot of resources and is currently the bottleneck) and perform the search. The 6-frame translation of the genome with reverse sequences is unable to run on our cluster with 48GB RAM and 12 cores. I am also using the MS/MS database search tool, Inspect and downstream proteogenomic scripts developed at the University of California, San Diego to do the analysis. The reason I'm also using MSGFPlus is to see how well it performs compared to Inspect, as Inspect will become depreciated in the not too distant future, replaced by MSGFPlus. This will become a good test case to further develop the proteogenomic pipeline by attempting to annotate such a large genome, improving its robustness across many other complicated genomes."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "GlatzSEQ",
                                    "usagePatterns": "These projects will involve 1-2 users with relatively small data sets",
                                    "useCase": "Cloud instances will be used to analyse transcriptome and amplicon sequencing data for 3 different projects."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Plant_Cell_Wall_Bioinformatics_Tools",
                                    "usagePatterns": "As above, the use case is for a small number of shell level users to access and manipulate large static files. These files may occasionally be shared with partner research nodes using the object store built in mechanisms.  Additionally, some analysis pipelines may expose web services which may result in additional web based users (e.g. BLAST search) from the Plant Cell Wall research group. ",
                                    "useCase": "Our two main aims for using NeCTAR are: (1) Storage of genomic and sequewnce data on the object store (large files that will be rarely modified/replaced, but accessed frequently) that will be : - processed on private NeCTAR instances  (see (2) -  accessed through other research cloud based resources, in particular the upcoming Galaxy instance through the Genomics Virtual Laboratory (https://genome.edu.au/) - accessed through other semi-local computing resources, in particular the Victorian Life Sciences Computation Initiative - accessed through other online services (2) Processing and analysis of the data stored in (1) on NeCTAR specific instances of bioinformatics software (eg CloudBioLinux, Ubuntu with BioLinux packages and custom images) including cluster based methods  "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "unsw.edu.au",
                                    "name": "NGS of type   ",
                                    "usagePatterns": "Single user, small data sets.",
                                    "useCase": "The immediate use will be analysing RNAseq data of cancer patients.  Single user will use the system. The data will be first quality-analysed, then downstream analyses including differential expression analysis will be conducted, and finally based on the analyses, inferences will be made.      "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Nippo Genome Assembly",
                                    "usagePatterns": "",
                                    "useCase": "I need to assemble the genome of a parasite species called nippostrongylus brasiliensis, using the short read archive data available in Genbank. the total data files are about 50gb. I hope another 25gb will be enough once assembled."
                                },
                                {
                                    "coreQuota": 6.3,
                                    "instanceQuota": 6.3,
                                    "institution": "mq.edu.au",
                                    "name": "UniCarbKB",
                                    "usagePatterns": "in the first instance a small number of users will utilize this resource, however, large datasets will be submitted.",
                                    "useCase": "This allocation will serve to test the performance and suitability of a proteomics data upload and sharing platform (http://www.proteios.org/) for UniCarbKB. The effort will mirror efforts by our Swedish partner to provide the community with suitable data sharing and processing capabilities. "
                                },
                                {
                                    "coreQuota": 9.0,
                                    "instanceQuota": 9.0,
                                    "institution": "mq.edu.au",
                                    "name": "UniCarbKB",
                                    "usagePatterns": "Small",
                                    "useCase": "Server instance required for hosting project documentation, tutorials and  web service examples."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "sydney.edu.au",
                                    "name": "GVL-SCF image testing",
                                    "usagePatterns": "Monday to Friday mostly. Sometimes I will have the image creating running overnight. There will be just me using the testing framework.  There is no large datasets required, except for what is needed to create the galaxy base image.",
                                    "useCase": "I am part of the Genome Virtual Laboratory, and I am working on getting the Science Collaboration Framework working within the GVL galaxy VM image.  I have it basically working now, and I want to test it more thoroughly now. I want to optimise the image creation script, so I can run it from one image, and it will install all the necessary prereqs so it can create the image on another VM."
                                },
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 64.0,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_NGS-HIE",
                                    "usagePatterns": "large data sets with a small number of users",
                                    "useCase": "In the field of biology, next-generation sequencing (NGS) technology has revolutionized the way we carryout research today. With rapidly decreasing sequencing costs and enormous data production, researchers are adopting this technology routinely. Typically a single run of a NGS experiment (HiSeq 2000) produces 600GB of data. Analysis of such a large data poses several technical and analytical challenges related to storage, speed and accuracy of results. Some of the challenges while analyzing NGS data are listed below: 1) Storage: Typically a single run of a NGS experiment (HiSeq 2000) produces 600GB of data. 2) Memory requirments: De novo assembly of plant and animal genomes and complex metagenomic data require a HPC with at least 64GB RAM per user. 3) Speed: accurate and rapid functional annotation of the sequence assemblies (or raw reads) requires a large number of CPU cores (usually more than 100) within a reasonable amount of time Current situation at ''Hawkesbury Institute for the Environment (HIE)\": At the Hawkesbury Institute for the Environment (University of Western Sydney), a significant amount of NGS data has been generated from the Roche and Illumina sequencing platforms. In the coming months/years, a deluge of sequencing data (genomic, transcriptomic, metagenomic etc.) is foreseen. Just in the next 3 months over 200 samples metagenomic samples are expected. Several plant and fungal genome re-sequencing and transcriptome sequencing projects are underway with approximately 100 NGS datasets being generated. Use case: As a Bioinformatics scientist representing HIE, I intend to provide essential Bioinformatics analysis support to researchers in addition to analyzing my own NGS data sets. "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin NGS",
                                    "usagePatterns": "Many Users with varying sizes of data sets.",
                                    "useCase": "NGS projects at Curtin University"
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF TAGdb",
                                    "usagePatterns": "The dataset is large, around 10TB, currently. The number of users various, based on the historical usage, it will be around 5~20 users per day.",
                                    "useCase": "The TAGdb is an online database enabling researchers to identify paried read sequences that share identity with a submitted query sequence. These tags can be used to design oligonucleotide primers for the PCR amplication of the region in the target genome. This tool is applicable for gene and promoter discovery in a wide range of species and greatly facilitates comparative genomics and molecular marker discovery in orphan crops or those with large and complex genomes. "
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF MiSeq",
                                    "usagePatterns": "Only the metadata and the 'just-processed' raw data will be located in this VM. Expecting there will be max 2T of data to be stored in this VM. End user will constantly accessing the web site to view the metadata via a web page. The number of users should be just a few at a time.",
                                    "useCase": "We have a MiSeq sequencing machine jointly with other Science faculty researchers. The DNA sequencing data generated from the MiSeq will need to be managed and a data server is needed to provide the following capabilities: Distributing the raw data for backup. Perform quality control (QC) and put the metadata to the database in the data server . Provide a web page to list the metadata and allow user to download the raw data. The MiSeq is managed jointly by several groups and the data server will be for managing data belonging to these groups. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Chickpea GBrowse",
                                    "usagePatterns": "For this VM, the data set will not be big but there will be many users.",
                                    "useCase": "The Chickpea genome browsing system is a web-based applications with Apache2, CGI and Bioperl at the backend. The system mainly contains two components: 1.A generic Genome Browser (GBrowse) which has a lot of customisable features. 2.An in-house developed application called GBrowseBlast which allows users to submit a query DNA sequence to search for similar sequences in the genome and visualise the matching region directly in the GBrowse. Besides the frame of the system, the data is equally important. The Chickpea genome in NCBI contains the raw sequences which will be the reference sequence in the GBrowse. Other required genomic features will need to be extracted from the raw sequences and loaded to GBrowse as different tracks for visualisation. The final Chickpea browsing system will contain tracks with important features, such as genes, exons, assembled contigs, reads sequences, restriction sites and GC content. With these capabilities, the Chickpea research community will be able to easily search and visualise the Chickpea genome with the provided genomic features enabled. Users can also query their interested sequences to find homolog sequences in the Chickpea genome that is especially important to the plant research community.  "
                                },
                                {
                                    "coreQuota": 50.0,
                                    "instanceQuota": 50.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO Bioinformatics training",
                                    "usagePatterns": "",
                                    "useCase": "We (the CSIRO Bioinformatics Core) collaborate with Bioplatforms Australia to deliver NGS training nationally using the NeCTAR cloud. The snapshot is named NGSTrainingV1.2.  This course has been very successful and we cannot meet demand. The courses are always heavily oversubscribed. We deliver a maximum of 5 courses nationally every year. To alleviate some of the demand, the CSIRO-based trainers will deliver parts of this course internally in CSIRO staff. This will lessen the demand from CSIRO for the national courses and make more room for others on those courses. "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Stemformatics",
                                    "usagePatterns": "Because we have users and collaborators from around the world the VM needs to be running 24/7 with scheduled downtime usually run on the Sunday. We have a small number of users that will increase rapidly and a small number of datasets that will continue to rise. This is due to the international collaborations we continue to acquire. We have less than 100 users and we currently need around 240GB of data with some relatively basic processing power (4 cores, 32GB RAM) . I put in for the extra extra large as it wouldn't hurt and our requirements may change. Total datasets stored is 100 microarray datasets with more coming in the near future. ",
                                    "useCase": "Web hosting for the Stemformatics Portal. The Stemformatics Portal provides stem cell biologists from around the world the ability to visually analyse and explore public and private stem cell datasets. We want to be able to use this VM as our new Production site."
                                },
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 64.0,
                                    "institution": "monash.edu",
                                    "name": "Monash_Galaxy_Server",
                                    "usagePatterns": "Probably many users with dataset sizes ranging from a few GB to over 100 or so GB.",
                                    "useCase": "We (Victorian Bioinformatics Consortium) intend to use this allocation to run Genomics Virtual Lab servers for Monash University Staff. (Note this is an interim measure until the monash node has persistent volumes and an object storage and is able to run the GVL images correctly.) It will be used for general bioinformatics after we have taught our users with the GVL's tutorials."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_Stemformatics",
                                    "usagePatterns": "Because we have users and collaborators from around the world the VM needs to be running 24/7 with scheduled downtime usually run on the Sunday. We have a small number of users that will increase rapidly and a small number of datasets that will continue to rise. This is due to the international collaborations we continue to acquire. We have less than 100 users and we currently need around 240GB of data with some relatively basic processing power (4 cores, 32GB RAM) . I put in for the extra extra large as it wouldn&#39;t hurt and our requirements may change. Total datasets stored is 100 microarray datasets with more coming in the near future. I don't know how many core hours you are asking for? It will be up 24/7",
                                    "useCase": "Web hosting for the Stemformatics Portal. The Stemformatics Portal provides stem cell biologists from around the world the ability to visually analyse and explore public and private stem cell datasets. We want to be able to use this VM as our new Production site."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "Stemformatics genomics computation",
                                    "usagePatterns": "Small number of technicians running Stemformatics data processing and annotation pipelines, with increasingly large datasets involved - current bottleneck in RNASeq and ChIPSeq processing preventing us from ramping up inclusion of these datasets in Stemformatics but if we can secure this compute server the larger datasets would start to become the norm moving forward. Typical size on disk of RNASeq dataset in the range of 200 or 300GB. At current demand, we'd be processing at least 2 of these a month and once our tools are updated to fully automate processing, this number could increase substantially. The 5TB persistent storage requested is about 80% of what we aim to use in the first 12 months of having this VM. Hard to project more than 12 months from now as it really depends on our available human resources moving forward. Object storage - not sure how we'd use that, so not requesting any right now although this may change in future as need arises.",
                                    "useCase": "The Stemformatics project is seeking to secure a compute / data processing virtual machine to meet its genomic data processing needs. Currently, we do not have server capacity through any available platform we use to streamline our data annotation and processing pipelines as we must manually use the Barrine HPC at UQ to process our RNASeq and other high throughput genomic data. Since we do not have a \"bridge\" into Barrine from our data processing server(s) it requires a lot of manual intervention, not to mention time waiting in queues for jobs to run. As we are ramping up inclusion of high throughout sequencing genomic data in Stemformatics we require a server that allows us to run our tools on the one machine without requiring external interfaces to remote HPCs. The Stemformatics project would greatly benefit from the reduced technical overhead in no longer having to manually deal with data transfer, job queues and debugging compute jobs on these remote HPC systems. Have requested 2 x 16 core nodes but we'd REALLY be wanting a single node with 32 cores if possible - otherwise we'd potentially have to split our processing across 2 servers which would go some way towards defeating the point as we may as well be using Barrine if we have to micro-manage data and job partitioning anyway, although I guess beggars can't be choosers!"
                                },
                                {
                                    "coreQuota": 96.0,
                                    "instanceQuota": 96.0,
                                    "institution": "monash.edu",
                                    "name": "Monash VBC Bioinformatics analysis",
                                    "usagePatterns": "",
                                    "useCase": "Our use for this is for a number of different bioinformatics analysis tasks.  These analyses involves collaborators across Victoria. The actual tasks we'd run would primarily be for de novo genome assembly and reference mapping of NGS data for RNA-seq and variant analysis. We have collaborations across Monash (and Victoria) on projects ranging from de novo bacterial assemblies to human cancer variant calling.  One of the computationally demanding tasks we run is de novo genome assembly which has both high memory and high compute demands.  We regularly do this for bacterial genomes with our collaborators in microbiology using our own hardware, however that hardware is shared and regularly overloaded.   And the computation demands are significantly higher when we work on eukaryotes. Some of the other tasks we perform with our collaborators involve variant calling, and RNA-seq analysis.  Both of which require mapping a large number of next-generation sequencing (NGS) reads to a genome.  This is reasonably parallelizable across compute nodes using existing software.  So, would map nicely onto a number of cloud compute instances. Apart from these specific tasks we'd like to use the cloud instances for, we also maintain a server that is well customized for the type of bioinformatics analysis we perform.  It would be useful to be able to have a snapshot of this setup on a cloud volume so that we could give other bioinformaticians around Monash the ability to easily create their own cloud instance based on our setup.  At the moment, we typically give these people access to our server which is regularly overloaded. "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Genome_assembly",
                                    "usagePatterns": "My experiments will deal with large data set of witch the size is about 10 gb. But I will be the only one to involve with the programming part of this program. ",
                                    "useCase": "The cloud will be used for processing the next-generation sequencing data. I'll try to solve the repeat problem in genome assembly which is based on K-mer method.  This will help biologist to build more complete reference genome. Since this program needs large computer memory and powerful computational ability, I ask for help. "
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "mq.edu.au",
                                    "name": "UniCarb_DB",
                                    "usagePatterns": "",
                                    "useCase": "Hadoop for testing and evaluating mass spectrometry data interpretation in accordance with unicarbkb objectives.  We plan to generate a large theoretical database of spectra for over 200,000 glycan structures. That is suitable for developing a Hadoop/HBase infrastructure for large-scale data processing.  This approach will be extended to support a web service API that will enable users to automatically assign data collections. A pilot study on a single instance is proofing successfully with a 90% accuracy rate. This is an innovative strategy and we have the opportunity to publish and provide a leading tool for this requirement by researchers. Support will be required to setup and configure Hadoop. Contact and requirements discussed with Nigel Ward."
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "BRAEMBL",
                                    "usagePatterns": "Persistent storage required to deliver datasets used in bioinformatics analysis. We currently use our own storage (750GB volume) and we would plan to have this data made available by RDSI.",
                                    "useCase": "The BRAEMBL project provides bioinformatics services to the Australian research community and currently delivers its services from its own infrastructure at UQ (in DC3) and also along with Barrine as our HPC compute. This is a pilot project to evaluate whether NeCTAR is capable of running the services that we deliver with an eventual goal of being able to supersede our existing infrastructure with cloud technology."
                                },
                                {
                                    "coreQuota": 250.0,
                                    "instanceQuota": 250.0,
                                    "institution": "uq.edu.au",
                                    "name": "GenomicsVL (Extension to an existing project)",
                                    "usagePatterns": "The request for Instances has been already made for this project and we  require 2 persistent volumes of 10TB each to be attached to the UCSC VM.",
                                    "useCase": "I am currently working on a mirror for the UCSC genome browser and Im using different VMs created in the GenomicsVL project to manage the data. For this project I have 80TB of RDSI storage that I can mount via NFS on any VM(10.255.100.50:/collection/Q0031/Q0031/) The UCSC genome browser is using different mechanisms to provide information to the user: -Massive flat files of metadata and sequences - Mysql database with annotations for different genomes. (UCSC generates a LOT of connections to Mysql) In addition UCSC have its own trash manager generating temp files. (Big I/O ). I need to have volumes faster than NFS for the Mysql DB and the Trash Manager. "
                                },
                                {
                                    "coreQuota": 56.0,
                                    "instanceQuota": 56.0,
                                    "institution": "uq.edu.au",
                                    "name": "Hadoop-based Genomic Mapping",
                                    "usagePatterns": "~4 users, Many small datasets. A few moderate datasets ",
                                    "useCase": "With application in genomics research, personalized medicine and diagnostics, the growth of next-generation sequencing (NGS) defies Moore's law. The resulting flood of data has led to the development of bioinformatics tools that can harvest the power of the Hadoop framework. This project aims at demonstrating the advantage of using the NeCTAR cloud infrastructure to run genome mapping in comparison with traditional cluster computing."
                                },
                                {
                                    "coreQuota": 70.0,
                                    "instanceQuota": 70.0,
                                    "institution": "monash.edu",
                                    "name": "Monash-Galaxy-Prod",
                                    "usagePatterns": "The project is likely to have 10 - 50 users and some users will use more than 500GB of Volume space. May even use one TB.",
                                    "useCase": "This allocation will be for the Monash Galaxy Production server. My other allocation is being used for  training users and post grad students in the use of Galaxy and bioinformatics analyses. Once trained, the users will be moved onto this allocation for their work. It is envisaged that this Galaxy server will be used by researchers from SOBS staff but will be open to anyone from the University."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "SACGF",
                                    "usagePatterns": "We will have only a few users (2-5), we have our own storage, but we require the cloud for worker nodes in a Torque cluster. I have amended the request to include some persistant storage.  While we won't need this in the longer term, it turns out that we will need it in the short term in order to test the setup on sample projects.",
                                    "useCase": "We are a genome sequencing facility and our research is in the area of genomics/bioinformatics.  For our computing we have been using Torque queues both on our own dedicated server as well as eResearch SA servers.  eResearch SA is now moving over to Nectar cloud computing, so we want to implement our workflows/pipelines on a Nectar-based setup.  It is likely that we will implement this using  a dynamic Torque set-up.  The present allocation is designed to allow us to develop, implement and test  this on some sample projects.  If successful, we would then request allocations on a more permanent basis."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 3.2,
                                    "institution": "uq.edu.au",
                                    "name": "UQ GVL training",
                                    "usagePatterns": "Just one user a and small test datasets. Most use will be in short bursts of launching new VMs, running them for a short while, and then closing them down again.",
                                    "useCase": "I am delivering training in resources developed under the GVL. Since some of these (particularly Galaxy) require volume storage to work, I want this access to test training material from the perspective of a standard user (or at least, one who has requested volume storage) rather than just testing things within the GVL allocation"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "QCMG-TEST1",
                                    "usagePatterns": "Heavy use when active.  Jobs typically run for hours to days.",
                                    "useCase": "Test case for possible migration of QCMG Australian Pancreatic Genome project dataset to RDSI/NECTAR."
                                },
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_NGS-HIE",
                                    "usagePatterns": "large data sets with a small number of users",
                                    "useCase": "In the field of biology, next-generation sequencing (NGS) technology has revolutionized the way we carryout research today. With rapidly decreasing sequencing costs and enormous data production, researchers are adopting this technology routinely. Typically a single run of a NGS experiment (HiSeq 2000) produces 600GB of data. Analysis of such a large data poses several technical and analytical challenges related to storage, speed and accuracy of results. Some of the challenges while analyzing NGS data are listed below: 1) Storage: Typically a single run of a NGS experiment (HiSeq 2000) produces 600GB of data. 2) Memory requirments: De novo assembly of plant and animal genomes and complex metagenomic data require a HPC with at least 64GB RAM per user. 3) Speed: accurate and rapid functional annotation of the sequence assemblies (or raw reads) requires a large number of CPU cores (usually more than 100) within a reasonable amount of time Current situation at ''Hawkesbury Institute for the Environment (HIE)\": At the Hawkesbury Institute for the Environment (University of Western Sydney), a significant amount of NGS data has been generated from the Roche and Illumina sequencing platforms. In the coming months/years, a deluge of sequencing data (genomic, transcriptomic, metagenomic etc.) is foreseen. Just in the next 3 months over 200 samples metagenomic samples are expected. Several plant and fungal genome re-sequencing and transcriptome sequencing projects are underway with approximately 100 NGS datasets being generated. Use case: As a Bioinformatics scientist representing HIE, I intend to provide essential Bioinformatics analysis support to researchers in addition to analyzing my own NGS data sets. "
                                },
                                {
                                    "coreQuota": 50.0,
                                    "instanceQuota": 25.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO Bioinformatics training",
                                    "usagePatterns": "",
                                    "useCase": "We (the CSIRO Bioinformatics Core) collaborate with Bioplatforms Australia to deliver NGS training nationally using the NeCTAR cloud. The snapshot is named NGSTrainingV1.2.  This course has been very successful and we cannot meet demand. The courses are always heavily oversubscribed. We deliver a maximum of 5 courses nationally every year. To alleviate some of the demand, the CSIRO-based trainers will deliver parts of this course internally in CSIRO staff. This will lessen the demand from CSIRO for the national courses and make more room for others on those courses. "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "Stemformatics genomics computation",
                                    "usagePatterns": "Small number of technicians running Stemformatics data processing and annotation pipelines, with increasingly large datasets involved - current bottleneck in RNASeq and ChIPSeq processing preventing us from ramping up inclusion of these datasets in Stemformatics but if we can secure this compute server the larger datasets would start to become the norm moving forward. Typical size on disk of RNASeq dataset in the range of 200 or 300GB. At current demand, we'd be processing at least 2 of these a month and once our tools are updated to fully automate processing, this number could increase substantially. The 5TB persistent storage requested is about 80% of what we aim to use in the first 12 months of having this VM. Hard to project more than 12 months from now as it really depends on our available human resources moving forward. Object storage - not sure how we'd use that, so not requesting any right now although this may change in future as need arises.",
                                    "useCase": "The Stemformatics project is seeking to secure a compute / data processing virtual machine to meet its genomic data processing needs. Currently, we do not have server capacity through any available platform we use to streamline our data annotation and processing pipelines as we must manually use the Barrine HPC at UQ to process our RNASeq and other high throughput genomic data. Since we do not have a \"bridge\" into Barrine from our data processing server(s) it requires a lot of manual intervention, not to mention time waiting in queues for jobs to run. As we are ramping up inclusion of high throughout sequencing genomic data in Stemformatics we require a server that allows us to run our tools on the one machine without requiring external interfaces to remote HPCs. The Stemformatics project would greatly benefit from the reduced technical overhead in no longer having to manually deal with data transfer, job queues and debugging compute jobs on these remote HPC systems. Have requested 2 x 16 core nodes but we'd REALLY be wanting a single node with 32 cores if possible - otherwise we'd potentially have to split our processing across 2 servers which would go some way towards defeating the point as we may as well be using Barrine if we have to micro-manage data and job partitioning anyway, although I guess beggars can't be choosers!"
                                },
                                {
                                    "coreQuota": 96.0,
                                    "instanceQuota": 96.0,
                                    "institution": "monash.edu",
                                    "name": "Monash VBC Bioinformatics analysis",
                                    "usagePatterns": "",
                                    "useCase": "Our use for this is for a number of different bioinformatics analysis tasks.  These analyses involves collaborators across Victoria. The actual tasks we'd run would primarily be for de novo genome assembly and reference mapping of NGS data for RNA-seq and variant analysis. We have collaborations across Monash (and Victoria) on projects ranging from de novo bacterial assemblies to human cancer variant calling.  One of the computationally demanding tasks we run is de novo genome assembly which has both high memory and high compute demands.  We regularly do this for bacterial genomes with our collaborators in microbiology using our own hardware, however that hardware is shared and regularly overloaded.   And the computation demands are significantly higher when we work on eukaryotes. Some of the other tasks we perform with our collaborators involve variant calling, and RNA-seq analysis.  Both of which require mapping a large number of next-generation sequencing (NGS) reads to a genome.  This is reasonably parallelizable across compute nodes using existing software.  So, would map nicely onto a number of cloud compute instances. Apart from these specific tasks we'd like to use the cloud instances for, we also maintain a server that is well customized for the type of bioinformatics analysis we perform.  It would be useful to be able to have a snapshot of this setup on a cloud volume so that we could give other bioinformaticians around Monash the ability to easily create their own cloud instance based on our setup.  At the moment, we typically give these people access to our server which is regularly overloaded. "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 1.2,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Genome_assembly",
                                    "usagePatterns": "My experiments will deal with large data set of witch the size is about 10 gb. But I will be the only one to involve with the programming part of this program. ",
                                    "useCase": "The cloud will be used for processing the next-generation sequencing data. I'll try to solve the repeat problem in genome assembly which is based on K-mer method.  This will help biologist to build more complete reference genome. Since this program needs large computer memory and powerful computational ability, I ask for help. "
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 5.0,
                                    "institution": "mq.edu.au",
                                    "name": "UniCarb_DB",
                                    "usagePatterns": "",
                                    "useCase": "Hadoop for testing and evaluating mass spectrometry data interpretation in accordance with unicarbkb objectives.  We plan to generate a large theoretical database of spectra for over 200,000 glycan structures. That is suitable for developing a Hadoop/HBase infrastructure for large-scale data processing.  This approach will be extended to support a web service API that will enable users to automatically assign data collections. A pilot study on a single instance is proofing successfully with a 90% accuracy rate. This is an innovative strategy and we have the opportunity to publish and provide a leading tool for this requirement by researchers. Support will be required to setup and configure Hadoop. Contact and requirements discussed with Nigel Ward."
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 10.0,
                                    "institution": "uq.edu.au",
                                    "name": "BRAEMBL",
                                    "usagePatterns": "Persistent storage required to deliver datasets used in bioinformatics analysis. We currently use our own storage (750GB volume) and we would plan to have this data made available by RDSI.",
                                    "useCase": "The BRAEMBL project provides bioinformatics services to the Australian research community and currently delivers its services from its own infrastructure at UQ (in DC3) and also along with Barrine as our HPC compute. This is a pilot project to evaluate whether NeCTAR is capable of running the services that we deliver with an eventual goal of being able to supersede our existing infrastructure with cloud technology."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "GenomicsVL (Extension to an existing project)",
                                    "usagePatterns": "The request for Instances has been already made for this project and we  require 2 persistent volumes of 10TB each to be attached to the UCSC VM.",
                                    "useCase": "I am currently working on a mirror for the UCSC genome browser and Im using different VMs created in the GenomicsVL project to manage the data. For this project I have 80TB of RDSI storage that I can mount via NFS on any VM(10.255.100.50:/collection/Q0031/Q0031/) The UCSC genome browser is using different mechanisms to provide information to the user: -Massive flat files of metadata and sequences - Mysql database with annotations for different genomes. (UCSC generates a LOT of connections to Mysql) In addition UCSC have its own trash manager generating temp files. (Big I/O ). I need to have volumes faster than NFS for the Mysql DB and the Trash Manager. "
                                },
                                {
                                    "coreQuota": 56.0,
                                    "instanceQuota": 7.0,
                                    "institution": "uq.edu.au",
                                    "name": "Hadoop-based Genomic Mapping",
                                    "usagePatterns": "~4 users, Many small datasets. A few moderate datasets ",
                                    "useCase": "With application in genomics research, personalized medicine and diagnostics, the growth of next-generation sequencing (NGS) defies Moore's law. The resulting flood of data has led to the development of bioinformatics tools that can harvest the power of the Hadoop framework. This project aims at demonstrating the advantage of using the NeCTAR cloud infrastructure to run genome mapping in comparison with traditional cluster computing."
                                },
                                {
                                    "coreQuota": 70.0,
                                    "instanceQuota": 10.5,
                                    "institution": "monash.edu",
                                    "name": "Monash-Galaxy-Prod",
                                    "usagePatterns": "The project is likely to have 10 - 50 users and some users will use more than 500GB of Volume space. May even use one TB.",
                                    "useCase": "This allocation will be for the Monash Galaxy Production server. My other allocation is being used for  training users and post grad students in the use of Galaxy and bioinformatics analyses. Once trained, the users will be moved onto this allocation for their work. It is envisaged that this Galaxy server will be used by researchers from SOBS staff but will be open to anyone from the University."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 4.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "SACGF",
                                    "usagePatterns": "We will have only a few users (2-5), we have our own storage, but we require the cloud for worker nodes in a Torque cluster. I have amended the request to include some persistant storage.  While we won't need this in the longer term, it turns out that we will need it in the short term in order to test the setup on sample projects.",
                                    "useCase": "We are a genome sequencing facility and our research is in the area of genomics/bioinformatics.  For our computing we have been using Torque queues both on our own dedicated server as well as eResearch SA servers.  eResearch SA is now moving over to Nectar cloud computing, so we want to implement our workflows/pipelines on a Nectar-based setup.  It is likely that we will implement this using  a dynamic Torque set-up.  The present allocation is designed to allow us to develop, implement and test  this on some sample projects.  If successful, we would then request allocations on a more permanent basis."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.3,
                                    "institution": "uq.edu.au",
                                    "name": "RNA-seq of iPS-derived human neurons",
                                    "usagePatterns": "This project will have a small number of users (primarily the main applicant), with large datasets, a relatively small number of big reference files (genome, annotation, mapping indexes) and a relatively small (<=100) number of output files for each sample (~60 samples total planned). The main bottlenecks include mapping RNA-seq reads (using STAR, which is quite fast, but loads the entire genome index into memory (~40Gb for human or mouse, the two species which will be interrogated for this project)) and assembling the reads into transcripts using cufflinks, which uses ~50Gb RAM for prolonged periods of time (up to 2 weeks per dataset)).   ==== Update 12.02.14 -> Have checked reference file requirements (which will be placed into object storage); currently am using 300Gb on barrine. -> In terms of volume storage, have looked at /ebi/bscratch and use ~1-1.5Tb per series of experiments (which is what would be run in parallel at a given time for the project). In order to trial running my analysis on one dataset at a time on the VMs, I would likely need ~150Gb of space; more would, of course, be better. ",
                                    "useCase": "This project focuses on characterising the differentiation of induced pluripotent stem (iPS) cells into neurons.  iPS cells are stem cells made from skin cells, which can be taken from normal people or those afflicted by a certain disease. These stem cells can then be differentiated into other cell types, for example brain cells. These cell types can be used for basic research to replace model animals, and drugs can be tested on these \"brain cells in a dish\", with the most effective drug that alleviates the patient's symptoms \"in a dish\" chosen to be administered to the patient in vivo.  While this is the hope for the application of stem cells in basic research and personalised medicine for drug testing, the reality is that we don't know how similar these neurons in the dish are to the ones we've got in our brains, which is what this project is attempting to find out. By sequencing the RNA of iPS derived stem cells and the neurons we are differentiating from them (at several time points of differentiation) we are attempting to understand what changes occur, at the molecular level, in these cells as they go from being a stem cell to being a neuron, and how similar these changes are to ones we know normally happen during neurological development.  The results of this work will be presented at local and international conferences, and subsequently written up as manuscripts for publication. ==== This research is being partly funded by NHMRC grant 1021005 \"ReprogrammingofAtaxiaTelangiectasiafibroblaststogenerateiPScells\" and APP1043023 \"Investigation of processed snoRNAs as cryptic regulators of the imprinted Prader-Willi syndrome locus\". This work represents a collaboration between bioinformaticians at the IMB, stem cell biologists at AIBN,  and stem cell biologists and clinicians at QIMR. "
                                },
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 4.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "ACAD",
                                    "usagePatterns": "Cluster in the cloud",
                                    "useCase": "Bayesian Analysis Population genomics Pyhlogenomics/Phylogenetics"
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_genes4all_clients",
                                    "usagePatterns": "Each instance will be a web-server that will serve data derived from a database server located elsewhere in the cloud (I had to file a separate allocation request).",
                                    "useCase": "Modern genomics has enabled a large number of researchers to produce vast amounts of data for their favourite organism. This is especially true for species of industrial and/or ecological importance (instead of the classic lab models: mouse, human, fruitfly etc). The bottleneck is not the processing of the data (dealt via HPC) but the dissemination and visualization in a standardized, pain-free format. Further, curation and interactivity allows the web users to edit and curate the metadata in a crowdsourcing approach. Our work has produced such a system, written in PHP (Drupal), JavaScript (ExtJS and JQuery) and some perl code. We are now seeking to deploy it as domain-specific web-servers (e.g. only for specific species or projects). Each instance will host a webserver that will serve domain-specific data in a common GUI. Users will be using port 80 to access the interface.  Administrators (one or two individuals) will be responsible for maintaining the web server but we don't expect frequent updates. Data will be derived from a database server located elsewhere on the NECTaR cloud and/or within CSIRO. An example can be seen at http://insectacentral.org. Previous work has been cited 38 times since 2008 (DOI: 10.1093/nar/gkm853). I'm more than happy to provide more information if this is required."
                                },
                                {
                                    "coreQuota": 68.0,
                                    "instanceQuota": 68.0,
                                    "institution": "bioplatforms.com",
                                    "name": "Bioplatforms-Au-NGS-training-course",
                                    "usagePatterns": "We will rune two 3-day workshops where 35-45 participants will be analyzing small data sets using a variety of bioinformatics software tools.",
                                    "useCase": "We would like to use cloud instances to run a national 3 day Next Generation Sequencing workshop. It will involve analysis of small data sets.   3 day workshop over which there will be small data sets that 35-45 participants will be analyzing"
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "NextGen sequencing data analysis for plants",
                                    "usagePatterns": "Very large datasets with several users concurrently using the data for analysis on the CLC genomics server",
                                    "useCase": "Computing to run CLC genomics server software which will link up with HPC nodes at central computing at UQ"
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 2.5,
                                    "institution": "uq.edu.au",
                                    "name": "Plants-Seq-UQ",
                                    "usagePatterns": "3/12/2013 - Ten or more people will be undertaking bioinformatics analysis of data spread across at least 10 projects. At present we have one node on the Nectar allocation presently used for management and processing our data. The data is on Q-cloud (presently about 25 TB) and we use this sequence data (in 10s of GB at a time) for data mining of million of sequence reads, mapping sequence, association genetics and statistical analysis. These processes require several cores and at least 64 or more of RAM per node. We have one node at present and is meaningless for our group as far as the number of projects, the number of people and the timelines of these project. Thus we request cores and RAM equating to several nodes for the Henry group. The nodes requested will be used on a regular basis for the analysis. In addition, the present workload will increase based on our recently funded projecst and additional work we have planned. ",
                                    "useCase": "This is an extension to the original UQ-Plants-Seq request. We will be using the CLC genomics server-software to drive our bioinformatics analysis processes. One VM of 16 cores will be used as the Management Node to drive the CLC-Genomics Server-software. The other VMs will be used as nodes to carry out the bioinformatics processing. 3/12/2013 - We have at present 10 genomics projects and the bioinformatic analysis would require at least a minimum of five nodes for us to make progress in our work leading to publication in a timely manner and for students to complete their work as well. "
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 0.6,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_Bisulphite_Adipose",
                                    "usagePatterns": "Large data sets with small amount of users.",
                                    "useCase": "The project aims to understand the Epigenetic modification and differences of several tissue types in Human and Sheep model. For this we are using WGBS. And the data size is enourmous. For the moment NICTA cloud would be the perfect solution for analysing these king of dataset as I am struggling with PBS jobs at our local CSIRO servers.   "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uq.edu.au",
                                    "name": "Marine Genomics: sea sponge and other invertebrate genomic resources for the Great Barrier Reef ",
                                    "usagePatterns": "We request the use of 2 XXL instances (32 cores total) hosted in Queensland when possible.  We will have < 50 researchers [primarily UQ, with usage from Univ. Sunshine Coast and Australian Institute of Marine Science (http://aims.gov.au/) and international collaborators (current collaborators include Okinawa Institute of Science and Technology, Japan; UC Santa Barbara; UC Berkeley, USA; Univ of Vienna, Austria; Technion Univ, Israel), potentially extending to other Queensland, Australian and international universities, institutes and industry in the future) using the computing nodes on a daily basis. Potentially > 10,000 users accessing our public browser from a web interface over the course of the whole project (not simultaneously).  The public browser will be hosted in an independent instance (Large) to enable simplified management.  We plan to keep images of our VMs and crucial data shared by all projects in the Object Storage. We are currently applying for storage space on the QCIF RDSI Node to mount on our VMs in order to process and generate our data collections, and so require our VMs on the QCIF Research Cloud Node. We plan to only use local storage for high I/O processes. We generally use few files >10GB and >10,000s of small files (<1GB). ",
                                    "useCase": "These cores will service primarily bioinformatics analyses in the Marine Genomics Laboratory at UQ. This laboratory is located in the School of Biological Sciences and Centre for Marine Science and is comprised on two research groups, the Sandie Degnan Lab and the Bernie Degnan Lab. There are 20+ researchers and postgraduate students in these labs, most of which have bioinformatics intensive projects. Most of the research is focussed on the analysis of the structure, function and organisation of the genome of the model marine sponge Amphimedon queenslandica and its bacterial symbionts.  This is one the premier models to understand the evolution and ecology of animals at the genomic level and the first, and still only, marine animal in Australia to have it genome sequenced and published.  Recent research outcomes from this genome project have been published in Science (IF 31.0), Nature (IF 38.6), Proc Natl Acad Sci USA (IF 9.7), PLoS Biol (IF 12.7), Current Biology (IF 9.5), Nature Microbiol Rev (IF 22.5), Nature Struct Mol Biol (IF 11.9) and Mol Biol Evol (IF 10.4).  The addition of other marine genomes and transcriptomes (e.g. the pearl oyster Pinctada maxima and the starfish Crown of Thorns Acanthaster planci) into this data collection enable collaborations with local universities and institutes and with Australian industry partners, in addition to a large diversity of international partners.  Each of these interactions provide unique platforms to expand the use of genomic information in marine science, particularly in tropical Australia. We require the capacity to undertake de novo genome, transcriptome and metagenome assembly, sequence annotation, database querying as well as host a private as well as publicly available genome browser that enables us, our collaborators and other researchers across the globe to interact with the genomic information we are generating.  "
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "murdoch.edu.au",
                                    "name": "Murdoch_ACCWI",
                                    "usagePatterns": "This server will not store the main data sets of the centre, but may be a temporary storage point as we prepare datasets for submission to larger repositories.",
                                    "useCase": "This server will be used by members of the Australia-China Centre for Wheat Improvement (ACCWI) at Murdoch University for general purpose tasks., such as writing and testing code. It will function as the main server \"hub\" for the centre (which does not have any physical compute infrastructure of its own). Small analysis tasks may also be run by some users. It will also be used as the base for file exchange with collaborators."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "Marine Genomics: sea sponge and other invertebrate genomic resources for the Great Barrier Reef ",
                                    "usagePatterns": "We request the use of 2 XXL instances (32 cores total) hosted in Queensland when possible.  We will have < 50 researchers [primarily UQ, with usage from Univ. Sunshine Coast and Australian Institute of Marine Science (http://aims.gov.au/) and international collaborators (current collaborators include Okinawa Institute of Science and Technology, Japan; UC Santa Barbara; UC Berkeley, USA; Univ of Vienna, Austria; Technion Univ, Israel), potentially extending to other Queensland, Australian and international universities, institutes and industry in the future) using the computing nodes on a daily basis. Potentially > 10,000 users accessing our public browser from a web interface over the course of the whole project (not simultaneously).  The public browser will be hosted in an independent instance (Large) to enable simplified management.  We plan to keep images of our VMs and crucial data shared by all projects in the Object Storage. We are currently applying for storage space on the QCIF RDSI Node to mount on our VMs in order to process and generate our data collections, and so require our VMs on the QCIF Research Cloud Node. We plan to only use local storage for high I/O processes. We generally use few files >10GB and >10,000s of small files (<1GB). ",
                                    "useCase": "These cores will service primarily bioinformatics analyses in the Marine Genomics Laboratory at UQ. This laboratory is located in the School of Biological Sciences and Centre for Marine Science and is comprised on two research groups, the Sandie Degnan Lab and the Bernie Degnan Lab. There are 20+ researchers and postgraduate students in these labs, most of which have bioinformatics intensive projects. Most of the research is focussed on the analysis of the structure, function and organisation of the genome of the model marine sponge Amphimedon queenslandica and its bacterial symbionts.  This is one the premier models to understand the evolution and ecology of animals at the genomic level and the first, and still only, marine animal in Australia to have it genome sequenced and published.  Recent research outcomes from this genome project have been published in Science (IF 31.0), Nature (IF 38.6), Proc Natl Acad Sci USA (IF 9.7), PLoS Biol (IF 12.7), Current Biology (IF 9.5), Nature Microbiol Rev (IF 22.5), Nature Struct Mol Biol (IF 11.9) and Mol Biol Evol (IF 10.4).  The addition of other marine genomes and transcriptomes (e.g. the pearl oyster Pinctada maxima and the starfish Crown of Thorns Acanthaster planci) into this data collection enable collaborations with local universities and institutes and with Australian industry partners, in addition to a large diversity of international partners.  Each of these interactions provide unique platforms to expand the use of genomic information in marine science, particularly in tropical Australia. We require the capacity to undertake de novo genome, transcriptome and metagenome assembly, sequence annotation, database querying as well as host a private as well as publicly available genome browser that enables us, our collaborators and other researchers across the globe to interact with the genomic information we are generating.  "
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "Plants-Seq-UQ",
                                    "usagePatterns": "3/12/2013 - Ten or more people will be undertaking bioinformatics analysis of data spread across at least 10 projects. At present we have one node on the Nectar allocation presently used for management and processing our data. The data is on Q-cloud (presently about 25 TB) and we use this sequence data (in 10s of GB at a time) for data mining of million of sequence reads, mapping sequence, association genetics and statistical analysis. These processes require several cores and at least 64 or more of RAM per node. We have one node at present and is meaningless for our group as far as the number of projects, the number of people and the timelines of these project. Thus we request cores and RAM equating to several nodes for the Henry group. The nodes requested will be used on a regular basis for the analysis. In addition, the present workload will increase based on our recently funded projecst and additional work we have planned. ",
                                    "useCase": "This is an extension to the original UQ-Plants-Seq request. We will be using the CLC genomics server-software to drive our bioinformatics analysis processes. One VM of 16 cores will be used as the Management Node to drive the CLC-Genomics Server-software. The other VMs will be used as nodes to carry out the bioinformatics processing. 3/12/2013 - We have at present 10 genomics projects and the bioinformatic analysis would require at least a minimum of five nodes for us to make progress in our work leading to publication in a timely manner and for students to complete their work as well. "
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "Plants-Seq-UQ",
                                    "usagePatterns": "3/12/2013 - Ten or more people will be undertaking bioinformatics analysis of data spread across at least 10 projects. At present we have one node on the Nectar allocation presently used for management and processing our data. The data is on Q-cloud (presently about 25 TB) and we use this sequence data (in 10s of GB at a time) for data mining of million of sequence reads, mapping sequence, association genetics and statistical analysis. These processes require several cores and at least 64 or more of RAM per node. We have one node at present and is meaningless for our group as far as the number of projects, the number of people and the timelines of these project. Thus we request cores and RAM equating to several nodes for the Henry group. The nodes requested will be used on a regular basis for the analysis. In addition, the present workload will increase based on our recently funded projecst and additional work we have planned. ",
                                    "useCase": "This is an extension to the original UQ-Plants-Seq request. We will be using the CLC genomics server-software to drive our bioinformatics analysis processes. One VM of 16 cores will be used as the Management Node to drive the CLC-Genomics Server-software. The other VMs will be used as nodes to carry out the bioinformatics processing. 3/12/2013 - We have at present 10 genomics projects and the bioinformatic analysis would require at least a minimum of five nodes for us to make progress in our work leading to publication in a timely manner and for students to complete their work as well. "
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 2.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UniCarb_NeCTAR_Support",
                                    "usagePatterns": "One setup for test purposes.",
                                    "useCase": "Support for UniCarbKB project"
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_QEH_Rhinology",
                                    "usagePatterns": "For now, I will be the only user in the department running and accessing the instances. The input files/datasets will be huge (in GBs), since they will be files produced by pyrosequencers, for example 454 Roche or Illumina sequencers, hence the need for big storage space in the form of Volumes and Object storage. At the beginning, I do not expect to run more than one large instance at a time in case a pipeline is running. Once we have all the VM images/snapshots ready and pipelines and scripts set in place, I can then request from NECTAR admins for more resources or storage space in case this is needed, but this won't be needed initially. Some genomics work is known to be RAM-limited rather than CPU-limited, due to the huge datasets, and this is the reason (high RAM) I may need to do run an xx-large instance, but again this will not be needed until I have set up my development environment on snapshots and running test runs of the pipeline (and I become comfortable with the NECTAR openstack system). In general, I estimate the workload per year will be fairly very low compared to researchers from other fields, since microbiome research constitutes only a percentage of all of our research.",
                                    "useCase": "We are the department Otorhinolaryngology, Head & Neck Surgery (Department of Surgery, University of Adelaide), based in the Queen Elizabeth Hospital (QEH) campus in SA. Our main research is Rhinology, revolving around chronic rhinosinusitis and endoscopic sinus surgery. Our department has recently started doing microbiome research in human sinuses, to study the role of the sinus microbiome in chronic rhinosinusitis. For example, we have recently published a paper on 16s rRNA amplicon sequencing to identify fungi in the sinuses) We will also be doing RNA-seq work in the near future to study the immunological response of the host to the microbiome. For us to start doing this work in-house, we need a cluster or a cloud. I had previously tried running our own in-house pipeline using QIIME v1.6 (Quantitative insights into Microcial Ecology) on a machine in our department and it constantly fails during the denoising step, because it is so computer-intensive and I hope NECTAR is going to help us develop our own tools and produce our own results instead of outsourcing them. The types of programs we will be running on the instances will be mostly bioinformatics/genomics scripts/programs for example: - denoising raw NGS datasets, such as pyronoise, ampliconnoise or QIIME - BLAST - cluserting algorithms, for Operational Taxonomic Units (OTUs) clustering - taxonomic classification of OTUs, for example using a Naive Bayes Classifier - genome sequence alignment, such as TopHat or Velvet - Python - Postgresql "
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_hmC_methylation",
                                    "usagePatterns": "The project will have large datasets but only a maximum of 5 users",
                                    "useCase": "We have a sequencing project between two departments that require a large amount of storage room for sequencing datasets. A student and myself will be using it to analyse DNA methylation states. The student is split between multiple campuses and has little experience with linux, so therefore an appropriately resourced virtual machine will help him immensely."
                                }
                            ],
                            "name": "060102"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 64.0,
                                    "institution": "monash.edu",
                                    "name": "AutoRickshaw",
                                    "usagePatterns": "The facility in Hamburg has been very successful with more than 1500 protein structures solved since 2005  representing more than 1000 different users and 300 labs. We expect that at least every user of the MX beamlines would like to use access this service to some extend, which would result in a minimum of a couple of hundred users per annum.  We would like to run at least 64 (virtual) cores. While the worker nodes do not need to have a lot of disks pace (< 50 GB), the webserver could do with around 500 GB. Our first estimate would be: 1 x 16 cores, 64 GB RAM, 500 GB disk 3 x 16 cores, 64 GB RAM, 50 GB disk All instances would run CentOS 6.x. ",
                                    "useCase": "Webservice with PBS backend offering automated crystal structure determination from X-ray diffraction images using the Auto-Rickshaw software package. The service is part of our NeCTAR eResearch Tools project (RT017).  Auto-Rickshaw makes use of publicly available macromolecular crystallography (MX) software and is based on several distinct computer-coded decision-makers for a number of phasing protocols (MAD, MR and variations thereof). At the moment, Auto-Rickshaw is only accessible at EMBL-Hamburg but the resources are limited and Australian researchers have to compete with the entire international structural biology community. The proposed service will offer the same functionality to users of the MX beamlimes of the Australian Synchrotron and any other Australian researcher in this field.  "
                                }
                            ],
                            "name": "060112"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 48.0,
                                    "institution": "monash.edu",
                                    "name": "CellDesigner-Workshop",
                                    "usagePatterns": "This project will have many users and small datasets.",
                                    "useCase": "The cloud instances will be used for a planned systems biology software 2-day workshop in May 7-8, 2013. The software that will be running on the instances is called CellDesigner, a tool for modelling and simulation of biochemical and gene regulatory networks developed by the Systems Biology Institute in Japan. Each workshop participant will be provided remote desktop access to a single cloud instance where they can perform their own modelling and simulations. The workshop is sponsored by defence science institute, state government of Victoria, EMBL Australia, Australian Bioinformatics Network, Monash University and Australian Regenerative Medicine Institute. "
                                },
                                {
                                    "coreQuota": 2.7,
                                    "instanceQuota": 2.7,
                                    "institution": "mq.edu.au",
                                    "name": "UniCarbKB",
                                    "usagePatterns": "in the first instance a small number of users will utilize this resource, however, large datasets will be submitted.",
                                    "useCase": "This allocation will serve to test the performance and suitability of a proteomics data upload and sharing platform (http://www.proteios.org/) for UniCarbKB. The effort will mirror efforts by our Swedish partner to provide the community with suitable data sharing and processing capabilities. "
                                }
                            ],
                            "name": "060101"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "monash.edu",
                                    "name": "Monash_WilceLab-vec_hsqc",
                                    "usagePatterns": "These will be used for data warehousing, and in addition the accumulation of processed and testing data.",
                                    "useCase": "I am a Post-Doctoral Researcher with the Wilce group in the Department of Biochemistry.  This group is focused on protein structure and function, primarily using X-ray crystallography and SAXS data.  In addition to my work within the Wilce group I have a continuing collaboration with the Scanlon group at the Department of Medicinal Chemistry (Monash Parkville Campus).  Our shared interest is in the rapid assignment of two dimensional Nuclear Magnetic Resonance (NMR) data.  Such data is routinely used at multiple stages in Drug Discovery, for the screening of ligands, estimation of ligand binding affinity and also to infer binding site information.  I am building a machine learning algorithm to automate the assignment of NMR data.  I seek access to cluster facilities in order to obtain a performance advantage for the underlying relational database and large-scale linear algebra and statistical operations I require.  Multiple processors and access to large memory reserves could enhance rapid performance testing and code optimistation.  In addition, access to 30 GB of disk storage would be advantageous for storing of raw data and test data for the various stages of code optimisation. The primary aim is for this work is to be the central theme of a publication in a peer-reviewed journal.  A secondary aim is to create a repository of manually assigned NMR data for further data mining / feature extraction, improved accuracy in the estimation of statistical parameters, and tuning the predictive algorithm for protein subclasses.  In addition, the work will have industry relevance for commercial drug discovery.  "
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "csiro.au",
                                    "name": "NeCTAR Image Processing and Analysis Toolkit",
                                    "usagePatterns": "",
                                    "useCase": "This is the storage for public installation of the image processing toolkit (CISIRO-CIAP) using Galaxy and Cloudman. For the time being we have sufficient allocation for compute, but we also need a  block storage for persistent storage of datasets."
                                }
                            ],
                            "name": "0601"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "monash.edu",
                                    "name": "The Proteome Browser",
                                    "usagePatterns": "around 100 users / month and 5G datasets growth / year",
                                    "useCase": "The Proteome Browser was established to assist in the Chromosome-centric Human Proteome Project (C-HPP) of the Human Proteome Organisation (HUPO), particularly the Australia/New Zealand Chromosome 7 project, and hence has both national and international significance by providing the baseline for that project.  It is also an integral part of a current application for an ARC Centre of Excellence in Human Proteomics. It integrates several distinct data sources into unique hierarchical data types to assist in identifying areas of missing information in the human proteome and also rapidly address high level research questions.  As it is continually updated from the source data, it is a valuable resource to monitor the progress of the C-HPP and provides an overview of the quality of data available from human proteins. The virtual resources will be used to host the this web portal."
                                },
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 10.0,
                                    "institution": "monash.edu",
                                    "name": "Monash-Galaxy-Prod",
                                    "usagePatterns": "The project is likely to have 10 - 50 users and some users will use more than 500GB of Volume space. May even use one TB.",
                                    "useCase": "This allocation will be for the Monash Galaxy Production server. My other allocation is being used for  training users and post grad students in the use of Galaxy and bioinformatics analyses. Once trained, the users will be moved onto this allocation for their work. It is envisaged that this Galaxy server will be used by researchers from SOBS staff but will be open to anyone from the University."
                                },
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 1.5,
                                    "institution": "monash.edu",
                                    "name": "Monash-Galaxy-Prod",
                                    "usagePatterns": "The project is likely to have 10 - 50 users and some users will use more than 500GB of Volume space. May even use one TB.",
                                    "useCase": "This allocation will be for the Monash Galaxy Production server. My other allocation is being used for  training users and post grad students in the use of Galaxy and bioinformatics analyses. Once trained, the users will be moved onto this allocation for their work. It is envisaged that this Galaxy server will be used by researchers from SOBS staff but will be open to anyone from the University."
                                }
                            ],
                            "name": "060109"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "unsw.edu.au",
                                    "name": "NGS of type   ",
                                    "usagePatterns": "Single user, small data sets.",
                                    "useCase": "The immediate use will be analysing RNAseq data of cancer patients.  Single user will use the system. The data will be first quality-analysed, then downstream analyses including differential expression analysis will be conducted, and finally based on the analyses, inferences will be made.      "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Stemformatics",
                                    "usagePatterns": "Because we have users and collaborators from around the world the VM needs to be running 24/7 with scheduled downtime usually run on the Sunday. We have a small number of users that will increase rapidly and a small number of datasets that will continue to rise. This is due to the international collaborations we continue to acquire. We have less than 100 users and we currently need around 240GB of data with some relatively basic processing power (4 cores, 32GB RAM) . I put in for the extra extra large as it wouldn't hurt and our requirements may change. Total datasets stored is 100 microarray datasets with more coming in the near future. ",
                                    "useCase": "Web hosting for the Stemformatics Portal. The Stemformatics Portal provides stem cell biologists from around the world the ability to visually analyse and explore public and private stem cell datasets. We want to be able to use this VM as our new Production site."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_Stemformatics",
                                    "usagePatterns": "Because we have users and collaborators from around the world the VM needs to be running 24/7 with scheduled downtime usually run on the Sunday. We have a small number of users that will increase rapidly and a small number of datasets that will continue to rise. This is due to the international collaborations we continue to acquire. We have less than 100 users and we currently need around 240GB of data with some relatively basic processing power (4 cores, 32GB RAM) . I put in for the extra extra large as it wouldn&#39;t hurt and our requirements may change. Total datasets stored is 100 microarray datasets with more coming in the near future. I don't know how many core hours you are asking for? It will be up 24/7",
                                    "useCase": "Web hosting for the Stemformatics Portal. The Stemformatics Portal provides stem cell biologists from around the world the ability to visually analyse and explore public and private stem cell datasets. We want to be able to use this VM as our new Production site."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "Stemformatics genomics computation",
                                    "usagePatterns": "Small number of technicians running Stemformatics data processing and annotation pipelines, with increasingly large datasets involved - current bottleneck in RNASeq and ChIPSeq processing preventing us from ramping up inclusion of these datasets in Stemformatics but if we can secure this compute server the larger datasets would start to become the norm moving forward. Typical size on disk of RNASeq dataset in the range of 200 or 300GB. At current demand, we'd be processing at least 2 of these a month and once our tools are updated to fully automate processing, this number could increase substantially. The 5TB persistent storage requested is about 80% of what we aim to use in the first 12 months of having this VM. Hard to project more than 12 months from now as it really depends on our available human resources moving forward. Object storage - not sure how we'd use that, so not requesting any right now although this may change in future as need arises.",
                                    "useCase": "The Stemformatics project is seeking to secure a compute / data processing virtual machine to meet its genomic data processing needs. Currently, we do not have server capacity through any available platform we use to streamline our data annotation and processing pipelines as we must manually use the Barrine HPC at UQ to process our RNASeq and other high throughput genomic data. Since we do not have a \"bridge\" into Barrine from our data processing server(s) it requires a lot of manual intervention, not to mention time waiting in queues for jobs to run. As we are ramping up inclusion of high throughout sequencing genomic data in Stemformatics we require a server that allows us to run our tools on the one machine without requiring external interfaces to remote HPCs. The Stemformatics project would greatly benefit from the reduced technical overhead in no longer having to manually deal with data transfer, job queues and debugging compute jobs on these remote HPC systems. Have requested 2 x 16 core nodes but we'd REALLY be wanting a single node with 32 cores if possible - otherwise we'd potentially have to split our processing across 2 servers which would go some way towards defeating the point as we may as well be using Barrine if we have to micro-manage data and job partitioning anyway, although I guess beggars can't be choosers!"
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "Stemformatics genomics computation",
                                    "usagePatterns": "Small number of technicians running Stemformatics data processing and annotation pipelines, with increasingly large datasets involved - current bottleneck in RNASeq and ChIPSeq processing preventing us from ramping up inclusion of these datasets in Stemformatics but if we can secure this compute server the larger datasets would start to become the norm moving forward. Typical size on disk of RNASeq dataset in the range of 200 or 300GB. At current demand, we'd be processing at least 2 of these a month and once our tools are updated to fully automate processing, this number could increase substantially. The 5TB persistent storage requested is about 80% of what we aim to use in the first 12 months of having this VM. Hard to project more than 12 months from now as it really depends on our available human resources moving forward. Object storage - not sure how we'd use that, so not requesting any right now although this may change in future as need arises.",
                                    "useCase": "The Stemformatics project is seeking to secure a compute / data processing virtual machine to meet its genomic data processing needs. Currently, we do not have server capacity through any available platform we use to streamline our data annotation and processing pipelines as we must manually use the Barrine HPC at UQ to process our RNASeq and other high throughput genomic data. Since we do not have a \"bridge\" into Barrine from our data processing server(s) it requires a lot of manual intervention, not to mention time waiting in queues for jobs to run. As we are ramping up inclusion of high throughout sequencing genomic data in Stemformatics we require a server that allows us to run our tools on the one machine without requiring external interfaces to remote HPCs. The Stemformatics project would greatly benefit from the reduced technical overhead in no longer having to manually deal with data transfer, job queues and debugging compute jobs on these remote HPC systems. Have requested 2 x 16 core nodes but we'd REALLY be wanting a single node with 32 cores if possible - otherwise we'd potentially have to split our processing across 2 servers which would go some way towards defeating the point as we may as well be using Barrine if we have to micro-manage data and job partitioning anyway, although I guess beggars can't be choosers!"
                                }
                            ],
                            "name": "060114"
                        }
                    ],
                    "name": "0601"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "murdoch.edu.au",
                                    "name": "centre for comparative genomics",
                                    "usagePatterns": "Typically small user base for applications. This initial allocation request is not using large datasets. (Depending on your definition of large of course).",
                                    "useCase": "genomics, staging area for software development projects, workshops at eResearch 2012.  BTW not really sure how to respond to the core hours question. Over what time frame is that for? Per day/week/year or the lifetime of the project?"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Lowe Lab Group",
                                    "usagePatterns": "Several vm instances are envisaged per genetic simulation calculations. Some of these applications can take several hours to produce an output. There are currently about 4 genetics computational applications and each will run on its own instance to get the power of each vm resources. Also students should be able to log concurrently on the same instance and use the application on that instance and not override one another's data (but use the same application) ",
                                    "useCase": "The Phd students will be running DNA genetics simulation and computation. It is envisage that from time to time they might want to store their result data in swift."
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_PeterMac_Bioinformatics_Pipelines",
                                    "usagePatterns": "Around 4 users. Very large data sets (next-generation sequencing data). ",
                                    "useCase": "The cloud instances will be used to analyse cancer genomics data generated by Next Generation Sequencers.  Recent technological advances in cancer research have led to substantial development in bioinformatics tools and methods. We have built VMs that hold the best-practice analysis pipelines. The VMs will be used by both internal users (under this project) and external users (who will launch our images under other projects). I hope you have the capacity to provide the required support. "
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 1.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_PeterMac_Bioinformatics_Pipelines",
                                    "usagePatterns": "Around 4 users. Very large data sets (next-generation sequencing data). ",
                                    "useCase": "The cloud instances will be used to analyse cancer genomics data generated by Next Generation Sequencers.  Recent technological advances in cancer research have led to substantial development in bioinformatics tools and methods. We have built VMs that hold the best-practice analysis pipelines. The VMs will be used by both internal users (under this project) and external users (who will launch our images under other projects). I hope you have the capacity to provide the required support. "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. Most of the resource (the extra we are requesting) will be for a trial deployment of a dynamic torque cluster on the SA node to augment shared HPC (cloudbursting). "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 1.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. We need some group resource to do this until the SA cloud node is in production. "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. We need some group resource to do this until the SA cloud node is in production. "
                                }
                            ],
                            "name": "0604"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 340.0,
                                    "instanceQuota": 340.0,
                                    "institution": "bioplatforms.com",
                                    "name": "Bioplatforms-Au-NGS-training-course",
                                    "usagePatterns": " 3 day workshop over which there will be small data sets that 25 participants will be analyzing. geographic_requirements: ",
                                    "useCase": "We would like to use cloud instances to run a national 3 day Next Generation Sequence workshop. It will involve analysis of small data sets. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_genes4all_db",
                                    "usagePatterns": "Both this project and genes4all_client require the servers online 24/7 for at least 1-2 years. We do not expect any heavy load. This server will probably only be accessible by other nectar machines (and CSIRO), not the general public It is unclear to us what you mean by number of core hours: do you mean CPU time (i.e. the CPU is actually being used) or do you mean the instance is online? ",
                                    "useCase": "This allocation request is linked to a previous one (genes4all_clients). It will provide for the database server we need. Please see use case of genes4all_client "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "intersect.org.au",
                                    "name": "Intersect Demonstrations",
                                    "usagePatterns": "Intermittent use during demonstrations/ trials. ",
                                    "useCase": "Intersect has a variety of software packages that have been developed using federal funding and are freely available to Australian researchers. We would like to install example instances of this software so researchers may trial the software easily before deciding to install their own instance. These two packages are Genomic Data Analysis and HIEv Environmental Data Management."
                                },
                                {
                                    "coreQuota": 272.0,
                                    "instanceQuota": 272.0,
                                    "institution": "bioplatforms.com",
                                    "name": "Bioplatforms-Au-NGS-training-course",
                                    "usagePatterns": "We will rune two 3-day workshops where 35-45 participants will be analyzing small data sets using a variety of bioinformatics software tools.",
                                    "useCase": "We would like to use cloud instances to run a national 3 day Next Generation Sequencing workshop. It will involve analysis of small data sets.   3 day workshop over which there will be small data sets that 35-45 participants will be analyzing"
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "NextGen sequencing data analysis for plants",
                                    "usagePatterns": "Very large datasets with several users concurrently using the data for analysis on the CLC genomics server",
                                    "useCase": "Computing to run CLC genomics server software which will link up with HPC nodes at central computing at UQ"
                                },
                                {
                                    "coreQuota": 100.0,
                                    "instanceQuota": 100.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "GenomicsVL Students",
                                    "usagePatterns": "The primary use case is this: genomics end-users (postdocs and students) will start their own GVLs, each on a med instance, and add one Xlarge instance as a worker node. Each GVL would be expected to use perhaps 200GB of storage each (100GB for reference data and 100GB for tools and user data). These per-student GVLs will initially be started in and used for tutorials, and then remain in the student's 'possession' for such time until they make their own allocation request to NeCTAR. This will allow for students to complete all tutorials, become familiar with the GVL, start their own analyses and understand the resource requirements for a working GVL. Users will be encouraged to store data and analyses in persistent volumes that can then be remounted for subsequent use. The pool of 200 cores will allow us to conduct tutorials with 20 (med + x.large = 2+8 = 10 cores each) GVLs concurrently (as for a tutorial with 20 students). We are not completely sure how students will continue to use their GVL instances, so this will also be something of a pilot; clearly we cannot support all 20 GVLs (=200 cores) persistently, so we will need to manage this, perhaps by limiting the time that the student GVLs are 'alive' to a month, in which time students need to request their own allocation and restart their GVL instance using that; this will free up cores in the GenomicsVL-student pool for the next tutorial. In this way we hope to expose many end-users to the GVL, and encourage a subset to deploy and manage their own, perhaps on behalf of several users in their research group, for instance. ",
                                    "useCase": "This project will provide for per-student GenomicsVL instances. The specific use case is detailed below."
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 2.5,
                                    "institution": "uq.edu.au",
                                    "name": "Plants-Seq-UQ",
                                    "usagePatterns": "3/12/2013 - Ten or more people will be undertaking bioinformatics analysis of data spread across at least 10 projects. At present we have one node on the Nectar allocation presently used for management and processing our data. The data is on Q-cloud (presently about 25 TB) and we use this sequence data (in 10s of GB at a time) for data mining of million of sequence reads, mapping sequence, association genetics and statistical analysis. These processes require several cores and at least 64 or more of RAM per node. We have one node at present and is meaningless for our group as far as the number of projects, the number of people and the timelines of these project. Thus we request cores and RAM equating to several nodes for the Henry group. The nodes requested will be used on a regular basis for the analysis. In addition, the present workload will increase based on our recently funded projecst and additional work we have planned. ",
                                    "useCase": "This is an extension to the original UQ-Plants-Seq request. We will be using the CLC genomics server-software to drive our bioinformatics analysis processes. One VM of 16 cores will be used as the Management Node to drive the CLC-Genomics Server-software. The other VMs will be used as nodes to carry out the bioinformatics processing. 3/12/2013 - We have at present 10 genomics projects and the bioinformatic analysis would require at least a minimum of five nodes for us to make progress in our work leading to publication in a timely manner and for students to complete their work as well. "
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics Compute",
                                    "usagePatterns": "The instance will be used for short periods of 1-3 weeks by the applicant - Kevin Gillinder. And will be purely for data analysis. Analysed files will be then transferred to our Cancer Genomics project for longer-term storage and visualisation on genome browsers.",
                                    "useCase": "We are a genomics laboratory requiring compute to perform bioinfomatic analysis of our sequencing data collections. This application is for a VM to run intermittently and perform compute intensive tasks, for example mapping sequencing reads to the human and mouse genomes."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Genomics",
                                    "usagePatterns": "",
                                    "useCase": "Genomics and Bioinformatics Research - data management/access front end"
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_Menzies",
                                    "usagePatterns": "instances used for galaxy instances to conduct training and data processing ",
                                    "useCase": "Can this request please be expidited due to the intended use being a as a tool used by a short course being conducted in early september. Testing for Galaxy cloud install. Proof of concept. Short course Education resource. Research processing of genomic data via galaxy. "
                                },
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 48.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_LeukoSeq",
                                    "usagePatterns": "Primarily will require one VM running as a Webserver 24/7. The webserver will host a custom portal for accessing clinical genomics results by our research team and clinical collaborators around the world.  07/11/13: Amended to add ability to use up to 4 XXL nodes for data analysis. This is required to enable full migration from pQERN/QERN where compute is currently based.",
                                    "useCase": "Leukodystrophies are inherited diseases that result from the pathological reduction of myelin, the dielectric material that insulates axons in the human brain. Children suffering from a leukodystrophy have substantial morbidity and mortality, with more than a third dying by age eight. Unfortunately, despite the fact that the incidence of these diseases is relatively high (at least 1 in 7,000 births), more than half of all leukodystrophy patients are never fully diagnosed  i.e. the inherited mutation at the root of their affliction remains unknown. In this research project we are harnessing the power next generation sequencing technology to identify novel gene variants that cause leukodystrophy, the first step required to develop treatments for this disease. To achieve this we are currently in the process sequencing the genomes and/or exomes (i.e. the subset of the genome that contains protein coding genes) of approximately 300 children affected by these illnesses and, where possible, their unaffected family members. In total we will be sequencing the exomes of over 1000 individuals in the next 12 months.  "
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-Genes4All",
                                    "usagePatterns": "Both this project and genes4all_client require the servers online 24/7 for at least 1-2 years. We do not expect any heavy load. This server will probably only be accessible by other nectar machines (and CSIRO).",
                                    "useCase": "Modern genomics has enabled a large number of researchers to produce vast amounts of data for their favourite organism. This is especially true for species of industrial and/or ecological importance (instead of the classic lab models: mouse, human, fruitfly etc). The bottleneck is not the processing of the data (dealt via HPC) but the dissemination and visualization in a standardized, pain-free format. Further, curation and interactivity allows the web users to edit and curate the metadata in a crowdsourcing approach. Our work has produced such a system, written in PHP (Drupal), JavaScript (ExtJS and JQuery) and some perl code. We are now seeking to deploy it as domain-specific web-servers (e.g. only for specific species or projects). Each instance will host a webserver that will serve domain-specific data in a common GUI. Users will be using port 80 to access the interface. Administrators (one or two individuals) will be responsible for maintaining the web server but we don't expect frequent updates. Data will be derived from a database server located elsewhere on the NECTaR cloud and/or within CSIRO. An example can be seen at http://insectacentral.org. Previous work has been cited 38 times since 2008 (DOI: 10.1093/nar/gkm853). I'm more than happy to provide more information if this is required. "
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "uq.edu.au",
                                    "name": "Marine Genomics: sea sponge and other invertebrate genomic resources for the Great Barrier Reef ",
                                    "usagePatterns": "We request the use of 2 XXL instances (32 cores total) hosted in Queensland when possible.  We will have < 50 researchers [primarily UQ, with usage from Univ. Sunshine Coast and Australian Institute of Marine Science (http://aims.gov.au/) and international collaborators (current collaborators include Okinawa Institute of Science and Technology, Japan; UC Santa Barbara; UC Berkeley, USA; Univ of Vienna, Austria; Technion Univ, Israel), potentially extending to other Queensland, Australian and international universities, institutes and industry in the future) using the computing nodes on a daily basis. Potentially > 10,000 users accessing our public browser from a web interface over the course of the whole project (not simultaneously).  The public browser will be hosted in an independent instance (Large) to enable simplified management.  We plan to keep images of our VMs and crucial data shared by all projects in the Object Storage. We are currently applying for storage space on the QCIF RDSI Node to mount on our VMs in order to process and generate our data collections, and so require our VMs on the QCIF Research Cloud Node. We plan to only use local storage for high I/O processes. We generally use few files >10GB and >10,000s of small files (<1GB). ",
                                    "useCase": "These cores will service primarily bioinformatics analyses in the Marine Genomics Laboratory at UQ. This laboratory is located in the School of Biological Sciences and Centre for Marine Science and is comprised on two research groups, the Sandie Degnan Lab and the Bernie Degnan Lab. There are 20+ researchers and postgraduate students in these labs, most of which have bioinformatics intensive projects. Most of the research is focussed on the analysis of the structure, function and organisation of the genome of the model marine sponge Amphimedon queenslandica and its bacterial symbionts.  This is one the premier models to understand the evolution and ecology of animals at the genomic level and the first, and still only, marine animal in Australia to have it genome sequenced and published.  Recent research outcomes from this genome project have been published in Science (IF 31.0), Nature (IF 38.6), Proc Natl Acad Sci USA (IF 9.7), PLoS Biol (IF 12.7), Current Biology (IF 9.5), Nature Microbiol Rev (IF 22.5), Nature Struct Mol Biol (IF 11.9) and Mol Biol Evol (IF 10.4).  The addition of other marine genomes and transcriptomes (e.g. the pearl oyster Pinctada maxima and the starfish Crown of Thorns Acanthaster planci) into this data collection enable collaborations with local universities and institutes and with Australian industry partners, in addition to a large diversity of international partners.  Each of these interactions provide unique platforms to expand the use of genomic information in marine science, particularly in tropical Australia. We require the capacity to undertake de novo genome, transcriptome and metagenome assembly, sequence annotation, database querying as well as host a private as well as publicly available genome browser that enables us, our collaborators and other researchers across the globe to interact with the genomic information we are generating.  "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQTRC Research 1",
                                    "usagePatterns": "Few users; infrequent access; large datasets",
                                    "useCase": "Galaxy pipeline for WGS and exome alignments and analysis"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_hmC_methylation",
                                    "usagePatterns": "The project will have large datasets but only a maximum of 5 users",
                                    "useCase": "We have a sequencing project between two departments that require a large amount of storage room for sequencing datasets. A student and myself will be using it to analyse DNA methylation states. The student is split between multiple campuses and has little experience with linux, so therefore an appropriately resourced virtual machine will help him immensely."
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 1.6,
                                    "institution": "unisa.edu.au",
                                    "name": "UniSA_Cystic_Fibrosis_Genetic_Therapy_Community_Engagement",
                                    "usagePatterns": "1 dataset (the animation), 1 instance with one user.",
                                    "useCase": "We need to render a short (~3min) educational clip intended for distribution over the internet. The goal is to explain in easy to understand terms what research is being conducted at the Women's and Children's Hospital towards a genetic therapy cure for Cystic Fibrosis. In this way we hope to educate the general public in an easily accessible format which will garner greater public support for our work, and hopefully increased donations to the Cure4CF foundation that provides some of our funding."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more object and storage space."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more volume storage space."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Genomics",
                                    "usagePatterns": "",
                                    "useCase": "Genomics and Bioinformatics Research - data management/access front end"
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 4.0,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_Menzies",
                                    "usagePatterns": "instances used for galaxy instances to conduct training and data processing ",
                                    "useCase": "Can this request please be expidited due to the intended use being a as a tool used by a short course being conducted in early september. Testing for Galaxy cloud install. Proof of concept. Short course Education resource. Research processing of genomic data via galaxy. "
                                },
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 4.8,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_LeukoSeq",
                                    "usagePatterns": "Primarily will require one VM running as a Webserver 24/7. The webserver will host a custom portal for accessing clinical genomics results by our research team and clinical collaborators around the world.  07/11/13: Amended to add ability to use up to 4 XXL nodes for data analysis. This is required to enable full migration from pQERN/QERN where compute is currently based.",
                                    "useCase": "Leukodystrophies are inherited diseases that result from the pathological reduction of myelin, the dielectric material that insulates axons in the human brain. Children suffering from a leukodystrophy have substantial morbidity and mortality, with more than a third dying by age eight. Unfortunately, despite the fact that the incidence of these diseases is relatively high (at least 1 in 7,000 births), more than half of all leukodystrophy patients are never fully diagnosed  i.e. the inherited mutation at the root of their affliction remains unknown. In this research project we are harnessing the power next generation sequencing technology to identify novel gene variants that cause leukodystrophy, the first step required to develop treatments for this disease. To achieve this we are currently in the process sequencing the genomes and/or exomes (i.e. the subset of the genome that contains protein coding genes) of approximately 300 children affected by these illnesses and, where possible, their unaffected family members. In total we will be sequencing the exomes of over 1000 individuals in the next 12 months.  "
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "Marine Genomics: sea sponge and other invertebrate genomic resources for the Great Barrier Reef ",
                                    "usagePatterns": "We request the use of 2 XXL instances (32 cores total) hosted in Queensland when possible.  We will have < 50 researchers [primarily UQ, with usage from Univ. Sunshine Coast and Australian Institute of Marine Science (http://aims.gov.au/) and international collaborators (current collaborators include Okinawa Institute of Science and Technology, Japan; UC Santa Barbara; UC Berkeley, USA; Univ of Vienna, Austria; Technion Univ, Israel), potentially extending to other Queensland, Australian and international universities, institutes and industry in the future) using the computing nodes on a daily basis. Potentially > 10,000 users accessing our public browser from a web interface over the course of the whole project (not simultaneously).  The public browser will be hosted in an independent instance (Large) to enable simplified management.  We plan to keep images of our VMs and crucial data shared by all projects in the Object Storage. We are currently applying for storage space on the QCIF RDSI Node to mount on our VMs in order to process and generate our data collections, and so require our VMs on the QCIF Research Cloud Node. We plan to only use local storage for high I/O processes. We generally use few files >10GB and >10,000s of small files (<1GB). ",
                                    "useCase": "These cores will service primarily bioinformatics analyses in the Marine Genomics Laboratory at UQ. This laboratory is located in the School of Biological Sciences and Centre for Marine Science and is comprised on two research groups, the Sandie Degnan Lab and the Bernie Degnan Lab. There are 20+ researchers and postgraduate students in these labs, most of which have bioinformatics intensive projects. Most of the research is focussed on the analysis of the structure, function and organisation of the genome of the model marine sponge Amphimedon queenslandica and its bacterial symbionts.  This is one the premier models to understand the evolution and ecology of animals at the genomic level and the first, and still only, marine animal in Australia to have it genome sequenced and published.  Recent research outcomes from this genome project have been published in Science (IF 31.0), Nature (IF 38.6), Proc Natl Acad Sci USA (IF 9.7), PLoS Biol (IF 12.7), Current Biology (IF 9.5), Nature Microbiol Rev (IF 22.5), Nature Struct Mol Biol (IF 11.9) and Mol Biol Evol (IF 10.4).  The addition of other marine genomes and transcriptomes (e.g. the pearl oyster Pinctada maxima and the starfish Crown of Thorns Acanthaster planci) into this data collection enable collaborations with local universities and institutes and with Australian industry partners, in addition to a large diversity of international partners.  Each of these interactions provide unique platforms to expand the use of genomic information in marine science, particularly in tropical Australia. We require the capacity to undertake de novo genome, transcriptome and metagenome assembly, sequence annotation, database querying as well as host a private as well as publicly available genome browser that enables us, our collaborators and other researchers across the globe to interact with the genomic information we are generating.  "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more volume storage space."
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "Plants-Seq-UQ",
                                    "usagePatterns": "3/12/2013 - Ten or more people will be undertaking bioinformatics analysis of data spread across at least 10 projects. At present we have one node on the Nectar allocation presently used for management and processing our data. The data is on Q-cloud (presently about 25 TB) and we use this sequence data (in 10s of GB at a time) for data mining of million of sequence reads, mapping sequence, association genetics and statistical analysis. These processes require several cores and at least 64 or more of RAM per node. We have one node at present and is meaningless for our group as far as the number of projects, the number of people and the timelines of these project. Thus we request cores and RAM equating to several nodes for the Henry group. The nodes requested will be used on a regular basis for the analysis. In addition, the present workload will increase based on our recently funded projecst and additional work we have planned. ",
                                    "useCase": "This is an extension to the original UQ-Plants-Seq request. We will be using the CLC genomics server-software to drive our bioinformatics analysis processes. One VM of 16 cores will be used as the Management Node to drive the CLC-Genomics Server-software. The other VMs will be used as nodes to carry out the bioinformatics processing. 3/12/2013 - We have at present 10 genomics projects and the bioinformatic analysis would require at least a minimum of five nodes for us to make progress in our work leading to publication in a timely manner and for students to complete their work as well. "
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "uq.edu.au",
                                    "name": "Plants-Seq-UQ",
                                    "usagePatterns": "3/12/2013 - Ten or more people will be undertaking bioinformatics analysis of data spread across at least 10 projects. At present we have one node on the Nectar allocation presently used for management and processing our data. The data is on Q-cloud (presently about 25 TB) and we use this sequence data (in 10s of GB at a time) for data mining of million of sequence reads, mapping sequence, association genetics and statistical analysis. These processes require several cores and at least 64 or more of RAM per node. We have one node at present and is meaningless for our group as far as the number of projects, the number of people and the timelines of these project. Thus we request cores and RAM equating to several nodes for the Henry group. The nodes requested will be used on a regular basis for the analysis. In addition, the present workload will increase based on our recently funded projecst and additional work we have planned. ",
                                    "useCase": "This is an extension to the original UQ-Plants-Seq request. We will be using the CLC genomics server-software to drive our bioinformatics analysis processes. One VM of 16 cores will be used as the Management Node to drive the CLC-Genomics Server-software. The other VMs will be used as nodes to carry out the bioinformatics processing. 3/12/2013 - We have at present 10 genomics projects and the bioinformatic analysis would require at least a minimum of five nodes for us to make progress in our work leading to publication in a timely manner and for students to complete their work as well. "
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Genomic basis of Adaptation",
                                    "usagePatterns": "This project will have between 3-4 users. 1 of the users will have very large data sets, 1 user with large datasets; both using computationally intensive analyses. The remaining users will have small data sets.  I expect to have a maximum of 2 VMs with 16 cores running most of the time. These VM's will serve as servers and portable workstations. ",
                                    "useCase": "This project aims to identify genomic characteristics of species with varying capacities to adapt to climate change, test the generality of these signatures across multiple groups and environments and to build this knowledge into predictive models of biodiversity response to climate change.  Within the scope of the project the following activities will be carried out:  1. Whole genome assembly of 2+ Drosophila species 2. Multi-species comparative-genomics across Drosophila species-groups using NGS (Next-generation sequencing) data for >50 samples per population per species for 5 species. 3. Variant analysis in comparative-genomics study to track population differentiation and linkage decay across and between species of the same species-group. 4. Construct high density SNP maps for Drosophila species and construct detailed phylogenies for the same. 5. Construct high density linkage disequilibrium maps across entire chromosomes using pair-wise analysis algorithms with >100,000 datapoints per chromosome. 6. Construct haploid genomes using NGS data from familially related individuals. "
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "adelaide.edu.au",
                                    "name": "Lowe Lab Group",
                                    "usagePatterns": "Several vm instances are envisaged per genetic simulation calculations. Some of these applications can take several hours to produce an output. There are currently about 4 genetics computational applications and each will run on its own instance to get the power of each vm resources. Also students should be able to log concurrently on the same instance and use the application on that instance and not override one another's data (but use the same application) ",
                                    "useCase": "The Phd students will be running DNA genetics simulation and computation. It is envisage that from time to time they might want to store their result data in swift."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "sydney.edu.au",
                                    "name": "GVL-SCF image testing",
                                    "usagePatterns": "Monday to Friday mostly. Sometimes I will have the image creating running overnight. There will be just me using the testing framework.  There is no large datasets required, except for what is needed to create the galaxy base image.",
                                    "useCase": "I am part of the Genome Virtual Laboratory, and I am working on getting the Science Collaboration Framework working within the GVL galaxy VM image.  I have it basically working now, and I want to test it more thoroughly now. I want to optimise the image creation script, so I can run it from one image, and it will install all the necessary prereqs so it can create the image on another VM."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF MiSeq",
                                    "usagePatterns": "Only the metadata and the 'just-processed' raw data will be located in this VM. Expecting there will be max 2T of data to be stored in this VM. End user will constantly accessing the web site to view the metadata via a web page. The number of users should be just a few at a time.",
                                    "useCase": "We have a MiSeq sequencing machine jointly with other Science faculty researchers. The DNA sequencing data generated from the MiSeq will need to be managed and a data server is needed to provide the following capabilities: Distributing the raw data for backup. Perform quality control (QC) and put the metadata to the database in the data server . Provide a web page to list the metadata and allow user to download the raw data. The MiSeq is managed jointly by several groups and the data server will be for managing data belonging to these groups. "
                                },
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "uq.edu.au",
                                    "name": "Hadoop-based Genomic Mapping",
                                    "usagePatterns": "~4 users, Many small datasets. A few moderate datasets ",
                                    "useCase": "With application in genomics research, personalized medicine and diagnostics, the growth of next-generation sequencing (NGS) defies Moore's law. The resulting flood of data has led to the development of bioinformatics tools that can harvest the power of the Hadoop framework. This project aims at demonstrating the advantage of using the NeCTAR cloud infrastructure to run genome mapping in comparison with traditional cluster computing."
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 25.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "SACGF",
                                    "usagePatterns": "We will have only a few users (2-5), we have our own storage, but we require the cloud for worker nodes in a Torque cluster. I have amended the request to include some persistant storage.  While we won't need this in the longer term, it turns out that we will need it in the short term in order to test the setup on sample projects.",
                                    "useCase": "We are a genome sequencing facility and our research is in the area of genomics/bioinformatics.  For our computing we have been using Torque queues both on our own dedicated server as well as eResearch SA servers.  eResearch SA is now moving over to Nectar cloud computing, so we want to implement our workflows/pipelines on a Nectar-based setup.  It is likely that we will implement this using  a dynamic Torque set-up.  The present allocation is designed to allow us to develop, implement and test  this on some sample projects.  If successful, we would then request allocations on a more permanent basis."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "UQ GVL training",
                                    "usagePatterns": "Just one user a and small test datasets. Most use will be in short bursts of launching new VMs, running them for a short while, and then closing them down again.",
                                    "useCase": "I am delivering training in resources developed under the GVL. Since some of these (particularly Galaxy) require volume storage to work, I want this access to test training material from the perspective of a standard user (or at least, one who has requested volume storage) rather than just testing things within the GVL allocation"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "QCMG-TEST1",
                                    "usagePatterns": "Heavy use when active.  Jobs typically run for hours to days.",
                                    "useCase": "Test case for possible migration of QCMG Australian Pancreatic Genome project dataset to RDSI/NECTAR."
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 1.5,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_NGSANE",
                                    "usagePatterns": "Large number of users with small fixed data set (provided toy data). ",
                                    "useCase": "Cluster with storage to showcase our sequence data analysis framework, NGSANE (https://github.com/BauerLab/ngsane), for the bioinformatics publication currently under review. One reviewer requested that our software should be testable without lengthy dependency installation and toy data download."
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 10.0,
                                    "institution": "intersect.org.au",
                                    "name": "Intersect_Snap",
                                    "usagePatterns": "",
                                    "useCase": "We are currently working on the Snap Deploy project which is Nectar funded. We would like additional resources to allow us to test our application with the Nectar Cloud. I cannot select a FOR of research as our application is to allow the automatic deployments of multiple projects from various disciplines. "
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "Advanced methods for genomic analysis in animal breeding ",
                                    "usagePatterns": "It is expected that the project will include the analysis of a  small number of large data sets. These data sets will be from 1-5 GB although intermediate analysis may result in much larger temporary files. It is envisaged that each of these analysis will utilise the resource for between 4 hours and a few day. There will be a small number of users (<5).",
                                    "useCase": "Genomics has revolutionised many aspects of livestock breeding. The analysis of such data often requires Bayesian analysis which runs for long periods or analysis of large datasets that require large computing capacity.  Access to this resource will enable us to more effectively analyse such data, to develop new analysis methods and also to test existing methods and provide results to animal breeders."
                                },
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 3.0,
                                    "institution": "uq.edu.au",
                                    "name": "Hadoop-based Genomic Mapping",
                                    "usagePatterns": "~4 users, Many small datasets. A few moderate datasets ",
                                    "useCase": "With application in genomics research, personalized medicine and diagnostics, the growth of next-generation sequencing (NGS) defies Moore's law. The resulting flood of data has led to the development of bioinformatics tools that can harvest the power of the Hadoop framework. This project aims at demonstrating the advantage of using the NeCTAR cloud infrastructure to run genome mapping in comparison with traditional cluster computing."
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 3.2,
                                    "institution": "adelaide.edu.au",
                                    "name": "SACGF",
                                    "usagePatterns": "We will have only a few users (2-5), we have our own storage, but we require the cloud for worker nodes in a Torque cluster. I have amended the request to include some persistant storage.  While we won't need this in the longer term, it turns out that we will need it in the short term in order to test the setup on sample projects.",
                                    "useCase": "We are a genome sequencing facility and our research is in the area of genomics/bioinformatics.  For our computing we have been using Torque queues both on our own dedicated server as well as eResearch SA servers.  eResearch SA is now moving over to Nectar cloud computing, so we want to implement our workflows/pipelines on a Nectar-based setup.  It is likely that we will implement this using  a dynamic Torque set-up.  The present allocation is designed to allow us to develop, implement and test  this on some sample projects.  If successful, we would then request allocations on a more permanent basis."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_genes4all_clients",
                                    "usagePatterns": "Each instance will be a web-server that will serve data derived from a database server located elsewhere in the cloud (I had to file a separate allocation request).",
                                    "useCase": "Modern genomics has enabled a large number of researchers to produce vast amounts of data for their favourite organism. This is especially true for species of industrial and/or ecological importance (instead of the classic lab models: mouse, human, fruitfly etc). The bottleneck is not the processing of the data (dealt via HPC) but the dissemination and visualization in a standardized, pain-free format. Further, curation and interactivity allows the web users to edit and curate the metadata in a crowdsourcing approach. Our work has produced such a system, written in PHP (Drupal), JavaScript (ExtJS and JQuery) and some perl code. We are now seeking to deploy it as domain-specific web-servers (e.g. only for specific species or projects). Each instance will host a webserver that will serve domain-specific data in a common GUI. Users will be using port 80 to access the interface.  Administrators (one or two individuals) will be responsible for maintaining the web server but we don't expect frequent updates. Data will be derived from a database server located elsewhere on the NECTaR cloud and/or within CSIRO. An example can be seen at http://insectacentral.org. Previous work has been cited 38 times since 2008 (DOI: 10.1093/nar/gkm853). I'm more than happy to provide more information if this is required."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF TAGdb",
                                    "usagePatterns": "The dataset is large, around 10TB, currently. The number of users various, based on the historical usage, it will be around 5~20 users per day.",
                                    "useCase": "The TAGdb is an online database enabling researchers to identify paried read sequences that share identity with a submitted query sequence. These tags can be used to design oligonucleotide primers for the PCR amplication of the region in the target genome. This tool is applicable for gene and promoter discovery in a wide range of species and greatly facilitates comparative genomics and molecular marker discovery in orphan crops or those with large and complex genomes. "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Lambert Ancient DNA",
                                    "usagePatterns": "We will typically have several users (up to 10-12 or so) with large datasets (10s to hundreds of Gb). ",
                                    "useCase": " We are sequencing ancient genomes of a number of species for population-level genomic studies. Included in our efforts are: 1) Antarctic Adelie penguin (Pygoscelis adeliae) samples from up to 30000 years ago, which will be compared to modern Adelie genomes to investigate adaptation to climate change occurring since the last glacial maximum 2) Samples of extinct Moa (Dinornis spp.) from New Zealand, which are analyzed in efforts to understand the evolution of flightlessness, as well as trace the evolutionary relationships amongst extant and extinct ratites 3) Samples of the First Australians, representing the earliest known human remains in Australia (ca. 45,000 ya), the sequencing of which will help us to better understand the origins and characteristics of the first human inhabitants of Australia. Our lab is currently composed our Principal Investigator David Lambert, five post-doctoral scholars and two PhD students. A third PhD student is expected within the next year. It is expected that most of the members of our lab will use the allotted computing space, both for storage of next-generation sequencing data that is currently being generated, and for analyses of this data. In addition, it is likely that several international collaborators will utilize the space for shared data and analyses. "
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "murdoch.edu.au",
                                    "name": "Murdoch_ACCWI",
                                    "usagePatterns": "This server will not store the main data sets of the centre, but may be a temporary storage point as we prepare datasets for submission to larger repositories.",
                                    "useCase": "This server will be used by members of the Australia-China Centre for Wheat Improvement (ACCWI) at Murdoch University for general purpose tasks., such as writing and testing code. It will function as the main server \"hub\" for the centre (which does not have any physical compute infrastructure of its own). Small analysis tasks may also be run by some users. It will also be used as the base for file exchange with collaborators."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Lambert Ancient DNA",
                                    "usagePatterns": "We will typically have several users (up to 10-12 or so) with large datasets (10s to hundreds of Gb). ",
                                    "useCase": " We are sequencing ancient genomes of a number of species for population-level genomic studies. Included in our efforts are: 1) Antarctic Adelie penguin (Pygoscelis adeliae) samples from up to 30000 years ago, which will be compared to modern Adelie genomes to investigate adaptation to climate change occurring since the last glacial maximum 2) Samples of extinct Moa (Dinornis spp.) from New Zealand, which are analyzed in efforts to understand the evolution of flightlessness, as well as trace the evolutionary relationships amongst extant and extinct ratites 3) Samples of the First Australians, representing the earliest known human remains in Australia (ca. 45,000 ya), the sequencing of which will help us to better understand the origins and characteristics of the first human inhabitants of Australia. Our lab is currently composed our Principal Investigator David Lambert, five post-doctoral scholars and two PhD students. A third PhD student is expected within the next year. It is expected that most of the members of our lab will use the allotted computing space, both for storage of next-generation sequencing data that is currently being generated, and for analyses of this data. In addition, it is likely that several international collaborators will utilize the space for shared data and analyses. "
                                }
                            ],
                            "name": "060408"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 60.0,
                                    "instanceQuota": 60.0,
                                    "institution": "monash.edu",
                                    "name": "CharacterisationVL - [Additional resource]",
                                    "usagePatterns": "",
                                    "useCase": "Collaboration research with interstate universities for Characterisation virtual laboratory"
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "RNA-seq of iPS-derived human neurons",
                                    "usagePatterns": "This project will have a small number of users (primarily the main applicant), with large datasets, a relatively small number of big reference files (genome, annotation, mapping indexes) and a relatively small (<=100) number of output files for each sample (~60 samples total planned). The main bottlenecks include mapping RNA-seq reads (using STAR, which is quite fast, but loads the entire genome index into memory (~40Gb for human or mouse, the two species which will be interrogated for this project)) and assembling the reads into transcripts using cufflinks, which uses ~50Gb RAM for prolonged periods of time (up to 2 weeks per dataset)).   ==== Update 12.02.14 -> Have checked reference file requirements (which will be placed into object storage); currently am using 300Gb on barrine. -> In terms of volume storage, have looked at /ebi/bscratch and use ~1-1.5Tb per series of experiments (which is what would be run in parallel at a given time for the project). In order to trial running my analysis on one dataset at a time on the VMs, I would likely need ~150Gb of space; more would, of course, be better. ",
                                    "useCase": "This project focuses on characterising the differentiation of induced pluripotent stem (iPS) cells into neurons.  iPS cells are stem cells made from skin cells, which can be taken from normal people or those afflicted by a certain disease. These stem cells can then be differentiated into other cell types, for example brain cells. These cell types can be used for basic research to replace model animals, and drugs can be tested on these \"brain cells in a dish\", with the most effective drug that alleviates the patient's symptoms \"in a dish\" chosen to be administered to the patient in vivo.  While this is the hope for the application of stem cells in basic research and personalised medicine for drug testing, the reality is that we don't know how similar these neurons in the dish are to the ones we've got in our brains, which is what this project is attempting to find out. By sequencing the RNA of iPS derived stem cells and the neurons we are differentiating from them (at several time points of differentiation) we are attempting to understand what changes occur, at the molecular level, in these cells as they go from being a stem cell to being a neuron, and how similar these changes are to ones we know normally happen during neurological development.  The results of this work will be presented at local and international conferences, and subsequently written up as manuscripts for publication. ==== This research is being partly funded by NHMRC grant 1021005 \"ReprogrammingofAtaxiaTelangiectasiafibroblaststogenerateiPScells\" and APP1043023 \"Investigation of processed snoRNAs as cryptic regulators of the imprinted Prader-Willi syndrome locus\". This work represents a collaboration between bioinformaticians at the IMB, stem cell biologists at AIBN,  and stem cell biologists and clinicians at QIMR. "
                                }
                            ],
                            "name": "060410"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "KNIME Hadoop extension",
                                    "usagePatterns": "Initially, the project will require only a single developer leading to six users with the extension. It will consist of small datasets initially during development and testing, but leading to analysis of Tissue Mass Spec imaging and Next Generation Sequencing datasets with up to 100GB per analysis.",
                                    "useCase": "The Konstanz Information Miner (knime.org) as an Eclipse Rich Client Platform is ready for a hadoop (apache.hadoop.org) extension. The applicant is part of plantcell.unimelb.edu.au, focussed on bioinformatics analysis. Although hadoop images are available on the Nectar cloud, it is for v1 of the software, and I intend to use Hadoop v2 (currently in alpha) as this provides more features As part of this project, we aim to: 1) write an extension to the KNIME platform to make it easy for users to lodge MapReduce programs 2) speed common operations: GroupBy and Join for large datasets using Apache Pig 3) provide data mining extensions using Apache Mahout (eg. SVM) which scale to the cloud 4) authenticate access using LDAP to the UNIMELB domain "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_NGS_Library_Evaluation",
                                    "usagePatterns": "",
                                    "useCase": "Research hypothesis: Barcoded lentiviral vectors offer greater quantitative potential and methodological simplicity for analysing clonal diversity in comparison to conventional integration site analysis. Aims of analysis: (1) To develop a primer extension method for efficiently generating a complex barcoded plasmid library, using the pEF1?.?c lentiviral construct; (2) Investigate the feasibility of NGS technologies for analysing the number and relative abundance of barcodes within a complex barcoded plasmid library;   (3) To evaluate the impact of sequencing error upon the analysis of complex barcoded plasmid libraries, and (4) To define the limit to the degree of library complexity that can be analysed using NGS. "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "utas.edu.au",
                                    "name": "LegumeSeq1",
                                    "usagePatterns": "large data sets and small number of usersLegume",
                                    "useCase": "Analysing DNA Next Generation Sequencing data sets from several legumes transcriptomics experiments."
                                }
                            ],
                            "name": "060405"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "NectarForHumanRates",
                                    "usagePatterns": "Probably only one user (maximum 2), to run ~100 of long (1 to 3 weeks) analyses from relatively small data sets (input ~5Mb each, output ~1Gb)",
                                    "useCase": "Computing requirement to run all replicate analyses and simulations (~ 100 long BEAST phylogenetic runs) before submitting the main publication of a research project (recalibrating the timescale of human evolutionary history using ancient mitochondrial genomes)"
                                },
                                {
                                    "coreQuota": 38.4,
                                    "instanceQuota": 2.4,
                                    "institution": "adelaide.edu.au",
                                    "name": "ACAD",
                                    "usagePatterns": "Cluster in the cloud",
                                    "useCase": "Bayesian Analysis Population genomics Pyhlogenomics/Phylogenetics"
                                }
                            ],
                            "name": "060401"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "StatGenMCRI",
                                    "usagePatterns": "Small number of users (approx. 5), small data sets (mostly code).  Test data (small versions of our main datasets).  Main computation and data storage will use other resource we have access to.",
                                    "useCase": "Host source code repositories (using e.g. GIT) for access by reserch group and external collaborators.  Data sharing.  Running ad hoc web services for research tools (e.g. our custom tools).  We were not sure about how to calculate the number of core hours.  We have just assumes a full year of use of one core with occasional use of a second core."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 0.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "NectarForHumanRates",
                                    "usagePatterns": "Probably only one user (maximum 2), to run ~100 of long (1 to 3 weeks) analyses from relatively small data sets (input ~5Mb each, output ~1Gb)",
                                    "useCase": "Computing requirement to run all replicate analyses and simulations (~ 100 long BEAST phylogenetic runs) before submitting the main publication of a research project (recalibrating the timescale of human evolutionary history using ancient mitochondrial genomes)"
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Genomic basis of Adaptation",
                                    "usagePatterns": "This project will have between 3-4 users. 1 of the users will have very large data sets, 1 user with large datasets; both using computationally intensive analyses. The remaining users will have small data sets.  I expect to have a maximum of 2 VMs with 16 cores running most of the time. These VM's will serve as servers and portable workstations. ",
                                    "useCase": "This project aims to identify genomic characteristics of species with varying capacities to adapt to climate change, test the generality of these signatures across multiple groups and environments and to build this knowledge into predictive models of biodiversity response to climate change.  Within the scope of the project the following activities will be carried out:  1. Whole genome assembly of 2+ Drosophila species 2. Multi-species comparative-genomics across Drosophila species-groups using NGS (Next-generation sequencing) data for >50 samples per population per species for 5 species. 3. Variant analysis in comparative-genomics study to track population differentiation and linkage decay across and between species of the same species-group. 4. Construct high density SNP maps for Drosophila species and construct detailed phylogenies for the same. 5. Construct high density linkage disequilibrium maps across entire chromosomes using pair-wise analysis algorithms with >100,000 datapoints per chromosome. 6. Construct haploid genomes using NGS data from familially related individuals. "
                                }
                            ],
                            "name": "060411"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF TAGdb",
                                    "usagePatterns": "The dataset is large, around 10TB, currently. The number of users various, based on the historical usage, it will be around 5~20 users per day.",
                                    "useCase": "The TAGdb is an online database enabling researchers to identify paried read sequences that share identity with a submitted query sequence. These tags can be used to design oligonucleotide primers for the PCR amplication of the region in the target genome. This tool is applicable for gene and promoter discovery in a wide range of species and greatly facilitates comparative genomics and molecular marker discovery in orphan crops or those with large and complex genomes. "
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Chickpea GBrowse",
                                    "usagePatterns": "For this VM, the data set will not be big but there will be many users.",
                                    "useCase": "The Chickpea genome browsing system is a web-based applications with Apache2, CGI and Bioperl at the backend. The system mainly contains two components: 1.A generic Genome Browser (GBrowse) which has a lot of customisable features. 2.An in-house developed application called GBrowseBlast which allows users to submit a query DNA sequence to search for similar sequences in the genome and visualise the matching region directly in the GBrowse. Besides the frame of the system, the data is equally important. The Chickpea genome in NCBI contains the raw sequences which will be the reference sequence in the GBrowse. Other required genomic features will need to be extracted from the raw sequences and loaded to GBrowse as different tracks for visualisation. The final Chickpea browsing system will contain tracks with important features, such as genes, exons, assembled contigs, reads sequences, restriction sites and GC content. With these capabilities, the Chickpea research community will be able to easily search and visualise the Chickpea genome with the provided genomic features enabled. Users can also query their interested sequences to find homolog sequences in the Chickpea genome that is especially important to the plant research community.  "
                                }
                            ],
                            "name": "060407"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "uq.edu.au",
                                    "name": "NBCF Breast Cancer Project",
                                    "usagePatterns": "The resources will mainly be used for analysis of large genomic datasets.  This typically requires unique reference datafiles that are frequently accessed by our bioinformatic tools; these reference datafile sets can frequently be in excess of 20GB each (e.g., 3 different human genome reference sets, each of which ~20GB each).  Each genome reference set will require persistent storage. The requirement for object storage relates to archiving our patient sequencing datasets, for ongoing access by our national research term.  We currently have an application with QCloud for allocation of storage for this project. ",
                                    "useCase": "We have been funded by the NBCF for 5 years to pursue next-generation diagnostic testing and screening on clinical tumour biopsies from women diagnosed with breast cancer.  The next phase of our project requires extensive next-generation sequencing analysis of these genetic and epigenetic changes, and this analysis will take place over the length of our 5 year project grant.  The data generated will be used collaboratively by multiple users across Australia, with the goal of improving patient outcomes for women currently suffering from disease."
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 0.3,
                                    "institution": "uq.edu.au",
                                    "name": "NBCF Breast Cancer Project",
                                    "usagePatterns": "The resources will mainly be used for analysis of large genomic datasets.  This typically requires unique reference datafiles that are frequently accessed by our bioinformatic tools; these reference datafile sets can frequently be in excess of 20GB each (e.g., 3 different human genome reference sets, each of which ~20GB each).  Each genome reference set will require persistent storage. The requirement for object storage relates to archiving our patient sequencing datasets, for ongoing access by our national research term.  We currently have an application with QCloud for allocation of storage for this project. ",
                                    "useCase": "We have been funded by the NBCF for 5 years to pursue next-generation diagnostic testing and screening on clinical tumour biopsies from women diagnosed with breast cancer.  The next phase of our project requires extensive next-generation sequencing analysis of these genetic and epigenetic changes, and this analysis will take place over the length of our 5 year project grant.  The data generated will be used collaboratively by multiple users across Australia, with the goal of improving patient outcomes for women currently suffering from disease."
                                }
                            ],
                            "name": "060404"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 1.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "ACAD",
                                    "usagePatterns": "Cluster in the cloud",
                                    "useCase": "Bayesian Analysis Population genomics Pyhlogenomics/Phylogenetics"
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uq.edu.au",
                                    "name": "Marine Genomics: sea sponge and other invertebrate genomic resources for the Great Barrier Reef ",
                                    "usagePatterns": "We request the use of 2 XXL instances (32 cores total) hosted in Queensland when possible.  We will have < 50 researchers [primarily UQ, with usage from Univ. Sunshine Coast and Australian Institute of Marine Science (http://aims.gov.au/) and international collaborators (current collaborators include Okinawa Institute of Science and Technology, Japan; UC Santa Barbara; UC Berkeley, USA; Univ of Vienna, Austria; Technion Univ, Israel), potentially extending to other Queensland, Australian and international universities, institutes and industry in the future) using the computing nodes on a daily basis. Potentially > 10,000 users accessing our public browser from a web interface over the course of the whole project (not simultaneously).  The public browser will be hosted in an independent instance (Large) to enable simplified management.  We plan to keep images of our VMs and crucial data shared by all projects in the Object Storage. We are currently applying for storage space on the QCIF RDSI Node to mount on our VMs in order to process and generate our data collections, and so require our VMs on the QCIF Research Cloud Node. We plan to only use local storage for high I/O processes. We generally use few files >10GB and >10,000s of small files (<1GB). ",
                                    "useCase": "These cores will service primarily bioinformatics analyses in the Marine Genomics Laboratory at UQ. This laboratory is located in the School of Biological Sciences and Centre for Marine Science and is comprised on two research groups, the Sandie Degnan Lab and the Bernie Degnan Lab. There are 20+ researchers and postgraduate students in these labs, most of which have bioinformatics intensive projects. Most of the research is focussed on the analysis of the structure, function and organisation of the genome of the model marine sponge Amphimedon queenslandica and its bacterial symbionts.  This is one the premier models to understand the evolution and ecology of animals at the genomic level and the first, and still only, marine animal in Australia to have it genome sequenced and published.  Recent research outcomes from this genome project have been published in Science (IF 31.0), Nature (IF 38.6), Proc Natl Acad Sci USA (IF 9.7), PLoS Biol (IF 12.7), Current Biology (IF 9.5), Nature Microbiol Rev (IF 22.5), Nature Struct Mol Biol (IF 11.9) and Mol Biol Evol (IF 10.4).  The addition of other marine genomes and transcriptomes (e.g. the pearl oyster Pinctada maxima and the starfish Crown of Thorns Acanthaster planci) into this data collection enable collaborations with local universities and institutes and with Australian industry partners, in addition to a large diversity of international partners.  Each of these interactions provide unique platforms to expand the use of genomic information in marine science, particularly in tropical Australia. We require the capacity to undertake de novo genome, transcriptome and metagenome assembly, sequence annotation, database querying as well as host a private as well as publicly available genome browser that enables us, our collaborators and other researchers across the globe to interact with the genomic information we are generating.  "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "Marine Genomics: sea sponge and other invertebrate genomic resources for the Great Barrier Reef ",
                                    "usagePatterns": "We request the use of 2 XXL instances (32 cores total) hosted in Queensland when possible.  We will have < 50 researchers [primarily UQ, with usage from Univ. Sunshine Coast and Australian Institute of Marine Science (http://aims.gov.au/) and international collaborators (current collaborators include Okinawa Institute of Science and Technology, Japan; UC Santa Barbara; UC Berkeley, USA; Univ of Vienna, Austria; Technion Univ, Israel), potentially extending to other Queensland, Australian and international universities, institutes and industry in the future) using the computing nodes on a daily basis. Potentially > 10,000 users accessing our public browser from a web interface over the course of the whole project (not simultaneously).  The public browser will be hosted in an independent instance (Large) to enable simplified management.  We plan to keep images of our VMs and crucial data shared by all projects in the Object Storage. We are currently applying for storage space on the QCIF RDSI Node to mount on our VMs in order to process and generate our data collections, and so require our VMs on the QCIF Research Cloud Node. We plan to only use local storage for high I/O processes. We generally use few files >10GB and >10,000s of small files (<1GB). ",
                                    "useCase": "These cores will service primarily bioinformatics analyses in the Marine Genomics Laboratory at UQ. This laboratory is located in the School of Biological Sciences and Centre for Marine Science and is comprised on two research groups, the Sandie Degnan Lab and the Bernie Degnan Lab. There are 20+ researchers and postgraduate students in these labs, most of which have bioinformatics intensive projects. Most of the research is focussed on the analysis of the structure, function and organisation of the genome of the model marine sponge Amphimedon queenslandica and its bacterial symbionts.  This is one the premier models to understand the evolution and ecology of animals at the genomic level and the first, and still only, marine animal in Australia to have it genome sequenced and published.  Recent research outcomes from this genome project have been published in Science (IF 31.0), Nature (IF 38.6), Proc Natl Acad Sci USA (IF 9.7), PLoS Biol (IF 12.7), Current Biology (IF 9.5), Nature Microbiol Rev (IF 22.5), Nature Struct Mol Biol (IF 11.9) and Mol Biol Evol (IF 10.4).  The addition of other marine genomes and transcriptomes (e.g. the pearl oyster Pinctada maxima and the starfish Crown of Thorns Acanthaster planci) into this data collection enable collaborations with local universities and institutes and with Australian industry partners, in addition to a large diversity of international partners.  Each of these interactions provide unique platforms to expand the use of genomic information in marine science, particularly in tropical Australia. We require the capacity to undertake de novo genome, transcriptome and metagenome assembly, sequence annotation, database querying as well as host a private as well as publicly available genome browser that enables us, our collaborators and other researchers across the globe to interact with the genomic information we are generating.  "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "NectarForHumanRates",
                                    "usagePatterns": "Probably only one user (maximum 2), to run ~100 of long (1 to 3 weeks) analyses from relatively small data sets (input ~5Mb each, output ~1Gb)",
                                    "useCase": "Computing requirement to run all replicate analyses and simulations (~ 100 long BEAST phylogenetic runs) before submitting the main publication of a research project (recalibrating the timescale of human evolutionary history using ancient mitochondrial genomes)"
                                }
                            ],
                            "name": "060409"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "StatGenMCRI",
                                    "usagePatterns": "Small number of users (approx. 5), small data sets (mostly code).  Test data (small versions of our main datasets).  Main computation and data storage will use other resource we have access to.",
                                    "useCase": "Host source code repositories (using e.g. GIT) for access by reserch group and external collaborators.  Data sharing.  Running ad hoc web services for research tools (e.g. our custom tools).  We were not sure about how to calculate the number of core hours.  We have just assumes a full year of use of one core with occasional use of a second core."
                                }
                            ],
                            "name": "060412"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "Lowe Lab Group",
                                    "usagePatterns": "Several vm instances are envisaged per genetic simulation calculations. Some of these applications can take several hours to produce an output. There are currently about 4 genetics computational applications and each will run on its own instance to get the power of each vm resources. Also students should be able to log concurrently on the same instance and use the application on that instance and not override one another's data (but use the same application) ",
                                    "useCase": "The Phd students will be running DNA genetics simulation and computation. It is envisage that from time to time they might want to store their result data in swift."
                                }
                            ],
                            "name": "060499"
                        }
                    ],
                    "name": "0604"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.4,
                                    "instanceQuota": 1.4,
                                    "institution": "anu.edu.au",
                                    "name": "Central Kalahari Lion Spatial Research, Botswana",
                                    "usagePatterns": "Small numbers of users, usually only one connection at a time, only four people with user accounts, Anticipating between a few hours a most weekdays connected, and occasionally a prolonged calculation running (without user connection) which may take several days in a month. Data is minimal. Download mostly restricted to updates, and package downloads. About once a month a few hundred megabytes of data is expected to be uploaded or downloaded. ",
                                    "useCase": "As part of a research group in the Central Kalahari Game Reserve in Botswana, who studies large predators - I specialise in lion spatial research. My supervisors are Canberra, and Bristol, UK. The VM instance allows me travel between the field site, both supervisory institutions, share data, modelling and statistical procedures with supervisors and statisticians, and have constant access to the data, and computing power of the GIS enabled (gdal, POST-GIS), R and  Rstudio instance.  The instance has already been running through the trial period attached to my university ID, and has been remarkable, and very helpful. I would request that the \"New Instance\" request actually extends the use of this original instance. The instance runs complex procedures over several days without supervision, and these are expected to continue into July 2013. "
                                }
                            ],
                            "name": "060801"
                        }
                    ],
                    "name": "0608"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "usc.edu.au",
                                    "name": "Nicole Ertl",
                                    "usagePatterns": "Should be for 4 to 5 weeks wtih extensive use over this time frame ",
                                    "useCase": "The project is funded by the University of the Sunshine Coast and the Seafood CRC. The project looks into the effects of environmental stressors on oysters at a molecular level. Oysters were chosen for this project as they are an ecologically and economically important species. The specific part of the project that we are hoping to obtain an allocation on nectar for is the assembly and analysis of next generation sequencing data that was obtained from control and stressed (various environmental stressors) oysters. The information gained from the next generation sequencing project will gives us a general understanding of the genes expressed in these animals."
                                }
                            ],
                            "name": "0699"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "usc.edu.au",
                                    "name": "Marine climate-change dynamics",
                                    "usagePatterns": "Usage patterns will vary. Analyses will initially be relatively small, running intermittently on one or two VMs, but as the project scales up, usage will escalate. There will not be more than three users.",
                                    "useCase": "VMs will be used to run global-scale analyses of the trajectories of climate-change metrics in the global ocean (sea-surface temperature, pH, salinity, aragonite, calcite and oxygen concentrations, etc.). All data underlying the analyses are publicly available."
                                }
                            ],
                            "name": "069902"
                        }
                    ],
                    "name": "0699"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Genomic basis of Adaptation",
                                    "usagePatterns": "This project will have between 3-4 users. 1 of the users will have very large data sets, 1 user with large datasets; both using computationally intensive analyses. The remaining users will have small data sets.  I expect to have a maximum of 2 VMs with 16 cores running most of the time. These VM's will serve as servers and portable workstations. ",
                                    "useCase": "This project aims to identify genomic characteristics of species with varying capacities to adapt to climate change, test the generality of these signatures across multiple groups and environments and to build this knowledge into predictive models of biodiversity response to climate change.  Within the scope of the project the following activities will be carried out:  1. Whole genome assembly of 2+ Drosophila species 2. Multi-species comparative-genomics across Drosophila species-groups using NGS (Next-generation sequencing) data for >50 samples per population per species for 5 species. 3. Variant analysis in comparative-genomics study to track population differentiation and linkage decay across and between species of the same species-group. 4. Construct high density SNP maps for Drosophila species and construct detailed phylogenies for the same. 5. Construct high density linkage disequilibrium maps across entire chromosomes using pair-wise analysis algorithms with >100,000 datapoints per chromosome. 6. Construct haploid genomes using NGS data from familially related individuals. "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Lambert Ancient DNA",
                                    "usagePatterns": "We will typically have several users (up to 10-12 or so) with large datasets (10s to hundreds of Gb). ",
                                    "useCase": " We are sequencing ancient genomes of a number of species for population-level genomic studies. Included in our efforts are: 1) Antarctic Adelie penguin (Pygoscelis adeliae) samples from up to 30000 years ago, which will be compared to modern Adelie genomes to investigate adaptation to climate change occurring since the last glacial maximum 2) Samples of extinct Moa (Dinornis spp.) from New Zealand, which are analyzed in efforts to understand the evolution of flightlessness, as well as trace the evolutionary relationships amongst extant and extinct ratites 3) Samples of the First Australians, representing the earliest known human remains in Australia (ca. 45,000 ya), the sequencing of which will help us to better understand the origins and characteristics of the first human inhabitants of Australia. Our lab is currently composed our Principal Investigator David Lambert, five post-doctoral scholars and two PhD students. A third PhD student is expected within the next year. It is expected that most of the members of our lab will use the allotted computing space, both for storage of next-generation sequencing data that is currently being generated, and for analyses of this data. In addition, it is likely that several international collaborators will utilize the space for shared data and analyses. "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Lambert Ancient DNA",
                                    "usagePatterns": "We will typically have several users (up to 10-12 or so) with large datasets (10s to hundreds of Gb). ",
                                    "useCase": " We are sequencing ancient genomes of a number of species for population-level genomic studies. Included in our efforts are: 1) Antarctic Adelie penguin (Pygoscelis adeliae) samples from up to 30000 years ago, which will be compared to modern Adelie genomes to investigate adaptation to climate change occurring since the last glacial maximum 2) Samples of extinct Moa (Dinornis spp.) from New Zealand, which are analyzed in efforts to understand the evolution of flightlessness, as well as trace the evolutionary relationships amongst extant and extinct ratites 3) Samples of the First Australians, representing the earliest known human remains in Australia (ca. 45,000 ya), the sequencing of which will help us to better understand the origins and characteristics of the first human inhabitants of Australia. Our lab is currently composed our Principal Investigator David Lambert, five post-doctoral scholars and two PhD students. A third PhD student is expected within the next year. It is expected that most of the members of our lab will use the allotted computing space, both for storage of next-generation sequencing data that is currently being generated, and for analyses of this data. In addition, it is likely that several international collaborators will utilize the space for shared data and analyses. "
                                }
                            ],
                            "name": "060303"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "griffith.edu.au",
                                    "name": "Lambert Ancient DNA",
                                    "usagePatterns": "We will typically have several users (up to 10-12 or so) with large datasets (10s to hundreds of Gb). ",
                                    "useCase": " We are sequencing ancient genomes of a number of species for population-level genomic studies. Included in our efforts are: 1) Antarctic Adelie penguin (Pygoscelis adeliae) samples from up to 30000 years ago, which will be compared to modern Adelie genomes to investigate adaptation to climate change occurring since the last glacial maximum 2) Samples of extinct Moa (Dinornis spp.) from New Zealand, which are analyzed in efforts to understand the evolution of flightlessness, as well as trace the evolutionary relationships amongst extant and extinct ratites 3) Samples of the First Australians, representing the earliest known human remains in Australia (ca. 45,000 ya), the sequencing of which will help us to better understand the origins and characteristics of the first human inhabitants of Australia. Our lab is currently composed our Principal Investigator David Lambert, five post-doctoral scholars and two PhD students. A third PhD student is expected within the next year. It is expected that most of the members of our lab will use the allotted computing space, both for storage of next-generation sequencing data that is currently being generated, and for analyses of this data. In addition, it is likely that several international collaborators will utilize the space for shared data and analyses. "
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.8,
                                    "institution": "griffith.edu.au",
                                    "name": "Lambert Ancient DNA",
                                    "usagePatterns": "We will typically have several users (up to 10-12 or so) with large datasets (10s to hundreds of Gb). ",
                                    "useCase": " We are sequencing ancient genomes of a number of species for population-level genomic studies. Included in our efforts are: 1) Antarctic Adelie penguin (Pygoscelis adeliae) samples from up to 30000 years ago, which will be compared to modern Adelie genomes to investigate adaptation to climate change occurring since the last glacial maximum 2) Samples of extinct Moa (Dinornis spp.) from New Zealand, which are analyzed in efforts to understand the evolution of flightlessness, as well as trace the evolutionary relationships amongst extant and extinct ratites 3) Samples of the First Australians, representing the earliest known human remains in Australia (ca. 45,000 ya), the sequencing of which will help us to better understand the origins and characteristics of the first human inhabitants of Australia. Our lab is currently composed our Principal Investigator David Lambert, five post-doctoral scholars and two PhD students. A third PhD student is expected within the next year. It is expected that most of the members of our lab will use the allotted computing space, both for storage of next-generation sequencing data that is currently being generated, and for analyses of this data. In addition, it is likely that several international collaborators will utilize the space for shared data and analyses. "
                                }
                            ],
                            "name": "0603"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "BEAST_exome",
                                    "usagePatterns": "Large datasets Few users",
                                    "useCase": "I hope to use cloud instances to perform phylogenetic analyses on Rattus exome sequence data using the *BEAST software package. This software requires major computational resources, and I have exhausted all resources available to me locally."
                                }
                            ],
                            "name": "060301"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "BEAST_exome",
                                    "usagePatterns": "Large datasets Few users",
                                    "useCase": "I hope to use cloud instances to perform phylogenetic analyses on Rattus exome sequence data using the *BEAST software package. This software requires major computational resources, and I have exhausted all resources available to me locally."
                                }
                            ],
                            "name": "060302"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "BEAST_exome",
                                    "usagePatterns": "Large datasets Few users",
                                    "useCase": "I hope to use cloud instances to perform phylogenetic analyses on Rattus exome sequence data using the *BEAST software package. This software requires major computational resources, and I have exhausted all resources available to me locally."
                                }
                            ],
                            "name": "060309"
                        }
                    ],
                    "name": "0603"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Soils2Sats",
                                    "usagePatterns": "As per current",
                                    "useCase": "Migration and consolidation of S2S infrastructure - currently split across CSIRO and TERN run NeCTAR tenancys Ive put in an Allocation Request for 2 extra instances for this tenancy. What we would like is a Small VM for the s2sDev-Portal and a Medium VM for the s2sProd-Portal added to the existing Soils2Sats project. This is so that we can consolidate the whole Soils2Satellites project infrastructure under the one tenancy. Currently the infrastructure is split across an ALA managed tenancy for our web portal and our TERN managed tenancy for the services and db. However the Allocation Request form doesnt really let me explain my request very well  is there any other channel I can use to clarify this request?"
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 2.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN-Adel",
                                    "usagePatterns": "The project provides a User Accessible portal - AEKOS that contains large amounts of ecological data, binary and images. The initial estimates for the amounts of data in the system have been revised and we now require extra data capacity. ",
                                    "useCase": "As per our initial request that established the TERN-Adel tenancy.  The issue we now face is that we are being requested to provide individual AEKOS environments to our data providers in order for them to test and verify their datasets inside AEKOS prior to publication in production. The data ingestion processes that we need to run to create AEKOS data is extremely memory and processor intensive and generates a significant amount of data as an output. Our existing allocation is no longer able to cope with the demand we are receiving.  "
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN Ecoinformatics (Australian Ecological Knowledge and Observation System)",
                                    "usagePatterns": "",
                                    "useCase": "Ingestion and federation of data from various government agencies, universities and NGOs; processing of textual, spatial and imagery data; databases; software development; web applications. Also interacting and exchanging data with other systems such as SHaRED (NeCTAR), Soils to Satellites (ANDS and ALA) and the TERN Multi-Scale Plot Network (MSPN), however these are separate projects acquiring their own allocations. Some VMs are envisaged as offering long-lived services whereas some will be temporary and used for development, testing or short-term processing. "
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_OzTrack",
                                    "usagePatterns": "Amendment, 2013-11-18: As part of testing the current deliverables for this NeCTAR e-Research Tools project, we need to set up a UAT environment. I have updated the requested Instance/Core/Volume numbers to reflect the addition of one Large instance with an attached 100GB persistent volume. ---- NeCTAR cloud instances will host the OzTrack Java web application, GeoServer instance, PostgreSQL database, and a pool of Rserve instances used to compute R-based home range analyses and movement models. Potential configuration: one large instance for the Java servlet container and database; and four medium instances, each running Rserve, connected via TCP to main instance. Storage breakdown for current OzTrack deployment (oztrack.org): animal tracking data 1.0 GiB; external environmental layers, e.g. land use and bathymetry grids, 5.7 GiB; daily/weekly/monthly back-ups of animal tracking data 4.4 GiB. Animal tracking data is likely to remain small, but plans for pre-seeding tile caches of environmental layers in GeoServer mean that we are requesting 100 GiB of storage. As for the object store, we are not sure at this stage whether it will be required. We are considering using it for backups and potentially for storing environmental data files. We have applied for an allocation in order to experiment with it and decide whether it is appropriate.",
                                    "useCase": "The OzTrack project is supported by the NeCTAR e-Research Tools program for 2012-13. The goal of this project is to develop a set of eResearch software tools to support the compute, storage and analysis of animal tracking data being generated through telemetry devices. See http://oztrack.org/ and http://www.itee.uq.edu.au/eresearch/projects/oztrack/"
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 2.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_OzTrack",
                                    "usagePatterns": "Amendment, 2013-11-18: As part of testing the current deliverables for this NeCTAR e-Research Tools project, we need to set up a UAT environment. I have updated the requested Instance/Core/Volume numbers to reflect the addition of one Large instance with an attached 100GB persistent volume. ---- NeCTAR cloud instances will host the OzTrack Java web application, GeoServer instance, PostgreSQL database, and a pool of Rserve instances used to compute R-based home range analyses and movement models. Potential configuration: one large instance for the Java servlet container and database; and four medium instances, each running Rserve, connected via TCP to main instance. Storage breakdown for current OzTrack deployment (oztrack.org): animal tracking data 1.0 GiB; external environmental layers, e.g. land use and bathymetry grids, 5.7 GiB; daily/weekly/monthly back-ups of animal tracking data 4.4 GiB. Animal tracking data is likely to remain small, but plans for pre-seeding tile caches of environmental layers in GeoServer mean that we are requesting 100 GiB of storage. As for the object store, we are not sure at this stage whether it will be required. We are considering using it for backups and potentially for storing environmental data files. We have applied for an allocation in order to experiment with it and decide whether it is appropriate.",
                                    "useCase": "The OzTrack project is supported by the NeCTAR e-Research Tools program for 2012-13. The goal of this project is to develop a set of eResearch software tools to support the compute, storage and analysis of animal tracking data being generated through telemetry devices. See http://oztrack.org/ and http://www.itee.uq.edu.au/eresearch/projects/oztrack/"
                                },
                                {
                                    "coreQuota": 4.5,
                                    "instanceQuota": 4.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN Ecoinformatics (Australian Ecological Knowledge and Observation System)",
                                    "usagePatterns": "",
                                    "useCase": "Ingestion and federation of data from various government agencies, universities and NGOs; processing of textual, spatial and imagery data; databases; software development; web applications. Also interacting and exchanging data with other systems such as SHaRED (NeCTAR), Soils to Satellites (ANDS and ALA) and the TERN Multi-Scale Plot Network (MSPN), however these are separate projects acquiring their own allocations. Some VMs are envisaged as offering long-lived services whereas some will be temporary and used for development, testing or short-term processing. "
                                }
                            ],
                            "name": "0602"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_ExploreTheSeafloor",
                                    "usagePatterns": "We will have large numbers of files that contain data related to the annotation of seafloor imagery.  The imagery itself is hosted elsewhere.",
                                    "useCase": "We are looking for an accessible machine to store and analyse data related to the Science Week Explore The Seafloor project (exploretheseafloor.net.au)."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_ExploreTheSeafloor",
                                    "usagePatterns": "We will have large numbers of files that contain data related to the annotation of seafloor imagery.  The imagery itself is hosted elsewhere.",
                                    "useCase": "We are looking for an accessible machine to store and analyse data related to the Science Week Explore The Seafloor project (exploretheseafloor.net.au)."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 1.2,
                                    "institution": "uq.edu.au",
                                    "name": "CSS_Shallow",
                                    "usagePatterns": "LArge data set and a small number of users (2-4).",
                                    "useCase": "These instances will be used to process image analysis from a large collection of underwater imager to handle, post-produce images, model 3D reconstructions and perform automated image annotation."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "csiro.au",
                                    "name": "TERN Australian Coastal Ecosystems Facility",
                                    "usagePatterns": "Please ignore the core hours field above - as far as I can tell, it does not really apply for how we plan to use these servers. We will be hosting 10 to 50 datasets, with most being regularly added to. About half will be file-based, and we hope to use object data storage for those: the initial space requirement is around 2TB, expected to grow by 100GB/month.  The remaining datasets will require databases: MySQL, PostgreSQL/PostGIS and MongoDB. These will need to be able to be backed up regularly. Open-source data-services and GUI applications will be used to make this data available to the general public. All data requests will be made via HTTP, routed to the appropriate services via Apache. Load will be highly variable, but should not be sustained at high levels. There will be a limited number of concurrent users, fewer than 10 with login rights.",
                                    "useCase": "This request is for infrastructure to be associated with several projects, including: * Australian Coastal Ecosystems Facility (ACEF),  which is part of TERN (see http://acef.tern.org.au/?q=data) * The South-east Queensland Integrated Terrestrial to Ocean Research (SEQITOR), which is funded by ANDS (see  http://blog.seqitor.org.au/)  * eReefs (http://ereefs.org.au/) These servers will store and serve a combination of spatial time-series data, geo-referenced video data, remote sensing imagery, and 4D model outputs.  The VM size specified above is not required for all 6 servers, but is the best choice the form would allow. The actual planned use and size requirement for each one is: Server 1. Map Rendering and tile cache (GeoServer / THREDDS). XXL VM please! Server 2. Real-time sensor storage and aggregation (SensorCloud). XL Server 3. Map Layer aggregation (GeoNetwork). XL Server 4. Web-GUI Applications. XL Server 5. Build and test environment. M. Server 6. Monitoring and control of other servers. M."
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "monash.edu",
                                    "name": "Monash_Water_Quality-Gippsland_Lakes",
                                    "usagePatterns": "It will only have a couples of users for this project at the moment. The number of files can get up to a few hundreds. The sizes of the individual files can range from a few KB to 20GB.",
                                    "useCase": "I am working on a water quality modelling project at the Gippsland Lakes. The research project looks into the  dynamics of the toxic cyanobacterial blooms in the lakes. The software we use is called  MIKE developed by DHI (http://www.dhisoftware.com/). The model requires a lot computing power. As approaching towards the end this study, we will need to do many simulations for model calibration and scenario explorations in the next 10 months. Unfortunately, the software I am using can  only run on windows platform and we just have a 6-core windows machine in our lab which is far from being capable to handles all the computations. Therefore we hope to get access to more computing power to overcome this constraints."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.4,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_IMAS_Predators",
                                    "usagePatterns": "The project will have moderately lager data sets (e.g remote sensing data from the Southern Ocean encompassing several decades), but relatively few simultaneous users - typically no more than 2",
                                    "useCase": "Our VM is used to run complex, Bayesian statistic models, usually in relation to animal movement and habitat analyses"
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 0.6,
                                    "institution": "monash.edu",
                                    "name": "Monash_Water_Quality-Gippsland_Lakes",
                                    "usagePatterns": "It will only have a couples of users for this project at the moment. The number of files can get up to a few hundreds. The sizes of the individual files can range from a few KB to 20GB.",
                                    "useCase": "I am working on a water quality modelling project at the Gippsland Lakes. The research project looks into the  dynamics of the toxic cyanobacterial blooms in the lakes. The software we use is called  MIKE developed by DHI (http://www.dhisoftware.com/). The model requires a lot computing power. As approaching towards the end this study, we will need to do many simulations for model calibration and scenario explorations in the next 10 months. Unfortunately, the software I am using can  only run on windows platform and we just have a 6-core windows machine in our lab which is far from being capable to handles all the computations. Therefore we hope to get access to more computing power to overcome this constraints."
                                }
                            ],
                            "name": "060205"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "utas.edu.au",
                                    "name": "NERP Web Fire Mapping",
                                    "usagePatterns": "A small number of users with moderate-sized data sets - initially one user managing a suite of spatial data, but more users may be added if necessary.  After initial map rendering, CPU requirements will be low.",
                                    "useCase": "Provide spatial data storage, map tile rendering and web mapping service serving for the National Environmental Research Program (Landscapes and Policy Hub)."
                                }
                            ],
                            "name": "060208"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "anu.edu.au",
                                    "name": "Central Kalahari Lion Spatial Research, Botswana",
                                    "usagePatterns": "Small numbers of users, usually only one connection at a time, only four people with user accounts, Anticipating between a few hours a most weekdays connected, and occasionally a prolonged calculation running (without user connection) which may take several days in a month. Data is minimal. Download mostly restricted to updates, and package downloads. About once a month a few hundred megabytes of data is expected to be uploaded or downloaded. ",
                                    "useCase": "As part of a research group in the Central Kalahari Game Reserve in Botswana, who studies large predators - I specialise in lion spatial research. My supervisors are Canberra, and Bristol, UK. The VM instance allows me travel between the field site, both supervisory institutions, share data, modelling and statistical procedures with supervisors and statisticians, and have constant access to the data, and computing power of the GIS enabled (gdal, POST-GIS), R and  Rstudio instance.  The instance has already been running through the trial period attached to my university ID, and has been remarkable, and very helpful. I would request that the \"New Instance\" request actually extends the use of this original instance. The instance runs complex procedures over several days without supervision, and these are expected to continue into July 2013. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Acoustic_Work_Bench",
                                    "usagePatterns": "I think initially about 100 users and large data sets 100TB total but users will only access a small part of them. The data storage has been requested separately through Nectar. (core hours is a guess)",
                                    "useCase": "Collection of audible range terrestrial bio-acoustic (sound) recordings used for species monitoring, bio-diversity assessment and animal behaviour studies."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_RTrack01",
                                    "usagePatterns": "Large data sets with a small number of users, tens of gigabytes and only 2-3 users for this proof-of-concept. ",
                                    "useCase": "Model runs of light-level geo-location estimation from marine animals (elephant seals, snow petrels) using  Bayesian methods. These methods are currently being updated and we are exploring parallelization options. This instance would be a small test bed to demonstrate a proof of concept. It could ultimately be an R server service to run these models for researchers at the AAD, IMAS and the ACE CRC. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Acoustic_Work_Bench",
                                    "usagePatterns": "I think initially about 100 users and large data sets 100TB total but users will only access a small part of them. The data storage has been requested separately through Nectar. (core hours is a guess)",
                                    "useCase": "Collection of audible range terrestrial bio-acoustic (sound) recordings used for species monitoring, bio-diversity assessment and animal behaviour studies."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_RTrack01",
                                    "usagePatterns": "Large data sets with a small number of users, tens of gigabytes and only 2-3 users for this proof-of-concept. ",
                                    "useCase": "Model runs of light-level geo-location estimation from marine animals (elephant seals, snow petrels) using  Bayesian methods. These methods are currently being updated and we are exploring parallelization options. This instance would be a small test bed to demonstrate a proof of concept. It could ultimately be an R server service to run these models for researchers at the AAD, IMAS and the ACE CRC. "
                                }
                            ],
                            "name": "060201"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "une.edu.au",
                                    "name": "RevegUNE",
                                    "usagePatterns": "Database",
                                    "useCase": "PhD project that will investigate germination and establishment requirements of about 160 common native species of tree, shrub, forb and grass in crop lands in Moree Area  NSW. UNE-Border Rivers Gwydir Catchment Management Authorithy."
                                }
                            ],
                            "name": "060203"
                        }
                    ],
                    "name": "0602"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 7.2,
                                    "instanceQuota": 7.2,
                                    "institution": "uts.edu.au",
                                    "name": "Intersect_Microbial_community_analysis",
                                    "usagePatterns": "3 users, data sets 10-30GB each, mix of I/O and CPU intensive work.",
                                    "useCase": "Analyzing microbial communities using 16S amplicon data generated by Illumina MiSeq instruments. Will use QIIME VM. "
                                },
                                {
                                    "coreQuota": 7.2,
                                    "instanceQuota": 1.8,
                                    "institution": "uts.edu.au",
                                    "name": "Intersect_Microbial_community_analysis",
                                    "usagePatterns": "3 users, data sets 10-30GB each, mix of I/O and CPU intensive work.",
                                    "useCase": "Analyzing microbial communities using 16S amplicon data generated by Illumina MiSeq instruments. Will use QIIME VM. "
                                }
                            ],
                            "name": "060504"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Dental16S",
                                    "usagePatterns": "Small number of users and relatively large data sets (typically 1Gb per sequence per run). QIIME will be with large multiplexed numbers of samples, up to 2 or 3 Gb of sequence (30 million reads) per run",
                                    "useCase": "Cloud Instances required for working with Bacterial Genome Assemblies (DeNovo and Resequencing) and Short Amplicon Phylogenetic survey work (QIIME). Data is mainly produced from our laboratories Ion Torrent PGM."
                                }
                            ],
                            "name": "060503"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "usc.edu.au",
                                    "name": "Joanne Macdonald",
                                    "usagePatterns": "",
                                    "useCase": "Run python scripts that call information from public databases, then bioinformatically assess for matches with query inputs. The process is CPU intensive but on site tests have shown it have a very small RAM and disk utilisation. This will be an ongoing project with new processes being run frequently"
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "usc.edu.au",
                                    "name": "USC-FIoV:",
                                    "usagePatterns": "",
                                    "useCase": "* project description, significance and expected outcomes We are developing next-generation diagnostic assays that use molecular circuitry to enable field-identification of viruses. The devices are constructed via nucleic acid networks, designed as a logic circuits, to link viral genetic signatures to an output text display of diagnoses. Viruses are a moving target for diagnostics, not just because of their infection time-course, but through their microevolution that helps them evade host defences. We have developed bioinformatics software for maximum capture of evolving viruses, which is able to identify regions of similarity in highly related sequences that are divergent from non-related sequences. The specificity of the software is controlled by user-inputs, and can be used to determine consensus sequences at the level of both species and subtype/strain as required.  Significance: Diagnostic technologies for individual detection of potential disease biomarkers are well established. Significant developments in recent years have occurred in the fields of (i) multiplexing, and (ii) rapid/low-resource detection. (i) Multiplex developments include high- and low-density spatial arrangements such as microarrays, or the use of multiple tags such as mass-tagged PCR, electrospray ionisation, or luminex beads. Our molecular circuits use a novel adaptation of spatial arrays to provide text outputs for autonomous diagnosis of multiple viral signature sequences.  (ii) The most striking low resource development in diagnostics has been the lateral flow dipstick, which operates similarly to an at-home pregnancy test. The sensitivity of the standard device is limited, however, the subsequently reported nucleic acid lateral flow device utilizes PCR-based sensitivity with rapid duplex detection. Combined with isothermal amplification, this set-up holds the most promise for rapid end-point detection in low resource settings. Variations include detection by fluorescence, which improves sensitivity at the expense of requiring portable read-out devices, extension to low-density micro-arrays, or portable lab-on-a-chip devices that combine sample preparation, amplification, and detection in a single user-friendly device. Our proposal moves beyond these singleplex or duplex field-amenable endpoint devices by providing a method to detect multiple viral disease markers and also quantifying their amount, yet still in a field-amenable assay format. Outcome: A robust molecular diagnostic technology able to detect and quanitify multiple viruses, undergoing reasonable levels of microevolution, in a single field-amenable device. * Impact and innovation of the research Our circuitry is purely driven by molecules: no electronics are incorporated, and no electronic power is required for operation. By replacing electronic circuitry with molecular circuitry, we reduce electrical requirements and contribute to reduction in carbon footprints. In addition, previous field-amenable diagnostic assays have focused on the detection of only a few agents in a single device. Our use of molecular automata allows the development of a single field-based device that can identify and analyse the significance of multiple biomarker presence. Finally, our innovative custom bioinformatics software enables rapid and automated development of devices that encompasses viral microevolution, enabling comprehensive viral detection. * National benefit of the research National research priority: Safeguarding Australia  (i) The ability to detect viral infections, and quantitatively assess viral loads, all onsite in a clinic, field, or airport, would have a significant impact on disease treatment and epidemiology, protecting Australia from invasive diseases and pests. Infectious but asymptomatic virus carriers, as well as those in the early stage of infection that do not yet show disease, could be screened and isolated. This has major ramifications in airports, cruise ships, animal export industries, and general border control, and could result in major economic gains. Additionally, while this research is entirely focused on viral infections, the generic principles of our integrated diagnostic platforms are entirely amenable to other infections, such as bacteria and parasites. (ii) A hand-held device that quickly analyses for biosecurity threats and does not require electricity to operate would be a transformational defence technology, allowing soldiers to assess and contain bioterrorism attacks before extensive disease spread.   Targeted research areas: This proposal directly addresses several targeted research areas, including the development of bioinformatics methodology for rapidly evolving viruses and the use of nucleic acid computing networks to provide an alternative green technology that replace electronics and wires with molecules. Our nucleic acid networks will perform autonomous pattern recognition and data mining for assessment of disease state and diagnosis of viral infections. Additionally, our proposal indirectly addresses indigenous health and wellbeing, in the sense that any point-of-care inexpensive rapid assay for the detection of viruses of national significance will provide an enabling technology for communities in low-resource settings to take ownership of their personal and community human and animal health. Rapid on-site diagnosis allows for faster decisions on treatment and isolation, which can reduce morbidity and disease spread. "
                                }
                            ],
                            "name": "060506"
                        }
                    ],
                    "name": "0605"
                }
            ],
            "name": "06"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "Service Innovation Hub",
                                    "usagePatterns": "Primarily mysql databases and small datasets, up to 100GB, with up to 20 regular users and a modest amount of web traffic from interested researchers and industry.",
                                    "useCase": "The node will be used to develop and host a research portal for the QUT lead Service Innovation Hub research program. The portal will host business model service capture tools, service aggregators, catalog editors,  and service animators, all of which are outputs of the service research stream. It will also host content utilizing a Drupal web CMS. It will be a collaboration point for QUT, the Smart Services CRC and our industry partners."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "Service Innovation Hub",
                                    "usagePatterns": "Primarily mysql databases and small datasets, up to 100GB, with up to 20 regular users and a modest amount of web traffic from interested researchers and industry.",
                                    "useCase": "The node will be used to develop and host a research portal for the QUT lead Service Innovation Hub research program. The portal will host business model service capture tools, service aggregators, catalog editors,  and service animators, all of which are outputs of the service research stream. It will also host content utilizing a Drupal web CMS. It will be a collaboration point for QUT, the Smart Services CRC and our industry partners."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "Service Innovation Hub",
                                    "usagePatterns": "Primarily mysql databases and small datasets, up to 100GB, with up to 20 regular users and a modest amount of web traffic from interested researchers and industry.",
                                    "useCase": "The node will be used to develop and host a research portal for the QUT lead Service Innovation Hub research program. The portal will host business model service capture tools, service aggregators, catalog editors,  and service animators, all of which are outputs of the service research stream. It will also host content utilizing a Drupal web CMS. It will be a collaboration point for QUT, the Smart Services CRC and our industry partners."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "monash.edu",
                                    "name": "Cluster Config Management Experiments",
                                    "usagePatterns": "My student will be using SSH predominantly, plus any TCP or UDP ports that might be devised for use by Parrot or supplementary protocols, e.g., SNMP. Core utilisation will be minimal; this isn't about number-crunching. Data transfers will be moderate at the most, as we will experiment with different models for deploying packages, e.g., centralised, peer-to-peer, cooperative, etc. The traffic will predominantly be between the instances and Monash University networks.",
                                    "useCase": "I'm supervising an experimental project on configuration management using Parrot, specifically looking at an automated configuration model for HTC/e-Science resources."
                                }
                            ],
                            "name": "080503"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "monash.edu",
                                    "name": "Meta-Batch-Scheduler Testing",
                                    "usagePatterns": "Small number of Users with small data sets. one or two sql and/or ldap servers.",
                                    "useCase": "Monash eResearch Centre's HPC division is developing a new Nimrod meta-batch-scheduler, which will submit jobs to multiple batch-schedulers based on Network and Hardware Locality. We need to test the prototype on small cluster.  "
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "jcu.edu.au",
                                    "name": "Scalling Sensor Observation Services",
                                    "usagePatterns": "1 user, but that user will hammer the system to evaluate whether the approah (SOS on HBase) is viable. I expect to use a good chunck of the local storage provisioned to the allocation. ",
                                    "useCase": "Analysing the performance of an Open Geo-Spatial Consortium (OGC) Sensor Observation Service (SOS) built on top of Apache HBase/Hadoop. The deployment will utilise the local storage of the VMs (insdead of the Object Storage)."
                                },
                                {
                                    "coreQuota": 3.5,
                                    "instanceQuota": 3.5,
                                    "institution": "csiro.au",
                                    "name": "eReefs",
                                    "usagePatterns": "Users: about 10 users (our project team plus extras).  Expected use: 2 or 3 of those 10 users using 2 or 3 machines at a time. Datasets: small (single digit gigabytes at a max). These machines are for config testing, not computation. Some of the datasets are just RDF files that need public web hosting so some machine's OSes will take up far more room than their data.",
                                    "useCase": "In eReefs we're designing and implementing server components that enable organisations holding data to deliver it via standardised service. We, the CSIRO team, build the servers or server parts and then hope to transfer images of them - or perhaps even just knowledge of how to install them - to the other eReefs partner organisations (BoM, AMIS, Queensland Government, IMOS) who will ultimately house them. This means we have a requirement to set up VMs, test their configurations, snapshot them and then take them down. We don't have large computational or data storage needs - our biggest challenge is configuration management."
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "monash.edu",
                                    "name": "Cluster Config Management Experiments",
                                    "usagePatterns": "My student will be using SSH predominantly, plus any TCP or UDP ports that might be devised for use by Parrot or supplementary protocols, e.g., SNMP. Core utilisation will be minimal; this isn't about number-crunching. Data transfers will be moderate at the most, as we will experiment with different models for deploying packages, e.g., centralised, peer-to-peer, cooperative, etc. The traffic will predominantly be between the instances and Monash University networks.",
                                    "useCase": "I'm supervising an experimental project on configuration management using Parrot, specifically looking at an automated configuration model for HTC/e-Science resources."
                                },
                                {
                                    "coreQuota": 400.0,
                                    "instanceQuota": 50.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_VM_Co-resident Attacks",
                                    "usagePatterns": "Large data sets with a small number of users",
                                    "useCase": "We aim to find a practical countermeasure to defend against the co-resident attacks in cloud computing, where malicious users aim to co-locate their VMs with target VMs on the same physical server, and then exploit side channels to extract private information from the victim. Specifically, we have designed a new VM allocation policy, and simulation experiments on CloudSim show that the policy not only makes it difficult for attackers to co-locate with their targets, but also achieves a reasonably good load balance, and low power consumption. We intend to test the policy in a real cloud environment."
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM TeachingCloudComputing",
                                    "usagePatterns": "The students will be harvesting Tweets in real time and pushing these into CouchDB (actually GeoCouch). They will then run a range of analyses on these databases using for example MapReduce based approaches. The data will not likely be large (several gigabytes or up to 30-40Gb?) per team. The individual CouchDB instances will be aggregated to produce a single populated CouchDB instance. ",
                                    "useCase": "I am to teach Cloud computing to grad students in Melbourne. They will be developing Cloud apps for harvesting Twitter data and pushing it into CouchDB for a range of data analytics scenarios. There will be 8 groups (with 4-5 team members per group). I want them to develop/deploy their applications across multiple Cloud resources (not just Melbourne for example). Note the core CPU hours above will not likely be contiguous. The students will use these machines in an ad hoc manner."
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 1.2,
                                    "institution": "swin.edu.au",
                                    "name": "Smart Services CRC Foundry Project",
                                    "usagePatterns": "",
                                    "useCase": "Hi, I am currently involved with the Smart Services CRC and am building a cloud based protoype for which I require a minimum of 3 instances at any given time. One instance will be long running while the remaining two will be turned on/off on demand. At the moment I can only start 2 VMs and would like access to atleast 1 more so that I can demonstrate my prototype system."
                                }
                            ],
                            "name": "080501"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "monash.edu",
                                    "name": "Adaptive Data Stream Mining for Data Partition Hot-spot Prediction",
                                    "usagePatterns": "Synthetic data will be generated using YCSB toolkit for populating the HBase data store from 2 client instances.",
                                    "useCase": "In this project we aim to predict the hot spotted logical data partitions in real time using data stream mining techniques. We will use HBase/Hadoop clusters (1 Master Node, 1 Zookeeper, 5 Data Nodes and 1 Monitoring node with Ganglia Server) for this project. Another 2 servers will be used to generate synthetic data using YCSB toolkit.  Contact:  Joarder Mohammad Mustafa Kamal PhD Student Gippsland School of Information Technology, Faculty of Information Technology, MONASH University Churchill VIC, Australia. 3842 Email: Joarder.Kamal@monash.edu Phone: 0351-226-133 (Room: 4N-247), 0470-578-819 (Mobile) Web: http://users.monash.edu/~jkamal/ Supervisor: Dr. Manzur Murshed"
                                },
                                {
                                    "coreQuota": 200.0,
                                    "instanceQuota": 200.0,
                                    "institution": "monash.edu",
                                    "name": "Monash Porting",
                                    "usagePatterns": "",
                                    "useCase": "Monash cloud setup & testing"
                                },
                                {
                                    "coreQuota": 100.0,
                                    "instanceQuota": 100.0,
                                    "institution": "monash.edu",
                                    "name": "MCC_On_R@CMON",
                                    "usagePatterns": "These VMs will be running CPU and CPU+I/O heavy jobs, so the use of the volume storage + ephemeral disks will be crucial to achieving good performance.",
                                    "useCase": " Hi Steve Id like to request the allocation of 10 % of the Monashs discretionary share for R@CMON/NeCTAR as an extension of the Monash Central HTC/HPC Compute Capability [MCC]. This project aims to provision multiple 8-core/16-core, 32-64GB RAM VM instances as compute nodes to add into the batch queueing system. Mechanisms will be developed to enable jobs to migrate from the central system to the VMs. Please contact us if you need further information. kind regards, philip I've requested for 1 TB of Object Storage for the potential to use this to \"store and forward\" data that is relevant to compute."
                                },
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 10.0,
                                    "institution": "anu.edu.au",
                                    "name": "NCI_StarCluster",
                                    "usagePatterns": "",
                                    "useCase": "I am working on the NeCTAR project at NCI. Currently we are porting the StarCluster to work with OpenStack.   We made some progress and would like to have some more CPU cores to test it on the NeCTAR cloud.  It would be nice if one StarCluster image can be added as well."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uws.edu.au",
                                    "name": "HPC system in Nectar cloud",
                                    "usagePatterns": "",
                                    "useCase": "Project: To develop a tool that configures HPC systems in the cloud (NECTAR) that UWS researchers and others will use to automate the setup of HPC systems in Nectar cloud. It is also intended that the tool will reduce the HPC systems setup time."
                                },
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 5.0,
                                    "institution": "anu.edu.au",
                                    "name": "NCI_StarCluster",
                                    "usagePatterns": "",
                                    "useCase": "I am working on the NeCTAR project at NCI. Currently we are porting the StarCluster to work with OpenStack.   We made some progress and would like to have some more CPU cores to test it on the NeCTAR cloud.  It would be nice if one StarCluster image can be added as well."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uws.edu.au",
                                    "name": "HPC system in Nectar cloud",
                                    "usagePatterns": "",
                                    "useCase": "Project: To develop a tool that configures HPC systems in the cloud (NECTAR) that UWS researchers and others will use to automate the setup of HPC systems in Nectar cloud. It is also intended that the tool will reduce the HPC systems setup time."
                                },
                                {
                                    "coreQuota": 1.4,
                                    "instanceQuota": 1.4,
                                    "institution": "uws.edu.au",
                                    "name": "ADELTA",
                                    "usagePatterns": "",
                                    "useCase": "ADELTA Adelta is the Australian Directory of Electronic Literature and Text-based Art The directory emerges from the Creative Nation ARC discovery funded project: Writers and Writing in the New Media Arts led by Assoc/Prof Anna Gibbs and Dr Maria Angel from the Writing and Society Research Group in the School of Humanities and Communication Arts at the University of Western Sydney. This project is building and/or adapting open source software (probably the Islandora repository platform) to house an interactive directory of Australian writers and writing in the New Media Arts. This work is taking place in the context of work on the Consortium of Electronic Literature (CELL). This consortium addresses the development of collaboration for the purposes of researching, publishing and archiving electronic works, and is currently working on the interoperability of international databases of electronic literature. Founding partners of CELL are the University of Western Sydney (adelta), ELMCIP, Po.Ex, NT2, Electronic Book Review, University of Siegen (Likumed), Hermeneia, and Archiveit.org/Library of Congress. Technical work on this project is being done by the UWS eResearch team, led by Peter Sefton, with technical work done by Lloyd Harischandra and Ifeanyi Egwutuoha "
                                },
                                {
                                    "coreQuota": 1.4,
                                    "instanceQuota": 1.4,
                                    "institution": "uws.edu.au",
                                    "name": "ADELTA",
                                    "usagePatterns": "",
                                    "useCase": "ADELTA Adelta is the Australian Directory of Electronic Literature and Text-based Art The directory emerges from the Creative Nation ARC discovery funded project: Writers and Writing in the New Media Arts led by Assoc/Prof Anna Gibbs and Dr Maria Angel from the Writing and Society Research Group in the School of Humanities and Communication Arts at the University of Western Sydney. This project is building and/or adapting open source software (probably the Islandora repository platform) to house an interactive directory of Australian writers and writing in the New Media Arts. This work is taking place in the context of work on the Consortium of Electronic Literature (CELL). This consortium addresses the development of collaboration for the purposes of researching, publishing and archiving electronic works, and is currently working on the interoperability of international databases of electronic literature. Founding partners of CELL are the University of Western Sydney (adelta), ELMCIP, Po.Ex, NT2, Electronic Book Review, University of Siegen (Likumed), Hermeneia, and Archiveit.org/Library of Congress. Technical work on this project is being done by the UWS eResearch team, led by Peter Sefton, with technical work done by Lloyd Harischandra and Ifeanyi Egwutuoha "
                                }
                            ],
                            "name": "0805"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.5,
                                    "instanceQuota": 3.5,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-eReefs",
                                    "usagePatterns": "The images will be created and destroyed many times over by one user (me) for the next year or so as configurations are tested and then, finally when correctly configured, most will be left to deliver very small amounts of metadata as described above.",
                                    "useCase": "These machines are low network and computational volume machines to be used for configuration testing for a number of geospatial Web Services. they will mostly deliver small amounts of metadata (vocabularies of terms, data models in XML, metadata documents in XML). They will be created and destroyed regularly at first until successful configurations are made then kept for a while."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "uq.edu.au",
                                    "name": "OwnCloud_Eval",
                                    "usagePatterns": "8-10 users, accessing the VMs intermittently.",
                                    "useCase": "Collaborative evaluation of OwnCloud by InterSect, QUT, JCU, RDSI and QCIF. Determine the suitability of OwnCloud as a solution for providing document sharing and DropBox like functionality."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uq.edu.au",
                                    "name": "OwnCloud_Eval",
                                    "usagePatterns": "8-10 users, accessing the VMs intermittently.",
                                    "useCase": "Collaborative evaluation of OwnCloud by InterSect, QUT, JCU, RDSI and QCIF. Determine the suitability of OwnCloud as a solution for providing document sharing and DropBox like functionality."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uws.edu.au",
                                    "name": "HPC system in Nectar cloud",
                                    "usagePatterns": "",
                                    "useCase": "Project: To develop a tool that configures HPC systems in the cloud (NECTAR) that UWS researchers and others will use to automate the setup of HPC systems in Nectar cloud. It is also intended that the tool will reduce the HPC systems setup time."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uws.edu.au",
                                    "name": "HPC system in Nectar cloud",
                                    "usagePatterns": "",
                                    "useCase": "Project: To develop a tool that configures HPC systems in the cloud (NECTAR) that UWS researchers and others will use to automate the setup of HPC systems in Nectar cloud. It is also intended that the tool will reduce the HPC systems setup time."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "Astronomy Data Visualization",
                                    "usagePatterns": "Initially there will be four sites involved, with individual users at each site.  After establishing the service, I will be engaging research groups at the sites to test the service.  It is expected that there will be users in the 10s, using the services for several hours a week each, with extremely large datasets (starting with many gigabytes but hopefully testing terabyte datasets as well).",
                                    "useCase": "My masters thesis is about using cloud VMs to enable remote access to HPC facilities for astronomy research groups.  I plan to establish an XXL portal service which will provide both access to facilities as well as distribute results around Australia to research groups.  I will also create several small \"templates\" with preconfigured access to data stores and HPC processing, for the purpose of testing the viability of rapid repurposing of VMs."
                                }
                            ],
                            "name": "080599"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.5,
                                    "instanceQuota": 3.5,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-eReefs",
                                    "usagePatterns": "The images will be created and destroyed many times over by one user (me) for the next year or so as configurations are tested and then, finally when correctly configured, most will be left to deliver very small amounts of metadata as described above.",
                                    "useCase": "These machines are low network and computational volume machines to be used for configuration testing for a number of geospatial Web Services. they will mostly deliver small amounts of metadata (vocabularies of terms, data models in XML, metadata documents in XML). They will be created and destroyed regularly at first until successful configurations are made then kept for a while."
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM TeachingCloudComputing",
                                    "usagePatterns": "The students will be harvesting Tweets in real time and pushing these into CouchDB (actually GeoCouch). They will then run a range of analyses on these databases using for example MapReduce based approaches. The data will not likely be large (several gigabytes or up to 30-40Gb?) per team. The individual CouchDB instances will be aggregated to produce a single populated CouchDB instance. ",
                                    "useCase": "I am to teach Cloud computing to grad students in Melbourne. They will be developing Cloud apps for harvesting Twitter data and pushing it into CouchDB for a range of data analytics scenarios. There will be 8 groups (with 4-5 team members per group). I want them to develop/deploy their applications across multiple Cloud resources (not just Melbourne for example). Note the core CPU hours above will not likely be contiguous. The students will use these machines in an ad hoc manner."
                                }
                            ],
                            "name": "080505"
                        }
                    ],
                    "name": "0805"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "versi.edu.au",
                                    "name": "Cyrus Keong Test server",
                                    "usagePatterns": "",
                                    "useCase": "I am using one \"small\" VM for my ANDS swinburne project. since I am versi staff very hard to get a VM from swinburne university. for more info about this project please contact: jared.winton@versi.edu.au I am using another \"small\" VM for my synchrotron nectar project- SAXS beamline. for more info about this project\" please contact: ulrich.felzmann@synchrotron.org.au I need a medium VM for testing the boto code for fire up a virtual machine on NeCTAR."
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 25.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM TeachingCloudComputing",
                                    "usagePatterns": "The students will be harvesting Tweets in real time and pushing these into CouchDB (actually GeoCouch). They will then run a range of analyses on these databases using for example MapReduce based approaches. The data will not likely be large (several gigabytes or up to 30-40Gb?) per team. The individual CouchDB instances will be aggregated to produce a single populated CouchDB instance. ",
                                    "useCase": "I am to teach Cloud computing to grad students in Melbourne. They will be developing Cloud apps for harvesting Twitter data and pushing it into CouchDB for a range of data analytics scenarios. There will be 8 groups (with 4-5 team members per group). I want them to develop/deploy their applications across multiple Cloud resources (not just Melbourne for example). Note the core CPU hours above will not likely be contiguous. The students will use these machines in an ad hoc manner."
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "EndoVL",
                                    "usagePatterns": "",
                                    "useCase": "The VMs will be used for endoVL project that needs to do some bioinformatic proceedings on the cloud. "
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "EndoVL",
                                    "usagePatterns": "",
                                    "useCase": "To be used for collaboration work between CSIRO and Melb Uni / eResearch group - this is part of endoVL project. We are aiming to produce a solution where CSIRO's TrustStore encryption technology can be used in the bioinformatics processing for endoVL"
                                },
                                {
                                    "coreQuota": 3.6,
                                    "instanceQuota": 1.8,
                                    "institution": "swin.edu.au",
                                    "name": "Smart Services CRC Foundry Project",
                                    "usagePatterns": "",
                                    "useCase": "Hi, I am currently involved with the Smart Services CRC and am building a cloud based protoype for which I require a minimum of 3 instances at any given time. One instance will be long running while the remaining two will be turned on/off on demand. At the moment I can only start 2 VMs and would like access to atleast 1 more so that I can demonstrate my prototype system."
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "murdoch.edu.au",
                                    "name": "centre for comparative genomics",
                                    "usagePatterns": "Typically small user base for applications. This initial allocation request is not using large datasets. (Depending on your definition of large of course).",
                                    "useCase": "genomics, staging area for software development projects, workshops at eResearch 2012.  BTW not really sure how to respond to the core hours question. Over what time frame is that for? Per day/week/year or the lifetime of the project?"
                                },
                                {
                                    "coreQuota": 1.8,
                                    "instanceQuota": 1.8,
                                    "institution": "sydney.edu.au",
                                    "name": "GVL-SCF image testing",
                                    "usagePatterns": "Monday to Friday mostly. Sometimes I will have the image creating running overnight. There will be just me using the testing framework.  There is no large datasets required, except for what is needed to create the galaxy base image.",
                                    "useCase": "I am part of the Genome Virtual Laboratory, and I am working on getting the Science Collaboration Framework working within the GVL galaxy VM image.  I have it basically working now, and I want to test it more thoroughly now. I want to optimise the image creation script, so I can run it from one image, and it will install all the necessary prereqs so it can create the image on another VM."
                                }
                            ],
                            "name": "080309"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 340.0,
                                    "instanceQuota": 340.0,
                                    "institution": "bioplatforms.com",
                                    "name": "Bioplatforms-Au-NGS-training-course",
                                    "usagePatterns": "These VM's will be used for a  3-day workshop 11-13th February.  Additionally a week before we will start testing and uploading the necessary data sets. We will have approx 35 users and will use some small to medium data sets for analysis.",
                                    "useCase": "We would like to use cloud instances to run a national 3-day Next Generation Sequence workshop in Canberra, ANU (11-13 Feb). It will involve analysis of small data sets.  We would like to increase my current allocation of 25 to 40 to cover increasing the number of attendee from 25 to 40 persons.  geographic_requirements. Could I also add Dr Sean Li as an additional manager of this allocation.  Sean email address is sean.li@csiro.au"
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "KNIME Hadoop extension",
                                    "usagePatterns": "Initially, the project will require only a single developer leading to six users with the extension. It will consist of small datasets initially during development and testing, but leading to analysis of Tissue Mass Spec imaging and Next Generation Sequencing datasets with up to 100GB per analysis.",
                                    "useCase": "The Konstanz Information Miner (knime.org) as an Eclipse Rich Client Platform is ready for a hadoop (apache.hadoop.org) extension. The applicant is part of plantcell.unimelb.edu.au, focussed on bioinformatics analysis. Although hadoop images are available on the Nectar cloud, it is for v1 of the software, and I intend to use Hadoop v2 (currently in alpha) as this provides more features As part of this project, we aim to: 1) write an extension to the KNIME platform to make it easy for users to lodge MapReduce programs 2) speed common operations: GroupBy and Join for large datasets using Apache Pig 3) provide data mining extensions using Apache Mahout (eg. SVM) which scale to the cloud 4) authenticate access using LDAP to the UNIMELB domain "
                                },
                                {
                                    "coreQuota": 100.0,
                                    "instanceQuota": 100.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "GenomicsVL Students",
                                    "usagePatterns": "The primary use case is this: genomics end-users (postdocs and students) will start their own GVLs, each on a med instance, and add one Xlarge instance as a worker node. Each GVL would be expected to use perhaps 200GB of storage each (100GB for reference data and 100GB for tools and user data). These per-student GVLs will initially be started in and used for tutorials, and then remain in the student's 'possession' for such time until they make their own allocation request to NeCTAR. This will allow for students to complete all tutorials, become familiar with the GVL, start their own analyses and understand the resource requirements for a working GVL. Users will be encouraged to store data and analyses in persistent volumes that can then be remounted for subsequent use. The pool of 200 cores will allow us to conduct tutorials with 20 (med + x.large = 2+8 = 10 cores each) GVLs concurrently (as for a tutorial with 20 students). We are not completely sure how students will continue to use their GVL instances, so this will also be something of a pilot; clearly we cannot support all 20 GVLs (=200 cores) persistently, so we will need to manage this, perhaps by limiting the time that the student GVLs are 'alive' to a month, in which time students need to request their own allocation and restart their GVL instance using that; this will free up cores in the GenomicsVL-student pool for the next tutorial. In this way we hope to expose many end-users to the GVL, and encourage a subset to deploy and manage their own, perhaps on behalf of several users in their research group, for instance. ",
                                    "useCase": "This project will provide for per-student GenomicsVL instances. The specific use case is detailed below."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics Compute",
                                    "usagePatterns": "The instance will be used for short periods of 1-3 weeks by the applicant - Kevin Gillinder. And will be purely for data analysis. Analysed files will be then transferred to our Cancer Genomics project for longer-term storage and visualisation on genome browsers.",
                                    "useCase": "We are a genomics laboratory requiring compute to perform bioinfomatic analysis of our sequencing data collections. This application is for a VM to run intermittently and perform compute intensive tasks, for example mapping sequencing reads to the human and mouse genomes."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_CI_Server_SDSL",
                                    "usagePatterns": "We have about 10 users and few files (<1000) of up to 2 GB. ",
                                    "useCase": "Automated Builds and Testing for a Data Structure Library I am developing an open source C++ library for succinct data structures, called SDSL (see https://github.com/simongog/sdsl-lite). It is developed together with other international researchers and therefore hosted on github.com to make it easy to contribute code. However, there is now service, which runs our test suits automatically. With the nectar cloud infrastructure, we can easily set up a continues integrations server (like jenkins) which builds and tests the software after each new commit. The software library itself is the basis for different software packages in Bioinformatics and Information Retrieval. Don't hesitate to contact me, if you need more details. I am located at level 8 DMD and can drop by very quickly."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Genome_assembly",
                                    "usagePatterns": "My experiments will deal with large data set of witch the size is about 10 gb. But I will be the only one to involve with the programming part of this program. ",
                                    "useCase": "The cloud will be used for processing the next-generation sequencing data. I'll try to solve the repeat problem in genome assembly which is based on K-mer method.  This will help biologist to build more complete reference genome. Since this program needs large computer memory and powerful computational ability, I ask for help. "
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 25.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_PeterMac_Bioinformatics_Pipelines",
                                    "usagePatterns": "Around 4 users. Very large data sets (next-generation sequencing data). ",
                                    "useCase": "The cloud instances will be used to analyse cancer genomics data generated by Next Generation Sequencers.  Recent technological advances in cancer research have led to substantial development in bioinformatics tools and methods. We have built VMs that hold the best-practice analysis pipelines. The VMs will be used by both internal users (under this project) and external users (who will launch our images under other projects). I hope you have the capacity to provide the required support. "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_NGSANE",
                                    "usagePatterns": "Large number of users with small fixed data set (provided toy data). ",
                                    "useCase": "Cluster with storage to showcase our sequence data analysis framework, NGSANE (https://github.com/BauerLab/ngsane), for the bioinformatics publication currently under review. One reviewer requested that our software should be testable without lengthy dependency installation and toy data download."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.1,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more object and storage space."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.1,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more volume storage space."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.2,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Genome_assembly",
                                    "usagePatterns": "My experiments will deal with large data set of witch the size is about 10 gb. But I will be the only one to involve with the programming part of this program. ",
                                    "useCase": "The cloud will be used for processing the next-generation sequencing data. I'll try to solve the repeat problem in genome assembly which is based on K-mer method.  This will help biologist to build more complete reference genome. Since this program needs large computer memory and powerful computational ability, I ask for help. "
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 1.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_PeterMac_Bioinformatics_Pipelines",
                                    "usagePatterns": "Around 4 users. Very large data sets (next-generation sequencing data). ",
                                    "useCase": "The cloud instances will be used to analyse cancer genomics data generated by Next Generation Sequencers.  Recent technological advances in cancer research have led to substantial development in bioinformatics tools and methods. We have built VMs that hold the best-practice analysis pipelines. The VMs will be used by both internal users (under this project) and external users (who will launch our images under other projects). I hope you have the capacity to provide the required support. "
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more volume storage space."
                                }
                            ],
                            "name": "080301"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.5,
                                    "instanceQuota": 3.5,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-eReefs",
                                    "usagePatterns": "",
                                    "useCase": "To be used for the long-running eReefs project which is about building an informatics platform to allow the fusion of multiple datasets about the Great Barrier Reef, both pre-existing and new."
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                }
                            ],
                            "name": "080302"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Test VMs",
                                    "usagePatterns": "Usage will be limited to trial basis only with the view to migrate systems to separate projects when they mature.",
                                    "useCase": "QUT is looking to support researchers in the use of Nectar and RDSI. In order to do this trial virtual machines will be used to prototype services to access RDSI storage. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM_PERMIS",
                                    "usagePatterns": "I will be the only user using carrying out the experiments. Only very small data sets will be used.",
                                    "useCase": "Hi, I am a current PhD student at the University of Melbourne under the supervision of Prof. Richard Sinnott. As part of my PhD, I will be running experiments on the cloud and require 4 virtual machines. I will be storing very little data in the databases that I will deploy to each VM, therefore, I do not need much storage capacity. "
                                }
                            ],
                            "name": "080303"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "utas.edu.au",
                                    "name": "cloud operations (internal)",
                                    "usagePatterns": "Occasional short-burst CPU, network and IO - will keep it short.",
                                    "useCase": "Hi, Nigel from TPAC, while we are building out our end of the network-of-nodes, I would like to be able to have a short-lived VM on each cell please so we can compare and contrast. "
                                }
                            ],
                            "name": "080307"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "versi.edu.au",
                                    "name": "VeRSI",
                                    "usagePatterns": "Most instances will be for relatively short term demonstrators and proof of concepts with no need for large or persistent data hosting.",
                                    "useCase": "Through my engagements at various Victorian universities I find it useful to be able to demonstrate the capabilities of the research cloud. It would be good for me to have a little more head room over and above the current 2 VM limit."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "intersect.org.au",
                                    "name": "RDSI_LiveArc_Dimensioning",
                                    "usagePatterns": "INTERSECT and QCIF will take part on this dimensioning exercise. We expect to store large datasets. Number of users may vary. We expect to have 10-12 users accessing the data using LiveArc instance. We will also transfer data between the two nodes (INTERSECT and QCIF) as part of this exercise.  We need the storage that is designed for research data (larger). O/S should be CentOS 6.4 64bit.",
                                    "useCase": "This VM will be used for LiveArc Dimensioning exercise for RDSI project."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 4.0,
                                    "institution": "intersect.org.au",
                                    "name": "RDSI_LiveArc_Dimensioning",
                                    "usagePatterns": "INTERSECT and QCIF will take part on this dimensioning exercise. We expect to store large datasets. Number of users may vary. We expect to have 10-12 users accessing the data using LiveArc instance. We will also transfer data between the two nodes (INTERSECT and QCIF) as part of this exercise.  We need the storage that is designed for research data (larger). O/S should be CentOS 6.4 64bit.",
                                    "useCase": "This VM will be used for LiveArc Dimensioning exercise for RDSI project."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "cqu.edu.au",
                                    "name": "Access Grid International Website Hosting",
                                    "usagePatterns": "Most of the content that will be accessed is in the simple form of webpages.  Additional to this, the site will also provide the Access Grid Software downloads (similar to http://www.accessgrid.org/software).  As an example, the latest Windows Bundle installer for the Access Grid Software is 75MB in size.",
                                    "useCase": "I would like to request a VM which will help support the Australian (and International) Access Grid Community. As you might not be aware, ANL withdrew support for the open source Access Grid project.  From this, the International community, as well as leveraging open services such as sourceforge, combined to provide resources to continue the Access Grid Project. One of these services included the International Access Grid Web Site (See http://www.accessgrid.org/ - which currently runs on Drupal).  WestGrid (Canada) currently provides this service on a temporary basis, until a new permanent location/website could be arranged.  The server infrastructure currently hosting the website is reaching end of life and WestGrid can no longer provide this service and host this website. Though the Access Grid is slowly losing support  particularly internationally, there is still significant usage (and even new uptake  especially with AMSI [Australian Mathematical Sciences Institute]) and it would be good if this Research and Teaching Tool could continue to be supported. There are a number of us within the international community who would be happy to setup and support the servers, we just need the infrastructure to host the international website. "
                                },
                                {
                                    "coreQuota": 100.0,
                                    "instanceQuota": 100.0,
                                    "institution": "monash.edu",
                                    "name": "MCC_On_R@CMON",
                                    "usagePatterns": "These VMs will be running CPU and CPU+I/O heavy jobs, so the use of the volume storage + ephemeral disks will be crucial to achieving good performance.",
                                    "useCase": " Hi Steve Id like to request the allocation of 10 % of the Monashs discretionary share for R@CMON/NeCTAR as an extension of the Monash Central HTC/HPC Compute Capability [MCC]. This project aims to provision multiple 8-core/16-core, 32-64GB RAM VM instances as compute nodes to add into the batch queueing system. Mechanisms will be developed to enable jobs to migrate from the central system to the VMs. Please contact us if you need further information. kind regards, philip I've requested for 1 TB of Object Storage for the potential to use this to \"store and forward\" data that is relevant to compute."
                                }
                            ],
                            "name": "0803"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 100.0,
                                    "instanceQuota": 100.0,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_Research_Incubator",
                                    "usagePatterns": "Many users, many small to medium data sets.",
                                    "useCase": "This project is to set up a Research Incubator for use by University of Sydney researchers. It will be managed and promoted by University of Sydney ICT. There will be many small projects hosted within this instance. This has been discussed by Steve Manos and Heath Cooper."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_National_Twitter_Harvesting",
                                    "usagePatterns": "The project will have 2-3 developers/users. The small VMs will be used to connect to Twitter to harvest Tweets that will be stored in local CouchDB instances on those VMs. These instances will be (periodically) incorporated into a large CouchDB instance for large scale machine learning/classification and use a range of MapReduce-like algorithms. Note that this project storage was accepted with a VicNode storage request for 5Tb. I suggest that only 1Tb is used to begin with as we scale up.",
                                    "useCase": "As per previous request, this work will establish/enhance Twitter data harvesting applications and extend them to run across all of Australia. This allocation is associated with a VicNode allocation that has been applied for/granted.  The original case required (initially) 9 small VMs and I asked for 1 large VM (4CPU/16GB RAM) for machine learning/analytics.  This request is for this large VM which wasn't allocated previously. This work will underpin a range of University wide efforts and provide a direct resource for use in the AURIN project."
                                },
                                {
                                    "coreQuota": 9.0,
                                    "instanceQuota": 9.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_National_Twitter_Harvesting",
                                    "usagePatterns": "The project will have 2-3 developers/users. The small VMs will be used to connect to Twitter to harvest Tweets that will be stored in local CouchDB instances on those VMs. These instances will be (periodically) incorporated into a large CouchDB instance for large scale machine learning/classification and use a range of MapReduce-like algorithms",
                                    "useCase": "This work will establish/enhance Twitter data harvesting applications and extend them to run across all of Australia. This allocation is associated with a VicNode allocation that has been applied for/granted.  The case itself requires (initially) 9 small VMs and 1 large VM (4CPU/16GB RAM) for machine learning/analytics. I haven't made a separate allocation request for the large VM but can do so if required. (It would be good to be able to request for example 1 large, 3 medium, 2 small in the user interface here!) This work will underpin a range of University wide efforts and provide a direct resource for use in the AURIN project."
                                },
                                {
                                    "coreQuota": 9.0,
                                    "instanceQuota": 9.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_National_Twitter_Harvesting",
                                    "usagePatterns": "The project will have 2-3 developers/users. The small VMs will be used to connect to Twitter to harvest Tweets that will be stored in local CouchDB instances on those VMs. These instances will be (periodically) incorporated into a large CouchDB instance for large scale machine learning/classification and use a range of MapReduce-like algorithms. Note that this project storage was accepted with a VicNode storage request for 5Tb. I suggest that only 1Tb is used to begin with as we scale up.",
                                    "useCase": "As per previous request, this work will establish/enhance Twitter data harvesting applications and extend them to run across all of Australia. This allocation is associated with a VicNode allocation that has been applied for/granted.  The original case required (initially) 9 small VMs and I asked for 1 large VM (4CPU/16GB RAM) for machine learning/analytics.  This request is for this large VM which wasn't allocated previously. This work will underpin a range of University wide efforts and provide a direct resource for use in the AURIN project."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "monash.edu",
                                    "name": "SAXS Data Reduction Cloud",
                                    "usagePatterns": "",
                                    "useCase": "Provide online access to \"scatterBrain\", Small Angle X-ray Scattering data reduction software developed in house at the Australian Synchrotron. The application will run on a virtual desktop (e.g., NX) on each virtual machine. Data will be accessed across the network, with no significant requirement for local storage on the NeCTAR cloud."
                                }
                            ],
                            "name": "080399"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "MoViT - Mobile Vision Tester",
                                    "usagePatterns": "The server will host collections of images that will migrate to the iPad before testing begins. These could be large (several GBs) per user, with only 2 to 3 users initially. These images will only be accessed intermittently (say monthly). Each user will also upload about 20 (say) small text files per day; log files of tests conducted on the iPads.  It is envisaged that during 2013 the server will only have a handful of users. We hope to recruit users during this period, and envisage other groups from around the world joining the project.",
                                    "useCase": "MoViT is an iPad platform for visual psychophysics developed a the University of Melbourne. As the iPad has limited local storage an filesystem, this virtual server will be used for storing configuration files and images for vision tests and for saving log files from vision tests. The app will be deployed for testing people with normal health and vision, in addition to cohorts with migraine, schizophrenia and autism both through unimelb and UWA. Ethics approval for the relevant projects has already been obtained through the host universities HRECs.  Upon completion of these pilot projects, it will be made available worldwide for other users to employ in research studies. We are hoping to provide a stable server for about 5 years (say) while we evaluate the utility of the app, its appeal in the broader psychophysics community, and build a user base. Once it becomes a vital piece of infrastructure for vision research, or traffic/usage becomes heavy, we will transition to a \"user pays\" model for providing the server."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "monash.edu",
                                    "name": "Meta-Batch-Scheduler Testing",
                                    "usagePatterns": "Small number of Users with small data sets. one or two sql and/or ldap servers.",
                                    "useCase": "Monash eResearch Centre's HPC division is developing a new Nimrod meta-batch-scheduler, which will submit jobs to multiple batch-schedulers based on Network and Hardware Locality. We need to test the prototype on small cluster.  "
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_RTrack01",
                                    "usagePatterns": "Large data sets with a small number of users, tens of gigabytes and only 2-3 users for this proof-of-concept. ",
                                    "useCase": "Model runs of light-level geo-location estimation from marine animals (elephant seals, snow petrels) using  Bayesian methods. These methods are currently being updated and we are exploring parallelization options. This instance would be a small test bed to demonstrate a proof of concept. It could ultimately be an R server service to run these models for researchers at the AAD, IMAS and the ACE CRC. "
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.3,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_RTrack01",
                                    "usagePatterns": "Large data sets with a small number of users, tens of gigabytes and only 2-3 users for this proof-of-concept. ",
                                    "useCase": "Model runs of light-level geo-location estimation from marine animals (elephant seals, snow petrels) using  Bayesian methods. These methods are currently being updated and we are exploring parallelization options. This instance would be a small test bed to demonstrate a proof of concept. It could ultimately be an R server service to run these models for researchers at the AAD, IMAS and the ACE CRC. "
                                }
                            ],
                            "name": "080306"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "qut.edu.au",
                                    "name": "Collaborative Content Creation of Gesture Systems",
                                    "usagePatterns": "Small numbers of users (25 to 50) with data sets ranging up to a 1GB. Back-end request will be made to access the defined data sets via API's small numbers of systems making backend requests.",
                                    "useCase": "We require servers to host both front end and back-end functionality to support research into the collaborative creation and delivery on interactive multimedia datasets for gesture based computing environments. Using the hosted front end people will be able to collaboratively define multimedia data sets, these data sets will then be accessed on the fly by the gesture based computing environments "
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 1.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Twitter_keyword_track",
                                    "usagePatterns": "",
                                    "useCase": "to track the stream tweet from Twitter by twitter API. set up Hadoop on this service to do the sentiment analysis with the tweet data. It is the final year project for my coursework. My supervisor is Dr Aaron Harwood "
                                }
                            ],
                            "name": "080305"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.6,
                                    "institution": "monash.edu",
                                    "name": "CAVE2 Development",
                                    "usagePatterns": "A few users (CAVE2 developers, internal and external to Monash e-Research) and each having a few large data sets.",
                                    "useCase": "We are working with R@CMON staff at Monash to develop the capability to use Cloud systems to develop for the Monash CAVE2. The principal field of work is scientific visualisation and high performance graphics rendering."
                                }
                            ],
                            "name": "080304"
                        }
                    ],
                    "name": "0803"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Modelling Algorithm Behaviour",
                                    "usagePatterns": "The project has only one user, me. It requires large data set. There will be around 4,000 experiments to be executed, four experiments require one process. Thus, with 128 processes running per batch, I need to reuse NECTAR for 8 times.",
                                    "useCase": "The instances will compute optimal schedules on various dynamical environmental settings using an innovative Evolutionary Algorithm to form a model that will predict the algorithm's behavior. From this behavior, algorithm's components and sub-algorithms will be justified."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Interactive Shape Optimisation",
                                    "usagePatterns": "About 30 simulations will be conducted over a small number of weeks. Each simulation will require roughly 300 CPU hours.  The instance will be used for a few hours at a time and shut down in between runs. User interaction will require some communication to an external server with a negligible amount of data being transmitted. ",
                                    "useCase": "The Instance will be used as part of a case study on using Interactive Multi-Objective Particle Swarm Optimisation on a Jet Engine Compressor Blade design problem. The instance will be used for the computational fluid dynamics simulation tool and the algorithm. The requested number of core hours represents an upper limit and will likely be less."
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 3.2,
                                    "institution": "uq.edu.au",
                                    "name": "Neural Netetwork Complex Dynamics Parameter Search",
                                    "usagePatterns": "This project will most likely continually run one virtual machine with the requested persistent volume storage attached.  This virtual machine will be used for debugging test experiments and storing data.  For larger experiments, a number of virtual machines will be spawned for the duration of a simulation and terminated after the job has been run.  This dynamic allocation approach aims to minimize the amount of resources that are continually being used, while greatly increasing the computational capacity for simulating neural networks.",
                                    "useCase": "The Complex and Intelligent Systems group (led by Prof J Wiles in ITEE) simulates a range of computational models of biological phenomena. The group is seeking computational resources to support a number of these projects.  I am currently conducting neural network simulations to study complex network activity across wide ranges of input parameters.  I intend to use these resources for two aspects of my project.  1) to increase the number of parameter sets I am able to simulate simultaneously. I am currently limited to 8 simultaneous simulations on my work desktop.   and 2) to increase the size of simulated networks.  I am currently limited to ~250 neurons, while some complex network dynamics only emerge with greater than ~1000 neurons.  The brain is able to continually change its activity state in response to internal representations and external stimuli.  This body of work aims to elucidate how various neural architectures are able to self-regulate these changes in activity states. Specifically, we would like to study dynamical switching mechanisms in neural networks that exhibit complex self-sustained activity.  Some of the neural regions of interest have over a million neurons and we expect that simulations siees will continue to improve with the development of modelling techniques coupled to increases in computational power."
                                }
                            ],
                            "name": "080108"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 5.6,
                                    "instanceQuota": 5.6,
                                    "institution": "sydney.edu.au",
                                    "name": "Autonomous soaring",
                                    "usagePatterns": "",
                                    "useCase": "This project requires the generation, storage and processing of large amounts of simulated flight data to perform online machine learning. The goal of the project is to learn optimal policies for flight trajectory planning to perform autonomous soaring on an unmanned autonomous glider in an unknown wind field."
                                },
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 14.4,
                                    "institution": "sydney.edu.au",
                                    "name": "Autonomous soaring 2",
                                    "usagePatterns": "This project will have one user running one instance generating large data sets. The data will be accessed by multiple users from different locations.",
                                    "useCase": "I intend to use the cloud instances to run further tests and simulations on machine learning methods for performing autonomous soaring in under powered unmanned aerial vehicles. This learning algorithm will require extensive validation, generating large data sets that will be accessed by multiple users."
                                },
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 0.9,
                                    "institution": "sydney.edu.au",
                                    "name": "Autonomous soaring 2",
                                    "usagePatterns": "This project will have one user running one instance generating large data sets. The data will be accessed by multiple users from different locations.",
                                    "useCase": "I intend to use the cloud instances to run further tests and simulations on machine learning methods for performing autonomous soaring in under powered unmanned aerial vehicles. This learning algorithm will require extensive validation, generating large data sets that will be accessed by multiple users."
                                },
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_ExploreTheSeafloor",
                                    "usagePatterns": "We will have large numbers of files that contain data related to the annotation of seafloor imagery.  The imagery itself is hosted elsewhere.",
                                    "useCase": "We are looking for an accessible machine to store and analyse data related to the Science Week Explore The Seafloor project (exploretheseafloor.net.au)."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 0.5,
                                    "institution": "unsw.edu.au",
                                    "name": "PhDProj",
                                    "usagePatterns": "My project will be a single user/large data project. Ideally, I would like to run several instances of the program as I need to conduct numerous statistical studies, with one point representing one run.",
                                    "useCase": "PhD Research Project Optimal Flight Paths for Engine-Out Large Transport Aircraft over Mountainous Terrain The program is a java run time and usually takes 3-8 hours to run."
                                },
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_ExploreTheSeafloor",
                                    "usagePatterns": "We will have large numbers of files that contain data related to the annotation of seafloor imagery.  The imagery itself is hosted elsewhere.",
                                    "useCase": "We are looking for an accessible machine to store and analyse data related to the Science Week Explore The Seafloor project (exploretheseafloor.net.au)."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 0.5,
                                    "institution": "unsw.edu.au",
                                    "name": "PhDProj",
                                    "usagePatterns": "My project will be a single user/large data project. Ideally, I would like to run several instances of the program as I need to conduct numerous statistical studies, with one point representing one run.",
                                    "useCase": "PhD Research Project Optimal Flight Paths for Engine-Out Large Transport Aircraft over Mountainous Terrain The program is a java run time and usually takes 3-8 hours to run."
                                }
                            ],
                            "name": "080101"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "csiro.au",
                                    "name": "Phoenix Data Relay",
                                    "usagePatterns": "There will only be 1 or 2 users on this VM.",
                                    "useCase": "Looking at trialling the service for relaying data to and from our tele-operated vehicles back to QCAT at Pullenvale. Examples include the Phoenix, Bobcat, Quadbike, Quadrotor and helicopter."
                                },
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 5.0,
                                    "institution": "monash.edu",
                                    "name": "AutoDesk Maya Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly.",
                                    "useCase": "My research group (http://infotech.monash.edu/research/groups/3dg/)  is engaged in a wide range of projects, many with an emphasis on the visualisation of the past (history and archaeology) and cultural heritage.  To date my team and I have gotten by rendering 3D visualisation research on our laptops at relatively low resolution, mainly for display on the internet. As the animations were neither long (in time) or large (in screen size), this was more less manageable. Recently there has been an increasing trend towards large scale landscapes and environments, as well as new formats for display (the new CAVE installation at Monash, for example, which requires images at resolutions of 27,000 pixels). We are currently quite compromised in what we can achieve using 3-4 Mac Book Pro laptops.  The preeminent projects where a rendering farm is required are the Visualising Angkor Project and the Monash Country Lines Archive. 1. The Visualising Angkor project explores the 3D generation and animation of landscapes, people, soundscapes and architecture in a medieval century Cambodian metropolis. The resulting scenes draw upon a wide range of archaeological and historical data, from bas-reliefs to Chinese eye-witness accounts and extensive mapping undertaken by the Greater Angkor Project and the EFEO. In comparison to the familiar historical staples of Rome, Greece and Egypt, the virtual image of Angkor remains unexplored. The recent inclusion of Angkor as a subject of study in the Australian national High School history curriculum is timely, but it also presents some interesting challenges. 2. The Monash Country Lines Archive (MCLA) is a collaborative Monash University project between the Monash Indigenous Centre (MIC), Faculty of Arts and the Faculty of Information Technology with a team of Monash researchers, digital animators and post-graduate students from the Monash Indigenous Centre, Faculty of Arts and the Faculty of Information Technology. The Monash Country Line Archive demands intellectual engagement in regards to issues associated with how best to construct a living archive that is a decolonised space in which communities are happy to see their material stored. It also provides an exciting place for scholars to work and share knowledge. "
                                }
                            ],
                            "name": "080111"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Machine Translation for Natural Language Preservation",
                                    "usagePatterns": "Data sets will be around 1 or 2 GBs. Experiments will be run that will run for up to a couple days, with anticipated gaps of up to a week or more between.",
                                    "useCase": "Experiments for a research master's project: Machine translation experiments will be run in order to investigate the application of new and promising algorithms to the translation of dying unwritten languages into larger ones such as English."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 1.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Twitter_keyword_track",
                                    "usagePatterns": "",
                                    "useCase": "to track the stream tweet from Twitter by twitter API. set up Hadoop on this service to do the sentiment analysis with the tweet data. It is the final year project for my coursework. My supervisor is Dr Aaron Harwood "
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd Moderator Assitant",
                                    "usagePatterns": "~100 users, not a big data set",
                                    "useCase": "Moderator Assistant is a project funded by the Young and Well CRC to develop a tool that helps moderators of mental health peer-support groups. This website will be used to annotate data and generate feedback by interns and users at the CRC and the Inspire Foundation "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Machine Translation for Natural Language Preservation",
                                    "usagePatterns": "Data sets will be around 1 or 2 GBs. Experiments will be run that will run for up to a couple days, with anticipated gaps of up to a week or more between.",
                                    "useCase": "Experiments for a research master's project: Machine translation experiments will be run in order to investigate the application of new and promising algorithms to the translation of dying unwritten languages into larger ones such as English."
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.6,
                                    "institution": "anu.edu.au",
                                    "name": "ANU_Pro-ana_data_service",
                                    "usagePatterns": "The intention is to have 3 instances, each running 2 cores and 2 persistent volumes.  2 instances (preferably geographically separated) will run replicas of mongodb (each using one persistent volume). This will allow system maintenance/failure without interrupting data collection. The third instance will run a data collection script.",
                                    "useCase": "\"pro-ana\" (pro-anorexia) is a recent phenomena on various online social media platforms. We intend to collect data from the Twitter pro-ana community (possibly expanding to other media outlets). This application will provide 2 replicas of a mongodb and one instance for polling the Twitter api 24/7 and storing the data in mongo. Object storage will be used for database backups."
                                }
                            ],
                            "name": "080107"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "monash.edu",
                                    "name": "Forensic Data Ranking and Analysis",
                                    "usagePatterns": "Very large dataset, one to two users typically (maximum three concurrent users).",
                                    "useCase": "Collection, indexing and analysis of geo-tagged multimedia.    Our goal is to develop a ranking and analysis tool, primarily focussed on acceleration of existing multimedia forensics analytics.   This research will result in a prototype tool intended for use by international law enforcement agencies for multi-jurisdictional victim identification."
                                },
                                {
                                    "coreQuota": 96.0,
                                    "instanceQuota": 96.0,
                                    "institution": "intersect.org.au",
                                    "name": "HCSvLab",
                                    "usagePatterns": "The project will have a few users with large datasets. This usage pattern is relevant now, during development, but it will likely change once the virtual laboratory goes into full production. The INDRI cluster we want to setup will potentially consist of 10-15 simultaneously running instances. We need additional instances for running other tools.",
                                    "useCase": "This is for testing and development of the HCSvLab NeCTAR Virtual Laboratory Project. We want to setup instances to test tools (e.g. DeMoLib, HTK and INDRI) that will be run over data sourced from the virtual laboratory. One particular use case will be to setup a cluster for indexing very large datasets using INDRI. The dataset is called ClueWeb09 and is 25TB (http://lemurproject.org/clueweb09/). The clustering will potentially be done using Hadoop. "
                                },
                                {
                                    "coreQuota": 96.0,
                                    "instanceQuota": 6.0,
                                    "institution": "intersect.org.au",
                                    "name": "HCSvLab",
                                    "usagePatterns": "The project will have a few users with large datasets. This usage pattern is relevant now, during development, but it will likely change once the virtual laboratory goes into full production. The INDRI cluster we want to setup will potentially consist of 10-15 simultaneously running instances. We need additional instances for running other tools.",
                                    "useCase": "This is for testing and development of the HCSvLab NeCTAR Virtual Laboratory Project. We want to setup instances to test tools (e.g. DeMoLib, HTK and INDRI) that will be run over data sourced from the virtual laboratory. One particular use case will be to setup a cluster for indexing very large datasets using INDRI. The dataset is called ClueWeb09 and is 25TB (http://lemurproject.org/clueweb09/). The clustering will potentially be done using Hadoop. "
                                }
                            ],
                            "name": "0801"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 7.2,
                                    "instanceQuota": 1.8,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_CAI_Mouse_Brain_ANDS",
                                    "usagePatterns": "The large storage is needed for the pre-tiling of the images to speed up delivery time for the end-user. Increased CPU usage will happen as part of the pre-tiling process but won't last for too long a time. Otherwise CPUs will be stressed only when new, un-tiled data is added so that the image-server has to perform the tiling on-the-fly. Likewise RAM requirements will peak during pre-tiling and whenever we add data sets that are not already pre-tiled but generated on the go.",
                                    "useCase": "Primarily the nodes will run Tissue Stack, a software system that allows the viewing of 3D data sets as 2D cross sections."
                                },
                                {
                                    "coreQuota": 7.2,
                                    "instanceQuota": 1.8,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_CAI_Mouse_Brain_ANDS",
                                    "usagePatterns": "The large storage is needed for the pre-tiling of the images to speed up delivery time for the end-user. Increased CPU usage will happen as part of the pre-tiling process but won't last for too long a time. Otherwise CPUs will be stressed only when new, un-tiled data is added so that the image-server has to perform the tiling on-the-fly. Likewise RAM requirements will peak during pre-tiling and whenever we add data sets that are not already pre-tiled but generated on the go.",
                                    "useCase": "Primarily the nodes will run Tissue Stack, a software system that allows the viewing of 3D data sets as 2D cross sections."
                                }
                            ],
                            "name": "080106"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "monash.edu",
                                    "name": "C4D Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly. ",
                                    "useCase": "We have a need to render a large amount high resolution 3D computer animation for a visualisation project. The research involves developmental simulations that mimic biological growth and morphogenesis. Simulation software we have developed generates very complex geometric models that are too large for realtime visualisation, hence the need for distributed rendering."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 2.0,
                                    "institution": "monash.edu",
                                    "name": "C4D Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly. ",
                                    "useCase": "We have a need to render a large amount high resolution 3D computer animation for a visualisation project. The research involves developmental simulations that mimic biological growth and morphogenesis. Simulation software we have developed generates very complex geometric models that are too large for realtime visualisation, hence the need for distributed rendering."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 2.4,
                                    "institution": "monash.edu",
                                    "name": "CAVE2 Development",
                                    "usagePatterns": "A few users (CAVE2 developers, internal and external to Monash e-Research) and each having a few large data sets.",
                                    "useCase": "We are working with R@CMON staff at Monash to develop the capability to use Cloud systems to develop for the Monash CAVE2. The principal field of work is scientific visualisation and high performance graphics rendering."
                                }
                            ],
                            "name": "080103"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "monash.edu",
                                    "name": "TwitterActivity-FIT4008_S2",
                                    "usagePatterns": "",
                                    "useCase": "I'm an honours student at Monash University Clayton (Faculty of IT), under the supervision of Dr. John Betts, doing some research on Twitter activity on a world-wide scale and I'm writing to you to make a request for a virtual machine with which I will use for the collection of public Twitter activity data."
                                },
                                {
                                    "coreQuota": 11.2,
                                    "instanceQuota": 1.4,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_Velodyne_processing",
                                    "usagePatterns": "I have ~400 GB of datasets which would be read only (with one user). I cache aggressively to speed up data processing. This takes up to 1TB, although I can reduce this, and is write once, read many.   Finally I save results to analyse on my local machine. I'd need only a few GB for this. ",
                                    "useCase": "For my research I am processing large datasets of 3D laser and camera data. This requires a lot of hard drive space (we have terabytes of data), and also a lot of CPU time. For my thesis I am developing new processing algorithms, and this requires continual testing and analysis. Currently I am running algorithms over night on my desktop computer, but having access to more computing resources would greatly speed up my work. My research is in field robotics - more specifically developing algorithms for dynamic scene understanding - which is applicable to self-driving cars, and other safety and security uses. I have around 4 months to finish my thesis, and would like to be able to scale up the size of datasets I am working on. This extra processing power would allow me to do that. I can run many experiments at once meaning I would make full use of the cores.  The CPU time estimate above is only rough, I could use much more if it were available. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "monash.edu",
                                    "name": "TwitterActivity-FIT4008_S2",
                                    "usagePatterns": "",
                                    "useCase": "I'm an honours student at Monash University Clayton (Faculty of IT), under the supervision of Dr. John Betts, doing some research on Twitter activity on a world-wide scale and I'm writing to you to make a request for a virtual machine with which I will use for the collection of public Twitter activity data."
                                }
                            ],
                            "name": "080199"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "monash.edu",
                                    "name": "Monash_EFTPOS_mining1",
                                    "usagePatterns": "2-3 users running single multi-core VMs for development and testing",
                                    "useCase": "The project involves running data mining algorithms to find meaningful patterns from historical EFTPOS transaction data.  The data has been categorised commercial in confidence.  The research cloud is needed to satisfy the data security requirements outlined by the data owner, i.e.  a sandboxed environment which keeps other users (e.g., on a shared system) from accessing the data and which will allow the work to scale using distributed computing tools and methods (e.g. Hadoop)"
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "monash.edu",
                                    "name": "Monash_EFTPOS_mining1",
                                    "usagePatterns": "2-3 users running single multi-core VMs for development and testing",
                                    "useCase": "The project involves running data mining algorithms to find meaningful patterns from historical EFTPOS transaction data.  The data has been categorised commercial in confidence.  The research cloud is needed to satisfy the data security requirements outlined by the data owner, i.e.  a sandboxed environment which keeps other users (e.g., on a shared system) from accessing the data and which will allow the work to scale using distributed computing tools and methods (e.g. Hadoop)"
                                },
                                {
                                    "coreQuota": 60.0,
                                    "instanceQuota": 30.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM_Trajectory_Inference_Attacks",
                                    "usagePatterns": "Data is stored on a remote server so no storage is needed.",
                                    "useCase": "In this project, an algorithm has been developed to infer a persons road trajectory using POI information sent to a LBS such as Google Maps. We have a number of use cases from the Microsoft Geolife dataset to test the algorithm on. The system that runs the algorithm is distributed so the more machines that run the software, the faster the results will be produced and refinements can be made. A working instance and snapshot has been created which runs the system, all that is required now is the ability to load more instances. Project Summary Mobile smart phones have become ubiquitous. The market success of these devices combined with advancements in mobile network infrastructure has created a strong market for third party apps and location based service (LBS) providers. While the apps and service providers are of great convenience to its users, there are concerns of the privacy implications associated. Smart phones capture a lot of information about a user. We hypothesis that data internal to these devices sent via a network can be combined with rich external data to recreate a very accurate profile about an individual.  While it's often speculated that personal information can be inferred from sensitive data, literature to highlight tools and techniques utilized to invade user privacy is scarce. Abstract Mobile privacy has recently become a well-researched topic in pervasive computing. However, little work has been attempted on inferring personal information about a user such as communications meta-data, trajectories of individuals accessing location-based services, social media services used and if mobile sensors can be used to ID a person. We present a collection of algorithms and techniques that is able to infer information about an individual based on data stored on a mobile device.  An individuals trajectory to a high degree of precision was established given that they send continuous queries requesting the closest point of interest (e.g., restaurant) from a location service provider. The algorithm to infer the users trajectory is based on the query result locations instead of the actual user locations. Legislation may not permit the location service provider to exchange user location data, but may permit the exchange of the query results with third parties. We reveal a users trajectory via an indirect approach showing that user privacy is not preserved with the storage and exchange of query results. Experimental evaluation indicates in some cases that a users trajectory can be determined to a high accuracy given a small set of points in real scenarios.  The current platforms dominating the mobile market including iOS and Android allow for easy collection of user information via the public APIs they provide. We demonstrate how data can be sent to a remote server without the user being aware and highlight mobile applications are requesting excessive use of permissions in the Google Play store relative to the advertised functionality they provide. Keywords: location, privacy, inference attack, trajectory, Voronoi diagram, points of interest, mobile smart phones "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "flinders.edu.au",
                                    "name": "Patient Trajectory Visualization tool",
                                    "usagePatterns": "Large data set with 10-30 users",
                                    "useCase": "We are developing an application that  uses prior health records of mental health patients. The application has 2 components.  HBase and neo4j database of patients prior data. n = 2000 patients, between 50-200 records for each patient. Javascript and HTML5 front end with analytics/visualisation tools. Users n = 20-40  "
                                },
                                {
                                    "coreQuota": 100.0,
                                    "instanceQuota": 100.0,
                                    "institution": "monash.edu",
                                    "name": "Monash_Webb-LearningFromBigData",
                                    "usagePatterns": "There will be three primary users.  Typical data size will initially be around 20Gb although we hope to obtain larger data with which to work as the project progresses.  All users will share the same data and large numbers of processes will be run on each set of data.",
                                    "useCase": "Effective extraction of information from massive data stores is increasingly problematic as data quantities continue to grow rapidly. Quite simply, effective techniques for learning from small data do not scale. However, the problem is even worse than this. Big data contain more information than the small data in which context most state-of-the-art learning algorithms have been developed. For small data overly detailed classifiers will overfit the data and so should be avoided. In contrast, big data provide fine detail and hence will benefit new types of learner that can capture it. This project will deliver learners that are not only capable of capturing this detail, but do so with the efficiency required to process terabytes of data. This project is funded as ARD Discovery Project DP140100087 (total funding 2014-2016: $958,000).  It is one of only 17 Discovery Projects awarded in 2013 to have received a highly-competitive and prestigious Discovery Outstanding Researcher Award. State-of-the-art approaches to learning from big data require holding large data samples in random access memory (RAM) in order to provide fast access. Our envisaged approaches will instead hold in RAM large tables of summary statistics that are efficiently learned by generative learning (maximum likelihood estimation). In order to compare our new approaches with the state-of-the-art, and in order to assess the trade-offs that occur when fewer or greater numbers of summary statistics are employed, we require large amounts of time on computers with large RAM (in some cases up to 100s of gigabytes). "
                                },
                                {
                                    "coreQuota": 30.0,
                                    "instanceQuota": 30.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM_Trajectory_Inference_Attacks",
                                    "usagePatterns": "Data is stored on a remote server so no storage is needed.",
                                    "useCase": "In this project, an algorithm has been developed to infer a persons road trajectory using POI information sent to a LBS such as Google Maps. We have a number of use cases from the Microsoft Geolife dataset to test the algorithm on. The system that runs the algorithm is distributed so the more machines that run the software, the faster the results will be produced and refinements can be made. A working instance and snapshot has been created which runs the system, all that is required now is the ability to load more instances. Project Summary Mobile smart phones have become ubiquitous. The market success of these devices combined with advancements in mobile network infrastructure has created a strong market for third party apps and location based service (LBS) providers. While the apps and service providers are of great convenience to its users, there are concerns of the privacy implications associated. Smart phones capture a lot of information about a user. We hypothesis that data internal to these devices sent via a network can be combined with rich external data to recreate a very accurate profile about an individual.  While it's often speculated that personal information can be inferred from sensitive data, literature to highlight tools and techniques utilized to invade user privacy is scarce. Abstract Mobile privacy has recently become a well-researched topic in pervasive computing. However, little work has been attempted on inferring personal information about a user such as communications meta-data, trajectories of individuals accessing location-based services, social media services used and if mobile sensors can be used to ID a person. We present a collection of algorithms and techniques that is able to infer information about an individual based on data stored on a mobile device.  An individuals trajectory to a high degree of precision was established given that they send continuous queries requesting the closest point of interest (e.g., restaurant) from a location service provider. The algorithm to infer the users trajectory is based on the query result locations instead of the actual user locations. Legislation may not permit the location service provider to exchange user location data, but may permit the exchange of the query results with third parties. We reveal a users trajectory via an indirect approach showing that user privacy is not preserved with the storage and exchange of query results. Experimental evaluation indicates in some cases that a users trajectory can be determined to a high accuracy given a small set of points in real scenarios.  The current platforms dominating the mobile market including iOS and Android allow for easy collection of user information via the public APIs they provide. We demonstrate how data can be sent to a remote server without the user being aware and highlight mobile applications are requesting excessive use of permissions in the Google Play store relative to the advertised functionality they provide. Keywords: location, privacy, inference attack, trajectory, Voronoi diagram, points of interest, mobile smart phones "
                                },
                                {
                                    "coreQuota": 30.0,
                                    "instanceQuota": 30.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM_Trajectory_Inference_Attacks",
                                    "usagePatterns": "Data is stored on a remote server so no storage is needed.",
                                    "useCase": "In this project, an algorithm has been developed to infer a persons road trajectory using POI information sent to a LBS such as Google Maps. We have a number of use cases from the Microsoft Geolife dataset to test the algorithm on. The system that runs the algorithm is distributed so the more machines that run the software, the faster the results will be produced and refinements can be made. A working instance and snapshot has been created which runs the system, all that is required now is the ability to load more instances. Project Summary Mobile smart phones have become ubiquitous. The market success of these devices combined with advancements in mobile network infrastructure has created a strong market for third party apps and location based service (LBS) providers. While the apps and service providers are of great convenience to its users, there are concerns of the privacy implications associated. Smart phones capture a lot of information about a user. We hypothesis that data internal to these devices sent via a network can be combined with rich external data to recreate a very accurate profile about an individual.  While it's often speculated that personal information can be inferred from sensitive data, literature to highlight tools and techniques utilized to invade user privacy is scarce. Abstract Mobile privacy has recently become a well-researched topic in pervasive computing. However, little work has been attempted on inferring personal information about a user such as communications meta-data, trajectories of individuals accessing location-based services, social media services used and if mobile sensors can be used to ID a person. We present a collection of algorithms and techniques that is able to infer information about an individual based on data stored on a mobile device.  An individuals trajectory to a high degree of precision was established given that they send continuous queries requesting the closest point of interest (e.g., restaurant) from a location service provider. The algorithm to infer the users trajectory is based on the query result locations instead of the actual user locations. Legislation may not permit the location service provider to exchange user location data, but may permit the exchange of the query results with third parties. We reveal a users trajectory via an indirect approach showing that user privacy is not preserved with the storage and exchange of query results. Experimental evaluation indicates in some cases that a users trajectory can be determined to a high accuracy given a small set of points in real scenarios.  The current platforms dominating the mobile market including iOS and Android allow for easy collection of user information via the public APIs they provide. We demonstrate how data can be sent to a remote server without the user being aware and highlight mobile applications are requesting excessive use of permissions in the Google Play store relative to the advertised functionality they provide. Keywords: location, privacy, inference attack, trajectory, Voronoi diagram, points of interest, mobile smart phones "
                                },
                                {
                                    "coreQuota": 30.0,
                                    "instanceQuota": 30.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM_Trajectory_Inference_Attacks",
                                    "usagePatterns": "Data is stored on a remote server so no storage is needed.",
                                    "useCase": "In this project, an algorithm has been developed to infer a persons road trajectory using POI information sent to a LBS such as Google Maps. We have a number of use cases from the Microsoft Geolife dataset to test the algorithm on. The system that runs the algorithm is distributed so the more machines that run the software, the faster the results will be produced and refinements can be made. A working instance and snapshot has been created which runs the system, all that is required now is the ability to load more instances. Project Summary Mobile smart phones have become ubiquitous. The market success of these devices combined with advancements in mobile network infrastructure has created a strong market for third party apps and location based service (LBS) providers. While the apps and service providers are of great convenience to its users, there are concerns of the privacy implications associated. Smart phones capture a lot of information about a user. We hypothesis that data internal to these devices sent via a network can be combined with rich external data to recreate a very accurate profile about an individual.  While it's often speculated that personal information can be inferred from sensitive data, literature to highlight tools and techniques utilized to invade user privacy is scarce. Abstract Mobile privacy has recently become a well-researched topic in pervasive computing. However, little work has been attempted on inferring personal information about a user such as communications meta-data, trajectories of individuals accessing location-based services, social media services used and if mobile sensors can be used to ID a person. We present a collection of algorithms and techniques that is able to infer information about an individual based on data stored on a mobile device.  An individuals trajectory to a high degree of precision was established given that they send continuous queries requesting the closest point of interest (e.g., restaurant) from a location service provider. The algorithm to infer the users trajectory is based on the query result locations instead of the actual user locations. Legislation may not permit the location service provider to exchange user location data, but may permit the exchange of the query results with third parties. We reveal a users trajectory via an indirect approach showing that user privacy is not preserved with the storage and exchange of query results. Experimental evaluation indicates in some cases that a users trajectory can be determined to a high accuracy given a small set of points in real scenarios.  The current platforms dominating the mobile market including iOS and Android allow for easy collection of user information via the public APIs they provide. We demonstrate how data can be sent to a remote server without the user being aware and highlight mobile applications are requesting excessive use of permissions in the Google Play store relative to the advertised functionality they provide. Keywords: location, privacy, inference attack, trajectory, Voronoi diagram, points of interest, mobile smart phones "
                                },
                                {
                                    "coreQuota": 30.0,
                                    "instanceQuota": 30.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM_Trajectory_Inference_Attacks",
                                    "usagePatterns": "Data is stored on a remote server so no storage is needed.",
                                    "useCase": "In this project, an algorithm has been developed to infer a persons road trajectory using POI information sent to a LBS such as Google Maps. We have a number of use cases from the Microsoft Geolife dataset to test the algorithm on. The system that runs the algorithm is distributed so the more machines that run the software, the faster the results will be produced and refinements can be made. A working instance and snapshot has been created which runs the system, all that is required now is the ability to load more instances. Project Summary Mobile smart phones have become ubiquitous. The market success of these devices combined with advancements in mobile network infrastructure has created a strong market for third party apps and location based service (LBS) providers. While the apps and service providers are of great convenience to its users, there are concerns of the privacy implications associated. Smart phones capture a lot of information about a user. We hypothesis that data internal to these devices sent via a network can be combined with rich external data to recreate a very accurate profile about an individual.  While it's often speculated that personal information can be inferred from sensitive data, literature to highlight tools and techniques utilized to invade user privacy is scarce. Abstract Mobile privacy has recently become a well-researched topic in pervasive computing. However, little work has been attempted on inferring personal information about a user such as communications meta-data, trajectories of individuals accessing location-based services, social media services used and if mobile sensors can be used to ID a person. We present a collection of algorithms and techniques that is able to infer information about an individual based on data stored on a mobile device.  An individuals trajectory to a high degree of precision was established given that they send continuous queries requesting the closest point of interest (e.g., restaurant) from a location service provider. The algorithm to infer the users trajectory is based on the query result locations instead of the actual user locations. Legislation may not permit the location service provider to exchange user location data, but may permit the exchange of the query results with third parties. We reveal a users trajectory via an indirect approach showing that user privacy is not preserved with the storage and exchange of query results. Experimental evaluation indicates in some cases that a users trajectory can be determined to a high accuracy given a small set of points in real scenarios.  The current platforms dominating the mobile market including iOS and Android allow for easy collection of user information via the public APIs they provide. We demonstrate how data can be sent to a remote server without the user being aware and highlight mobile applications are requesting excessive use of permissions in the Google Play store relative to the advertised functionality they provide. Keywords: location, privacy, inference attack, trajectory, Voronoi diagram, points of interest, mobile smart phones "
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Astronomy Data Visualization",
                                    "usagePatterns": "Initially there will be four sites involved, with individual users at each site.  After establishing the service, I will be engaging research groups at the sites to test the service.  It is expected that there will be users in the 10s, using the services for several hours a week each, with extremely large datasets (starting with many gigabytes but hopefully testing terabyte datasets as well).",
                                    "useCase": "My masters thesis is about using cloud VMs to enable remote access to HPC facilities for astronomy research groups.  I plan to establish an XXL portal service which will provide both access to facilities as well as distribute results around Australia to research groups.  I will also create several small \"templates\" with preconfigured access to data stores and HPC processing, for the purpose of testing the viability of rapid repurposing of VMs."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Food Desert Index Computation For The Greater Melbourne Area",
                                    "usagePatterns": "For each case study dataset (e.g. the Greater Melbourne Areas / Supermarkets), the Mapquest API calling is the most time-consuming part, which will only be performed once. The food desert index computation and analysis could be carried out multiple times to produce a series of comparative results. ",
                                    "useCase": "These two instances will be deployed as parallel computation nodes using R SNOW package (Simple Network of Workstations). First, we plan to generate a travel information database by calling Mapquest Direction API to query the route information between 53771 census tracts (in ABS MeshBox spacial unit) and their nearby major supermarkets (700+ in total). Three different travel modes are concerned: walking, driving and public transport. Secondly, the analysis on the travelinfo db will be performed to generate \"food desert index\" for each tract area for each travel mode. Finally, a general index can be synthesised based on three mode index values."
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.8,
                                    "institution": "monash.edu",
                                    "name": "Monash MCTS Server",
                                    "usagePatterns": "***UPDATED REQUEST*** The amount of data being produced by experiments for this project is far larger than anticipated at inception. As such, I have amended this request to add 2TB of volume storage for this project. ***END UPDATED REQUEST*** As this server will be running a batch of tasks, it is anticipated that this instance will be running at high levels of activity and I/O for processor, RAM and local storage on a fairly \"flat\" usage pattern for the duration of this time. ",
                                    "useCase": "***UPDATED REQUEST*** Positive results within the initial experiment runs has led to an expansion of test scenarios to incorporate a wider range of data. As such, a larger amount of test data is being produced. This instance is now being used to store and process test data using MongoDB. ***UPDATED REQUEST ENDS*** This instance will be used to implement and assess the efficacy of numerous ranking algorithms in the context of search across large, hierarchical datasets.  In this case, a 7 million plus corpus of emails taken from an open source (Apache Software Foundation) will be used as the corpus. Attempts to date using available computing hardware have intermittently failed due to RAM constraints (16GB per node) "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Genome_assembly",
                                    "usagePatterns": "My experiments will deal with large data set of witch the size is about 10 gb. But I will be the only one to involve with the programming part of this program. ",
                                    "useCase": "The cloud will be used for processing the next-generation sequencing data. I'll try to solve the repeat problem in genome assembly which is based on K-mer method.  This will help biologist to build more complete reference genome. Since this program needs large computer memory and powerful computational ability, I ask for help. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "rmit.edu.au",
                                    "name": "RMIT_Repository",
                                    "usagePatterns": "There will be up to 10 users, and a database of several thousand records. ",
                                    "useCase": "Collaborative repository of assessment questions for programming. These will be entered by various academics around Australia, and used for research into computing education."
                                },
                                {
                                    "coreQuota": 1.8,
                                    "instanceQuota": 0.9,
                                    "institution": "anu.edu.au",
                                    "name": "ANU_Pro-ana_data_service",
                                    "usagePatterns": "The intention is to have 3 instances, each running 2 cores and 2 persistent volumes.  2 instances (preferably geographically separated) will run replicas of mongodb (each using one persistent volume). This will allow system maintenance/failure without interrupting data collection. The third instance will run a data collection script.",
                                    "useCase": "\"pro-ana\" (pro-anorexia) is a recent phenomena on various online social media platforms. We intend to collect data from the Twitter pro-ana community (possibly expanding to other media outlets). This application will provide 2 replicas of a mongodb and one instance for polling the Twitter api 24/7 and storing the data in mongo. Object storage will be used for database backups."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.6,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Genome_assembly",
                                    "usagePatterns": "My experiments will deal with large data set of witch the size is about 10 gb. But I will be the only one to involve with the programming part of this program. ",
                                    "useCase": "The cloud will be used for processing the next-generation sequencing data. I'll try to solve the repeat problem in genome assembly which is based on K-mer method.  This will help biologist to build more complete reference genome. Since this program needs large computer memory and powerful computational ability, I ask for help. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "rmit.edu.au",
                                    "name": "RMIT_Repository",
                                    "usagePatterns": "There will be up to 10 users, and a database of several thousand records. ",
                                    "useCase": "Collaborative repository of assessment questions for programming. These will be entered by various academics around Australia, and used for research into computing education."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Acoustic_Work_Bench",
                                    "usagePatterns": "I think initially about 100 users and large data sets 100TB total but users will only access a small part of them. The data storage has been requested separately through Nectar. (core hours is a guess)",
                                    "useCase": "Collection of audible range terrestrial bio-acoustic (sound) recordings used for species monitoring, bio-diversity assessment and animal behaviour studies."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Acoustic_Work_Bench",
                                    "usagePatterns": "I think initially about 100 users and large data sets 100TB total but users will only access a small part of them. The data storage has been requested separately through Nectar. (core hours is a guess)",
                                    "useCase": "Collection of audible range terrestrial bio-acoustic (sound) recordings used for species monitoring, bio-diversity assessment and animal behaviour studies."
                                }
                            ],
                            "name": "080109"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "rmit.edu.au",
                                    "name": "\"Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory. "
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "rmit.edu.au",
                                    "name": "Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "I've requested this allocation for another 12 months as we expect to be using this allocation  throughout this time. We expect to have a small number of users (~4) but some of us will be running large jobs that may take 30-50 core hours to run each. These may involve input and output data that take up to 5 Gig per machine (though many runs will be less then this). This will be for running simulations using the tzar framework for publications we are working on and also for testing and developing tzar further to be a component in the Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. ",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large Simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory."
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "MUtopia Research Platform",
                                    "usagePatterns": "The number of users will most likely vary between 4-10 on our prototype system. Usage will be mainly in business working hours. The datasets are relatively small (in the order of hundreds of megabytes at most).",
                                    "useCase": "Hosts experimental servers for user experience, API and client testing for third-party usage with our existing decision support system prototype. This can entail 3 separate systems running at once such that we can very rapidly separate different versions of the system and test each in isolation (the data hosted on each system must be separate). "
                                },
                                {
                                    "coreQuota": 160.0,
                                    "instanceQuota": 160.0,
                                    "institution": "monash.edu",
                                    "name": "Plexos-Nimrod",
                                    "usagePatterns": "",
                                    "useCase": "This research concerns the identification of optimal investment configurations for renewable energy resources in a large and complex power grid. It is tacked through analysis and simulation of electricity networks using a mixture of time-sequential simulations and global optimisation of the parameters of these simulations using Nimrod/O. The outcomes will provide a better understanding of how the topology of a power system affects the selection of technology types and the lowest cost investment solution. It can be used to inform policy makers on distributed generation market design and will form part of the research in the CSIRO's Future Grid flagship collaboration cluster: www.futuregrid.org.au"
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "MUtopia Research Platform",
                                    "usagePatterns": "The number of users will most likely vary between 4-10 on our prototype system. Usage will be mainly in business working hours. The datasets are relatively small (in the order of hundreds of megabytes at most).",
                                    "useCase": "Hosts experimental servers for user experience, API and client testing for third-party usage with our existing decision support system prototype. This can entail 3 separate systems running at once such that we can very rapidly separate different versions of the system and test each in isolation (the data hosted on each system must be separate). "
                                },
                                {
                                    "coreQuota": 160.0,
                                    "instanceQuota": 160.0,
                                    "institution": "monash.edu",
                                    "name": "Plexos-Nimrod",
                                    "usagePatterns": "",
                                    "useCase": "This research concerns the identification of optimal investment configurations for renewable energy resources in a large and complex power grid. It is tacked through analysis and simulation of electricity networks using a mixture of time-sequential simulations and global optimisation of the parameters of these simulations using Nimrod/O. The outcomes will provide a better understanding of how the topology of a power system affects the selection of technology types and the lowest cost investment solution. It can be used to inform policy makers on distributed generation market design and will form part of the research in the CSIRO's Future Grid flagship collaboration cluster: www.futuregrid.org.au"
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "rmit.edu.au",
                                    "name": "Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "I've requested this allocation for 6 months as we expect to be using this allocation sporadically throughout this time. We expect to have a small number of users (~4) but some of us will be running large jobs that may take 30-50 core hours to run each. These may involve input and output data that take up to 10 Gig per machine (though many runs will be less then this). This will be for running simulations using the tzar framework for publications we are working on and also for testing and developing tzar further to be a component in the Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. ",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory."
                                }
                            ],
                            "name": "080110"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_ExploreTheSeafloor",
                                    "usagePatterns": "We will have large numbers of files that contain data related to the annotation of seafloor imagery.  The imagery itself is hosted elsewhere.",
                                    "useCase": "We are looking for an accessible machine to store and analyse data related to the Science Week Explore The Seafloor project (exploretheseafloor.net.au)."
                                },
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_ExploreTheSeafloor",
                                    "usagePatterns": "We will have large numbers of files that contain data related to the annotation of seafloor imagery.  The imagery itself is hosted elsewhere.",
                                    "useCase": "We are looking for an accessible machine to store and analyse data related to the Science Week Explore The Seafloor project (exploretheseafloor.net.au)."
                                }
                            ],
                            "name": "080104"
                        }
                    ],
                    "name": "0801"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Australian National Data Service",
                                    "usagePatterns": "will begin small, but hope to enable Universities to start using these core data management systems themselves over time. But for the immediate project it will be small at first, though we will test on larger instances to make sure the load can be handled for heavy data usage with these systems.",
                                    "useCase": "As discussed with NeCTAR ref Steve Manos and Tom Fifield.  These servers will be used for various open research data provisions test services for ANDS #betterdata programme of work, including building public images for several established research data management systems, including but not limited to: DMP, DCC, BibSoup, tDAR, DAP, DataFlow, etc."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Paperminer",
                                    "usagePatterns": "",
                                    "useCase": "The objectives of this project and the PaperMiner portal are: Develop a small set of data mining and indexing techniques to meaningfully organise the National Library of Australia Trove Newspapers Online dataset for useful knowledge discovery; Store and manage the indexes and provide the linkages back to the NLA Trove api for record retrieval; Provide a web based portal through which registered users can perform queries against the indexes, parse results, view summaries in both textual and graphical forms, and initiate NLA Trove record retrieval. "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 6.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIFTester_RAnKO",
                                    "usagePatterns": "Small number of users and small data sets.",
                                    "useCase": "Will be used by QCIF Team to test and document various configurations for client support."
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "uq.edu.au",
                                    "name": "ARMS Production Service",
                                    "usagePatterns": "Small numbers of users in an intermittent usage pattern.",
                                    "useCase": "The allocation is required to run the first release of the request management system for collection storage. This system is being developed by QCIF and will be expanded to other nodes in October 2013."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 6.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIFTester_RAnKO",
                                    "usagePatterns": "Small number of users and small data sets.",
                                    "useCase": "Will be used by QCIF Team to test and document various configurations for client support."
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 3.0,
                                    "institution": "uq.edu.au",
                                    "name": "ARMS Production Service",
                                    "usagePatterns": "Small numbers of users in an intermittent usage pattern.",
                                    "useCase": "The allocation is required to run the first release of the request management system for collection storage. This system is being developed by QCIF and will be expanded to other nodes in October 2013."
                                }
                            ],
                            "name": "089999"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                }
                            ],
                            "name": "0899"
                        }
                    ],
                    "name": "0899"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Research data registry (ANDS)",
                                    "usagePatterns": "many users, small datasets",
                                    "useCase": "the University is developing a research data registry that will support the register and discovery of research data "
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Porting",
                                    "usagePatterns": "Mainly myself for testing purpose.",
                                    "useCase": "I am working at eRSA and we are trying out things to migrate applications to cloud. One example we want to try is: Fire up one instance when the running one is dead and attach persistent volume to it.  "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "uq.edu.au",
                                    "name": "Mars",
                                    "usagePatterns": "1 user",
                                    "useCase": "QCIF RDSI node testing, client support and documentation"
                                },
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 64.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Node Zero",
                                    "usagePatterns": "Mostly test environments requiring large storage for a short period of time (e.g. months) and then being deleted. Though some servers might need to be up for a long period of time for production purposes.",
                                    "useCase": "Supporting QCIF's RDSI Node Zero activities."
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 3.0,
                                    "institution": "uq.edu.au",
                                    "name": "Mars",
                                    "usagePatterns": "1 user",
                                    "useCase": "QCIF RDSI node testing, client support and documentation"
                                },
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 8.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Node Zero",
                                    "usagePatterns": "Mostly test environments requiring large storage for a short period of time (e.g. months) and then being deleted. Though some servers might need to be up for a long period of time for production purposes.",
                                    "useCase": "Supporting QCIF's RDSI Node Zero activities."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Data Collections Registry",
                                    "usagePatterns": "The service aggregates data from internal metadata stores together. It provides the resulting merged data as feeds to the National Library of Australia & Research Data Australia, and as RDFa-annotated web pages to the general public. The datasets are not large, and most processing is batched. Direct traffic volume is very low, as most hits are to sites receiving the outputted data feeds. (e.g. NLA Trove) Existing hosting consists of two servers with 8GB of persistent disk (including OS). Application size including libraries & logs is around <2GB, with database size of <100MB. For security reasons, staff records are indexed and anonymized on a separate server to the one which provides public data. Application size is ~500MB, with database and indexes <200MB. Object storage may be used for periodic rolling backups of the databases.",
                                    "useCase": "The ANDS-funded UQ Data Collections Registry (UQ-DCR) project has developed an institutional metadata store for The University of Queensland.  It issues NLA party identifiers for UQ researchers and provides a central registry where research data collection metadata records for UQ can be stored & accessed. It is currently available at: http://research.data.uq.edu.au/ More information on the project can be found here: https://projects.ands.org.au/id/MS06 It is currently hosted on the QCIF Early Research Node (QERN), which is being decommissioned. "
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Data Collections Registry",
                                    "usagePatterns": "The service aggregates data from internal metadata stores together. It provides the resulting merged data as feeds to the National Library of Australia & Research Data Australia, and as RDFa-annotated web pages to the general public. The datasets are not large, and most processing is batched. Direct traffic volume is very low, as most hits are to sites receiving the outputted data feeds. (e.g. NLA Trove) Existing hosting consists of two servers with 8GB of persistent disk (including OS). Application size including libraries & logs is around <2GB, with database size of <100MB. For security reasons, staff records are indexed and anonymized on a separate server to the one which provides public data. Application size is ~500MB, with database and indexes <200MB. Object storage may be used for periodic rolling backups of the databases.",
                                    "useCase": "The ANDS-funded UQ Data Collections Registry (UQ-DCR) project has developed an institutional metadata store for The University of Queensland.  It issues NLA party identifiers for UQ researchers and provides a central registry where research data collection metadata records for UQ can be stored & accessed. It is currently available at: http://research.data.uq.edu.au/ More information on the project can be found here: https://projects.ands.org.au/id/MS06 It is currently hosted on the QCIF Early Research Node (QERN), which is being decommissioned. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "QUT Smart Cloud Broker",
                                    "usagePatterns": "",
                                    "useCase": "The Smart Cloud Broker consists of a suite of software tools that are being developed to support cloud users in comparing the different infrastructure offerings and selecting the cloud configuration and providers with the most appropriate usage terms and conditions based on their specific requirements. It includes:  Benchmarking - a system that uses a suite of benchmark applications to benchmark the cost, configuration and performance of the different cloud instances under flexible and customizable load conditions corresponding to user specific requirements   Comparator  a system to automatically compare different configurations based on the specific needs of the user in terms of infrastructure requirements, application performance, costs, security, geographic location, compliance, regulatory requirements and other requisite criteria. Smart Cloud Broker is a research collaboration between Smart Service CRC  and Swinburne University of Technology. The scale of the project has increased and we would like to increase our allocation to successfully deliver the project"
                                }
                            ],
                            "name": "080609"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "griffith.edu.au",
                                    "name": "DSpace3.0",
                                    "usagePatterns": "",
                                    "useCase": "Proof of concept for using new version of DSpace 3 with RIMs for integration with Griffith Research Online."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "griffith.edu.au",
                                    "name": "DSpace3.1",
                                    "usagePatterns": "Small dataset with small no of users.",
                                    "useCase": "Install of Dspace 3.1 as institutional repository to test integration into RIMs."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "rmit.edu.au",
                                    "name": "RMIT_Repository",
                                    "usagePatterns": "There will be up to 10 users, and a database of several thousand records. ",
                                    "useCase": "Collaborative repository of assessment questions for programming. These will be entered by various academics around Australia, and used for research into computing education."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "MatlabEAIT",
                                    "usagePatterns": "Usage pattern will change over time, but will envisage small data sets with just myself as the user.",
                                    "useCase": "Evaluating Windows Matlab performance and firewall issues including licensing and connectivity to UQ EAIT file server .  "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Domestic_Technology_Use",
                                    "usagePatterns": "Two or three researchers in charge of developing the website over a few weeks. Then just leave it as a server. Only log on to download video files.  Estimated usage in practice of roughly 30 participants uploading up to an hour of video recordings (home movies) once every 3 months for the next three years. I crunched some numbers and 1TB should be okay (we'll take video off server as study progresses).",
                                    "useCase": "Cloud instance will be used to host a set of webpages visited by participants in a longitudinal study into domestic usages of the NBN.  The study consists of roughly 20 households (100 individuals) over three years. Every three months, participants take pictures, video and audio recording of their day to day technology usage using iPads they have been given for the study. Participants make this media by visiting the website hosted on the server we are requesting.  The website then posts this information to the server. The server runs a php script to make a record in the database and saves the file to the local HD. The script can also use the Youtube API and the Curl extension (see below) to post videos onto Youtube (if the participants choose to do this). The website may also be used for researchers to download participant video.  We' re looking for a LAMP (linux, apache, mysql, php) environment. Would be handy if the php extension Curl was already installed. (http://php.net/manual/en/book.curl.php) The ethics clearance for this project is 1339699.1, with the title \"An Investigation of the Early Adoption and Appropriation of High-Speed Broadband in the Domestic Environment\". Dr Martin Gibbs is the Responsible researcher on the application. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "rmit.edu.au",
                                    "name": "RMIT_Repository",
                                    "usagePatterns": "There will be up to 10 users, and a database of several thousand records. ",
                                    "useCase": "Collaborative repository of assessment questions for programming. These will be entered by various academics around Australia, and used for research into computing education."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "sydney.edu.au",
                                    "name": "VL201 Industrial Ecology",
                                    "usagePatterns": "Initial usage is 15, with a maximum of 8 concurrent, final numbers could be up to 65 users nationally. Datasets are comprised of large files of statistical data from various sources e.g. ABS, Water usage, Waste data. The file sizes are significant with only 5 datasets  using the initial 10gb allocated to the project.",
                                    "useCase": "The Industrial Ecology project currently has a Medium environment as a pilot and there are insufficient RAM to run the processes to reach the initial datafeed milestones or consequent milestones that require further processing. Emails from the business sponsor and technical lead of the project can be forwarded as evidence of lack of RAM if required. Basic requirements from Technical lead are below: We need to operate a web server, a storage for a potentially large number of different MRIOs and the computing capability to run the analysis. If we run all of that stuff on the cloud, we need more crunch. If not, we should explore different solutions soon. For example that mass-storage facility at UQ. The homepage that we are running here for our global model has its own, dedicated machine with 8 cores, 1 TB of storage and 64 GB or RAM. I reckon for this project we would need at least the same. Kind regards Neal"
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "sydney.edu.au",
                                    "name": "SYD-IndEcologyVL",
                                    "usagePatterns": "Large data sets from a medium group of users which is growing on a weekly basis, ie, two new PHDs in May - June.",
                                    "useCase": "The team requires a storage allocation of 5tb to enable testign and work to continue as the existing 480gb has been consumed and it is estimated that at least 3tb will be in use by go-live December 2013. Hi Neal We're hitting constant disk-space problems - this problem has now become urgent - we need more space. Thanks Manfred [cid:E0E9A103-BEE5-4644-9E87-BDD1D77D24A7] Read our article in Nature: http://www.nature.com/nature/journal/v486/n7401/full/nature11145.html Begin forwarded message: From: Arne Geschke > Subject: disc full Date: May 31, 2013 6:50:12 AM GMT+10:00 To: Manfred Lenzen >, Arunima Malik >, Daniel Moran >, Joe Lane > Hi all, I've changed the upload code to adjust the rights of the uploaded file so that everybody can now delete their work directory. So you shouldn't need my help anymore to delete it. I've noticed that the disc is - again - completely full. Dan has got one job that is currently processing on scarlett, I don't know what will happen when that hits the upload queue. I can stop the uploader to avoid any problems. In that case, the data would remain on scarlett until the uploader is restarted. Dan, if you like, you can keep that data of this run on scarlett if that helps. Please advise/delete data from your directory. Thanks, Arne Dr Arne Geschke | Postdoctoral Fellow Integrated Sustainability Analysis School of Physics | Faculty of Science THE UNIVERSITY OF SYDNEY Rm 501, School of Physics A28 | Physics Road | The University of Sydney | NSW | 2006 T +61 2 9036 7505 | F +61 2 9351 7726 E arne.geschke@sydney.edu.au | W http://www.isa.org.usyd.edu.au "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "sydney.edu.au",
                                    "name": "SYD-IndEcologyVL",
                                    "usagePatterns": "Large data sets from a medium group of users which is growing on a   weekly basis, ie, two new PHDs in May - June.   ",
                                    "useCase": "The team requires a storage allocation of 5tb to enable testing and  > work to continue as the existing 480gb has been consumed and it is  > estimated that at least 3tb will be in use by go-live December 2013. >  > Hi Neal > We&#39;re hitting constant disk-space problems - this problem has now  > become urgent - we need more space. > Thanks > Manfred > [cid:E0E9A103-BEE5-4644-9E87-BDD1D77D24A7] > Read our article in Nature: > http://www.nature.com/nature/journal/v486/n7401/full/nature11145.html > Begin forwarded message: > From: Arne Geschke &gt; > Subject: disc full > Date: May 31, 2013 6:50:12 AM GMT+10:00 > To: Manfred Lenzen &gt;, Arunima Malik &gt;, Daniel Moran &gt;, Joe  > Lane &gt; Hi all, I&#39;ve changed the upload code to adjust the  > rights of the uploaded file so that everybody can now delete their  > work directory. So you shouldn&#39;t need my help anymore to delete it. > I&#39;ve noticed that the disc is - again - completely full. Dan has  > got one job that is currently processing on scarlett, I don&#39;t know  > what will happen when that hits the upload queue. I can stop the  > uploader to avoid any problems. In that case, the data would remain on  > scarlett until the uploader is restarted. Dan, if you like, you can  > keep that data of this run on scarlett if that helps. > Please advise/delete data from your directory. > Thanks, > Arne > Dr Arne Geschke | Postdoctoral Fellow > Integrated Sustainability Analysis > School of Physics | Faculty of Science THE UNIVERSITY OF SYDNEY Rm  > 501, School of Physics A28 | Physics Road | The University of Sydney |  > NSW | 2006 T +61 2 9036 7505 | F +61 2 9351 7726 E  > arne.geschke@sydney.edu.au | W http://www.isa.org.usyd.edu.au >  >  > The usage pattern is: > Large data sets from a medium group of users which is growing on a  > weekly basis, ie, two new PHDs in May - June. >  > And the geographic requirements are: > Can be any available node, however, proximity to Massive would be  > preferred. >  > The tenant has specified the breakdown of their Fields Of Research as: >  > 0103 NUMERICAL AND COMPUTATIONAL MATHEMATICS (30%) >  >  > 0502 ENVIRONMENTAL SCIENCE AND MANAGEMENT (40%) >  >  > 0806 INFORMATION SYSTEMS (30%) >  >  > Yours, > the rcDashBoard. "
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_OzTrack",
                                    "usagePatterns": "Amendment, 2013-11-18: As part of testing the current deliverables for this NeCTAR e-Research Tools project, we need to set up a UAT environment. I have updated the requested Instance/Core/Volume numbers to reflect the addition of one Large instance with an attached 100GB persistent volume. ---- NeCTAR cloud instances will host the OzTrack Java web application, GeoServer instance, PostgreSQL database, and a pool of Rserve instances used to compute R-based home range analyses and movement models. Potential configuration: one large instance for the Java servlet container and database; and four medium instances, each running Rserve, connected via TCP to main instance. Storage breakdown for current OzTrack deployment (oztrack.org): animal tracking data 1.0 GiB; external environmental layers, e.g. land use and bathymetry grids, 5.7 GiB; daily/weekly/monthly back-ups of animal tracking data 4.4 GiB. Animal tracking data is likely to remain small, but plans for pre-seeding tile caches of environmental layers in GeoServer mean that we are requesting 100 GiB of storage. As for the object store, we are not sure at this stage whether it will be required. We are considering using it for backups and potentially for storing environmental data files. We have applied for an allocation in order to experiment with it and decide whether it is appropriate.",
                                    "useCase": "The OzTrack project is supported by the NeCTAR e-Research Tools program for 2012-13. The goal of this project is to develop a set of eResearch software tools to support the compute, storage and analysis of animal tracking data being generated through telemetry devices. See http://oztrack.org/ and http://www.itee.uq.edu.au/eresearch/projects/oztrack/"
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 1.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_OzTrack",
                                    "usagePatterns": "Amendment, 2013-11-18: As part of testing the current deliverables for this NeCTAR e-Research Tools project, we need to set up a UAT environment. I have updated the requested Instance/Core/Volume numbers to reflect the addition of one Large instance with an attached 100GB persistent volume. ---- NeCTAR cloud instances will host the OzTrack Java web application, GeoServer instance, PostgreSQL database, and a pool of Rserve instances used to compute R-based home range analyses and movement models. Potential configuration: one large instance for the Java servlet container and database; and four medium instances, each running Rserve, connected via TCP to main instance. Storage breakdown for current OzTrack deployment (oztrack.org): animal tracking data 1.0 GiB; external environmental layers, e.g. land use and bathymetry grids, 5.7 GiB; daily/weekly/monthly back-ups of animal tracking data 4.4 GiB. Animal tracking data is likely to remain small, but plans for pre-seeding tile caches of environmental layers in GeoServer mean that we are requesting 100 GiB of storage. As for the object store, we are not sure at this stage whether it will be required. We are considering using it for backups and potentially for storing environmental data files. We have applied for an allocation in order to experiment with it and decide whether it is appropriate.",
                                    "useCase": "The OzTrack project is supported by the NeCTAR e-Research Tools program for 2012-13. The goal of this project is to develop a set of eResearch software tools to support the compute, storage and analysis of animal tracking data being generated through telemetry devices. See http://oztrack.org/ and http://www.itee.uq.edu.au/eresearch/projects/oztrack/"
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 2.4,
                                    "institution": "vu.edu.au",
                                    "name": "VU Phoenix Program",
                                    "usagePatterns": "Small number of users expected, essentially web and web service driven execution of processing.",
                                    "useCase": "The generic and core technologies of the project come from the Phoenix Program with the following application domains: 1) tourism and 2) supply chain and logistics.   We currently hold a category one research grant under Australia China Council in the domain of ICT application for tourism, and also holding an online real-time demonstration in the domain of supply chain and logistics through a research prototype (that is currently being upgraded in terms of concurrent processing and scaling up).   Applications in the above domains are to occur through cloud, a strategic direction for the project. Since we have industry partners of IBM and TIBCO who offered us commercial grade of products, some of those are to be used by the project and likely hosted on cloud, so we would like to apply reasonable resources that are adequate to support a private cloud scenario (e.g. hosting IBM Smart Cloud Technology, a private cloud environment). Depending on what is possible and available, I tentatively propose the specified resource requirements. -- Wei Dai, College of Business, Centre for Strategic and Economic Syudies. "
                                }
                            ],
                            "name": "0806"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "qut.edu.au",
                                    "name": "Collaborative Content Creation of Gesture Systems",
                                    "usagePatterns": "Small numbers of users (25 to 50) with data sets ranging up to a 1GB. Back-end request will be made to access the defined data sets via API's small numbers of systems making backend requests.",
                                    "useCase": "We require servers to host both front end and back-end functionality to support research into the collaborative creation and delivery on interactive multimedia datasets for gesture based computing environments. Using the hosted front end people will be able to collaboratively define multimedia data sets, these data sets will then be accessed on the fly by the gesture based computing environments "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_Flipped_Classroom",
                                    "usagePatterns": "The instances will be used to capture interactions with a virtual learning environment of cohorts of students of up to 300. They will contribute files and resources, thus the need for significant storage usage. The dataset per course ",
                                    "useCase": "The deployment of active learnign and more precisely Flipped Classroom methodology requires the use of advanced personalisation techniques that shape a learning environment differently for every student. The project proposes the design of a platform to support personalisation in Flipped Classrooms."
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd Moderator Assitant",
                                    "usagePatterns": "~100 users, not a big data set",
                                    "useCase": "Moderator Assistant is a project funded by the Young and Well CRC to develop a tool that helps moderators of mental health peer-support groups. This website will be used to annotate data and generate feedback by interns and users at the CRC and the Inspire Foundation "
                                }
                            ],
                            "name": "080602"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 44.8,
                                    "instanceQuota": 44.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM__iDDSS",
                                    "usagePatterns": "The platform provides a small group of experienced users with the capability of analysing large geospatial datasets (both static and dynamic).",
                                    "useCase": "The architecture of iDDSS platform consists of one Postgres database instance (VM1), one Apache web server instance (VM2) and two GeoServer instances (VM3/4). The platform is loosely coupled, and six types of services running upon these instances form the application layer of iDDSS:  (1) VM1 hosts Data Gathering Service (DGS) and Data Integration Service (DIS). Aggregated datasets from DIS are stored in VM3/4. (2) VM2 hosts Simulation Service (SS) and Visualisation Service (VS), fetching data from VM3/4. (3) VM3/4 host Disaster Modelling Service (DMS) and Optimization Service (OS). "
                                },
                                {
                                    "coreQuota": 44.8,
                                    "instanceQuota": 11.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM__iDDSS",
                                    "usagePatterns": "The platform provides a small group of experienced users with the capability of analysing large geospatial datasets (both static and dynamic).",
                                    "useCase": "The architecture of iDDSS platform consists of one Postgres database instance (VM1), one Apache web server instance (VM2) and two GeoServer instances (VM3/4). The platform is loosely coupled, and six types of services running upon these instances form the application layer of iDDSS:  (1) VM1 hosts Data Gathering Service (DGS) and Data Integration Service (DIS). Aggregated datasets from DIS are stored in VM3/4. (2) VM2 hosts Simulation Service (SS) and Visualisation Service (VS), fetching data from VM3/4. (3) VM3/4 host Disaster Modelling Service (DMS) and Optimization Service (OS). "
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "monash.edu",
                                    "name": "Adaptive Data Stream Mining for Data Partition Hot-spot Prediction",
                                    "usagePatterns": "Synthetic data will be generated using YCSB toolkit for populating the HBase data store from 2 client instances.",
                                    "useCase": "In this project we aim to predict the hot spotted logical data partitions in real time using data stream mining techniques. We will use HBase/Hadoop clusters (1 Master Node, 1 Zookeeper, 5 Data Nodes and 1 Monitoring node with Ganglia Server) for this project. Another 2 servers will be used to generate synthetic data using YCSB toolkit.  Contact:  Joarder Mohammad Mustafa Kamal PhD Student Gippsland School of Information Technology, Faculty of Information Technology, MONASH University Churchill VIC, Australia. 3842 Email: Joarder.Kamal@monash.edu Phone: 0351-226-133 (Room: 4N-247), 0470-578-819 (Mobile) Web: http://users.monash.edu/~jkamal/ Supervisor: Dr. Manzur Murshed"
                                }
                            ],
                            "name": "080605"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 2.0,
                                    "institution": "rmit.edu.au",
                                    "name": "iDriveSafe",
                                    "usagePatterns": "We will have many users and large data sets. ",
                                    "useCase": "This research aims to investigate and develop a suite of real-time data provision and analytics techniques to substantially enhance road safety by establishing a cloud-based infrastructure which can provide dynamic risk assessment based on driver context.  An extensible and scalable cloud-based infrastructure will be delivered to host contextual web services to store, manage, analyse, and visualise driving behaviours and traffic patterns.   "
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "unsw.edu.au",
                                    "name": "FAIMS",
                                    "usagePatterns": "1 user (me) most of the time until we start really getting under weigh. The data sets will be large and varied, whatever we can beg from tDar and OpenContext and Heurist in the main. I'll be testing data extraction and import, so a decent pipe will be nice. But very low utilization expected most of the time based on research needs. Number of core hours unknown. Reqeuest help estimating. Number of instances: We have 8 core servers, each requiring at least a medium allocation (we've experienced processor overload on our smalls) running our website, issue tracker, wiki, authentication server, primary data repository, development data repository, primary mobile app server, and development app server. With our new grant, we also will need to spin up up to 15 2-core instances to support our researchers. I've increased our volume storage request to provide room for our mandated backup regimen. ",
                                    "useCase": "This is to support the NeCTAR funded FAIMS project. Our current VM is a local-tenancy test VM that cannot support sample instances of tDAR, openContext, Heurist, and the 8 other databases I need to investigate and experiment with.  We need a front-end tiny web-host for our blog and workshop management software. And either 1 machine or a set of machines to run the various archaeological warehouses to test with. We will also likely need one or more machines to serve as storage backends to the mobile tool we're developing."
                                },
                                {
                                    "coreQuota": 3.5,
                                    "instanceQuota": 3.5,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-eReefs",
                                    "usagePatterns": "",
                                    "useCase": "To be used for the long-running eReefs project which is about building an informatics platform to allow the fusion of multiple datasets about the Great Barrier Reef, both pre-existing and new."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "unsw.edu.au",
                                    "name": "FAIMS",
                                    "usagePatterns": "1 user (me) most of the time until we start really getting under weigh. The data sets will be large and varied, whatever we can beg from tDar and OpenContext and Heurist in the main. I'll be testing data extraction and import, so a decent pipe will be nice. But very low utilization expected most of the time based on research needs. Number of core hours unknown. Reqeuest help estimating. Number of instances unknown, also need help there. 2 to start with, 1 web facing with wordpress and ocs, and 1 research-facing that can run 8 different data warehouses.  Object storage size is highly variable, as a function of what archaeologist buy-in we get.",
                                    "useCase": "This is to support the NeCTAR funded FAIMS project. Our current VM is a local-tenancy test VM that cannot support sample instances of tDAR, openContext, Heurist, and the 8 other databases I need to investigate and experiment with.  We need a front-end tiny web-host for our blog and workshop management software. And either 1 machine or a set of machines to run the various archaeological warehouses to test with. We will also likely need one or more machines to serve as storage backends to the mobile tool we're developing."
                                }
                            ],
                            "name": "080612"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "monash.edu",
                                    "name": "Adaptive Data Stream Mining for Data Partition Hot-spot Prediction",
                                    "usagePatterns": "Synthetic data will be generated using YCSB toolkit for populating the HBase data store from 2 client instances.",
                                    "useCase": "In this project we aim to predict the hot spotted logical data partitions in real time using data stream mining techniques. We will use HBase/Hadoop clusters (1 Master Node, 1 Zookeeper, 5 Data Nodes and 1 Monitoring node with Ganglia Server) for this project. Another 2 servers will be used to generate synthetic data using YCSB toolkit.  Contact:  Joarder Mohammad Mustafa Kamal PhD Student Gippsland School of Information Technology, Faculty of Information Technology, MONASH University Churchill VIC, Australia. 3842 Email: Joarder.Kamal@monash.edu Phone: 0351-226-133 (Room: 4N-247), 0470-578-819 (Mobile) Web: http://users.monash.edu/~jkamal/ Supervisor: Dr. Manzur Murshed"
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Data Collections Registry",
                                    "usagePatterns": "The service aggregates data from internal metadata stores together. It provides the resulting merged data as feeds to the National Library of Australia & Research Data Australia, and as RDFa-annotated web pages to the general public. The datasets are not large, and most processing is batched. Direct traffic volume is very low, as most hits are to sites receiving the outputted data feeds. (e.g. NLA Trove) Existing hosting consists of two servers with 8GB of persistent disk (including OS). Application size including libraries & logs is around <2GB, with database size of <100MB. For security reasons, staff records are indexed and anonymized on a separate server to the one which provides public data. Application size is ~500MB, with database and indexes <200MB. Object storage may be used for periodic rolling backups of the databases.",
                                    "useCase": "The ANDS-funded UQ Data Collections Registry (UQ-DCR) project has developed an institutional metadata store for The University of Queensland.  It issues NLA party identifiers for UQ researchers and provides a central registry where research data collection metadata records for UQ can be stored & accessed. It is currently available at: http://research.data.uq.edu.au/ More information on the project can be found here: https://projects.ands.org.au/id/MS06 It is currently hosted on the QCIF Early Research Node (QERN), which is being decommissioned. "
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Data Collections Registry",
                                    "usagePatterns": "The service aggregates data from internal metadata stores together. It provides the resulting merged data as feeds to the National Library of Australia & Research Data Australia, and as RDFa-annotated web pages to the general public. The datasets are not large, and most processing is batched. Direct traffic volume is very low, as most hits are to sites receiving the outputted data feeds. (e.g. NLA Trove) Existing hosting consists of two servers with 8GB of persistent disk (including OS). Application size including libraries & logs is around <2GB, with database size of <100MB. For security reasons, staff records are indexed and anonymized on a separate server to the one which provides public data. Application size is ~500MB, with database and indexes <200MB. Object storage may be used for periodic rolling backups of the databases.",
                                    "useCase": "The ANDS-funded UQ Data Collections Registry (UQ-DCR) project has developed an institutional metadata store for The University of Queensland.  It issues NLA party identifiers for UQ researchers and provides a central registry where research data collection metadata records for UQ can be stored & accessed. It is currently available at: http://research.data.uq.edu.au/ More information on the project can be found here: https://projects.ands.org.au/id/MS06 It is currently hosted on the QCIF Early Research Node (QERN), which is being decommissioned. "
                                }
                            ],
                            "name": "080604"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 0.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Timetable Optimisation",
                                    "usagePatterns": "Access to the VM 's web server is essential; ideally via a remote web browser. Eg, port forwarding.",
                                    "useCase": "The TRC (Matthew Britton) has been working with Paul Duldig (UoA VP services & resources) and Virginia Deegan to optimise and experiment with the University of Adelaide timetable, using open-source software called UniTime. Until now we had only been working with a fraction of the entire UoA student body and course offerings, but we are now ready for the whole database. However, the computing requirements (32++ GB of RAM) of the task running in UniTime exceeds anything that is available to us (16 GB)."
                                }
                            ],
                            "name": "080608"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM MUtopia",
                                    "usagePatterns": "Currently, our system is hosting moderately large precinct models (thousands of buildings with hundreds of parameters each), plus some large assets (such as uploaded 3D models). The scale and number of these projects is expected to grow, potentially substantially, over the next year as we acquire more clients to work with and develop more sophisticated models. However CPU usage is typically light, with the occasional demanding simulation. We would have a primary instance as a production server, and a secondary, identical server for development staging. The final server would be split between hosting our development tools and smaller, temporary demo instances in VMs. Object storage would be utilised to store assets, uploaded/downloadable by the user, as well as for redundant database backup.",
                                    "useCase": "The MUtopia team is developing a large web application for the modelling, simulation and visualisation of urban development projects. The servers will host the web application and its databases for access by researchers and clients, public and private, current and future. They should improve performance over our current arrangement, and provide greater redundancy and data security. The project will be ongoing beyond this year, but this should be sufficient for that time. I'm not sure how to estimate core hours for a 24/7 web server. See http://mutopia.unimelb.edu.au/ for more information."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.3,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM MUtopia",
                                    "usagePatterns": "Currently, our system is hosting moderately large precinct models (thousands of buildings with hundreds of parameters each), plus some large assets (such as uploaded 3D models). The scale and number of these projects is expected to grow, potentially substantially, over the next year as we acquire more clients to work with and develop more sophisticated models. However CPU usage is typically light, with the occasional demanding simulation. We would have a primary instance as a production server, and a secondary, identical server for development staging. The final server would be split between hosting our development tools and smaller, temporary demo instances in VMs. Object storage would be utilised to store assets, uploaded/downloadable by the user, as well as for redundant database backup.",
                                    "useCase": "The MUtopia team is developing a large web application for the modelling, simulation and visualisation of urban development projects. The servers will host the web application and its databases for access by researchers and clients, public and private, current and future. They should improve performance over our current arrangement, and provide greater redundancy and data security. The project will be ongoing beyond this year, but this should be sufficient for that time. I'm not sure how to estimate core hours for a 24/7 web server. See http://mutopia.unimelb.edu.au/ for more information."
                                }
                            ],
                            "name": "080603"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 0.3,
                                    "institution": "uq.edu.au",
                                    "name": "Crop Trait Mining Informatics Platform",
                                    "usagePatterns": "Initially a pilot project so will begin with 10s of users and moderate sized data sets (by plant improvement and genetic resources standards). Expected to gradually grow and either become the primary instance or a mirror of the primary instance of a global information system to facilitate the access to and use of germplasm in plant improvement.",
                                    "useCase": "Pilot phase will be a single instance for development and data aggregation purposes within Australia. It will involve installing Genesys I and uploading data from several GRDC funded research projects. Classes of data stored include plant genetic resources passport information together with associated characterization, phenotypic and environmental data. A GRDC funded use case to identify spring radiation frost tolerance in wheat will validate the value of a collaborative, global approach to sharing, storing and making these data classes publically available to facilitate their use in addressing food security/productivity, climate change and associated challenges. The project is anticipated to evolve into an integrated, easily accessible research data resource, together with tools to facilitate the identification and use of plant germplasm, and be part of a wider global system for plant improvement."
                                }
                            ],
                            "name": "080606"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.2,
                                    "institution": "murdoch.edu.au",
                                    "name": "Murdoch_Agri_Bio_Data_Integration",
                                    "usagePatterns": "This is a low level computing project will require the upload/process large datasets (500Mb per data source). There will be extensive testing (of random subsets of a data source) to optimise which (and how many data-sources) to be used before large process will occur.",
                                    "useCase": "As more datasets become available, researchers need to be smarter in how they integrate this information into a structure that will enable the evaluation of how cereal (wheat/barley) plants respond to changing environmental conditions to meet international grain markets. This research activity will utilise research in data ontology standards and statistical/bioinformatic tools to research data-pipelines for data mining of agricultural data."
                                }
                            ],
                            "name": "080610"
                        }
                    ],
                    "name": "0806"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 5.0,
                                    "instanceQuota": 5.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Juju on Nectar",
                                    "usagePatterns": "Single user.",
                                    "useCase": "I've had some success setting up Canonical's JuJu to work on the research cloud, however I haven't had an opportunity to try a deploy beyond 2 machines (control and the actual deployed instance), given the limits of the initial tenancy. Assuming it all works, I'll make all the details and configuration available to the wider community."
                                },
                                {
                                    "coreQuota": 22.0,
                                    "instanceQuota": 22.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "RDSI Aspera central services",
                                    "usagePatterns": "",
                                    "useCase": "Test central aspera node operations for RDSI storage options. currently testing on PT-1400 and have run out of cores"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 5.0,
                                    "institution": "monash.edu",
                                    "name": "ANDS_NeCTAR_RDSI_Collaboration_Dev",
                                    "usagePatterns": "",
                                    "useCase": "ANDS-NeCTAR-RDSI are working together to mock up a few exemplars to show the possible collaboration between three key eResearch providers. To better progress with this project, I need additional VM for testing and demo purposes. "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uq.edu.au",
                                    "name": "Windows server QCIF services",
                                    "usagePatterns": "Require windows flavour q3.large.",
                                    "useCase": "Test case to determine the ease of creation of windows servers for Queensland Node Clients."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 5.0,
                                    "institution": "monash.edu",
                                    "name": "ANDS_NeCTAR_RDSI_Collaboration_Dev",
                                    "usagePatterns": "",
                                    "useCase": "ANDS-NeCTAR-RDSI are working together to mock up a few exemplars to show the possible collaboration between three key eResearch providers. To better progress with this project, I need additional VM for testing and demo purposes. "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "Windows server QCIF services",
                                    "usagePatterns": "Require windows flavour q3.large.",
                                    "useCase": "Test case to determine the ease of creation of windows servers for Queensland Node Clients."
                                },
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 20.0,
                                    "institution": "anu.edu.au",
                                    "name": "NCI_OpenStack_Test",
                                    "usagePatterns": "random read/write mix and variable IOPS workloads. Small to medium datasets 100M-10G",
                                    "useCase": "NCI requires access to CPU core hours, block storage and object storage at production Nectar nodes to get familiar with existing environments."
                                }
                            ],
                            "name": "08"
                        }
                    ],
                    "name": "08"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_CI_Server_SDSL",
                                    "usagePatterns": "We have about 10 users and few files (<1000) of up to 2 GB. ",
                                    "useCase": "Automated Builds and Testing for a Data Structure Library I am developing an open source C++ library for succinct data structures, called SDSL (see https://github.com/simongog/sdsl-lite). It is developed together with other international researchers and therefore hosted on github.com to make it easy to contribute code. However, there is now service, which runs our test suits automatically. With the nectar cloud infrastructure, we can easily set up a continues integrations server (like jenkins) which builds and tests the software after each new commit. The software library itself is the basis for different software packages in Bioinformatics and Information Retrieval. Don't hesitate to contact me, if you need more details. I am located at level 8 DMD and can drop by very quickly."
                                }
                            ],
                            "name": "080401"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.1,
                                    "instanceQuota": 2.1,
                                    "institution": "csiro.au",
                                    "name": "eReefs",
                                    "usagePatterns": "Users: about 10 users (our project team plus extras).  Expected use: 2 or 3 of those 10 users using 2 or 3 machines at a time. Datasets: small (single digit gigabytes at a max). These machines are for config testing, not computation. Some of the datasets are just RDF files that need public web hosting so some machine's OSes will take up far more room than their data.",
                                    "useCase": "In eReefs we're designing and implementing server components that enable organisations holding data to deliver it via standardised service. We, the CSIRO team, build the servers or server parts and then hope to transfer images of them - or perhaps even just knowledge of how to install them - to the other eReefs partner organisations (BoM, AMIS, Queensland Government, IMOS) who will ultimately house them. This means we have a requirement to set up VMs, test their configurations, snapshot them and then take them down. We don't have large computational or data storage needs - our biggest challenge is configuration management."
                                }
                            ],
                            "name": "080404"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_CI_Server_SDSL",
                                    "usagePatterns": "We have about 10 users and few files (<1000) of up to 2 GB. ",
                                    "useCase": "Automated Builds and Testing for a Data Structure Library I am developing an open source C++ library for succinct data structures, called SDSL (see https://github.com/simongog/sdsl-lite). It is developed together with other international researchers and therefore hosted on github.com to make it easy to contribute code. However, there is now service, which runs our test suits automatically. With the nectar cloud infrastructure, we can easily set up a continues integrations server (like jenkins) which builds and tests the software after each new commit. The software library itself is the basis for different software packages in Bioinformatics and Information Retrieval. Don't hesitate to contact me, if you need more details. I am located at level 8 DMD and can drop by very quickly."
                                },
                                {
                                    "coreQuota": 1.4,
                                    "instanceQuota": 1.4,
                                    "institution": "csiro.au",
                                    "name": "eReefs",
                                    "usagePatterns": "Users: about 10 users (our project team plus extras).  Expected use: 2 or 3 of those 10 users using 2 or 3 machines at a time. Datasets: small (single digit gigabytes at a max). These machines are for config testing, not computation. Some of the datasets are just RDF files that need public web hosting so some machine's OSes will take up far more room than their data.",
                                    "useCase": "In eReefs we're designing and implementing server components that enable organisations holding data to deliver it via standardised service. We, the CSIRO team, build the servers or server parts and then hope to transfer images of them - or perhaps even just knowledge of how to install them - to the other eReefs partner organisations (BoM, AMIS, Queensland Government, IMOS) who will ultimately house them. This means we have a requirement to set up VMs, test their configurations, snapshot them and then take them down. We don't have large computational or data storage needs - our biggest challenge is configuration management."
                                }
                            ],
                            "name": "080403"
                        }
                    ],
                    "name": "0804"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Data Collections Registry",
                                    "usagePatterns": "The service aggregates data from internal metadata stores together. It provides the resulting merged data as feeds to the National Library of Australia & Research Data Australia, and as RDFa-annotated web pages to the general public. The datasets are not large, and most processing is batched. Direct traffic volume is very low, as most hits are to sites receiving the outputted data feeds. (e.g. NLA Trove) Existing hosting consists of two servers with 8GB of persistent disk (including OS). Application size including libraries & logs is around <2GB, with database size of <100MB. For security reasons, staff records are indexed and anonymized on a separate server to the one which provides public data. Application size is ~500MB, with database and indexes <200MB. Object storage may be used for periodic rolling backups of the databases.",
                                    "useCase": "The ANDS-funded UQ Data Collections Registry (UQ-DCR) project has developed an institutional metadata store for The University of Queensland.  It issues NLA party identifiers for UQ researchers and provides a central registry where research data collection metadata records for UQ can be stored & accessed. It is currently available at: http://research.data.uq.edu.au/ More information on the project can be found here: https://projects.ands.org.au/id/MS06 It is currently hosted on the QCIF Early Research Node (QERN), which is being decommissioned. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Data Collections Registry",
                                    "usagePatterns": "The service aggregates data from internal metadata stores together. It provides the resulting merged data as feeds to the National Library of Australia & Research Data Australia, and as RDFa-annotated web pages to the general public. The datasets are not large, and most processing is batched. Direct traffic volume is very low, as most hits are to sites receiving the outputted data feeds. (e.g. NLA Trove) Existing hosting consists of two servers with 8GB of persistent disk (including OS). Application size including libraries & logs is around <2GB, with database size of <100MB. For security reasons, staff records are indexed and anonymized on a separate server to the one which provides public data. Application size is ~500MB, with database and indexes <200MB. Object storage may be used for periodic rolling backups of the databases.",
                                    "useCase": "The ANDS-funded UQ Data Collections Registry (UQ-DCR) project has developed an institutional metadata store for The University of Queensland.  It issues NLA party identifiers for UQ researchers and provides a central registry where research data collection metadata records for UQ can be stored & accessed. It is currently available at: http://research.data.uq.edu.au/ More information on the project can be found here: https://projects.ands.org.au/id/MS06 It is currently hosted on the QCIF Early Research Node (QERN), which is being decommissioned. "
                                }
                            ],
                            "name": "080707"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 2.0,
                                    "institution": "rmit.edu.au",
                                    "name": "RMIT_TRIIBE",
                                    "usagePatterns": "",
                                    "useCase": "This is a Linkage project Title: TRIIBE  TRacking Indoor Information BEhaviour (code: LP120200413) This project will research the passive tracking of user's mobile devices in indoor spaces correlating their spatial behaviour with their information needs to deliver personalised information. The project will create a system that enables owners of large buildings (for example, shopping malls, airports, universities) to better manage their spaces  and services."
                                }
                            ],
                            "name": "0807"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 22.4,
                                    "instanceQuota": 1.4,
                                    "institution": "flinders.edu.au",
                                    "name": "Patient Trajectory Visualization tool",
                                    "usagePatterns": "Large data set with 10-30 users",
                                    "useCase": "We are developing an application that  uses prior health records of mental health patients. The application has 2 components.  HBase and neo4j database of patients prior data. n = 2000 patients, between 50-200 records for each patient. Javascript and HTML5 front end with analytics/visualisation tools. Users n = 20-40  "
                                }
                            ],
                            "name": "080702"
                        }
                    ],
                    "name": "0807"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 48.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Computational Resource Framework Project",
                                    "usagePatterns": "",
                                    "useCase": "The Computational Resource Framework (CRF) Project is a solution being developed at Griffith University in collaboration with Monash University. This Project aims to provide seamless, user-friendly access, submission and file storage for multiple HPC and cloud resources."
                                },
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 12.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Computational Resource Framework Project",
                                    "usagePatterns": "",
                                    "useCase": "The Computational Resource Framework (CRF) Project is a solution being developed at Griffith University in collaboration with Monash University. This Project aims to provide seamless, user-friendly access, submission and file storage for multiple HPC and cloud resources."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.8,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The project is now in the phase where parallel calculations are possible.  We use all available cores, scheduling done by GNU parallel. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The memory is the real bottleneck, so possibly we will not be able to use all cores. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The memory is the real bottleneck, so possibly we will not be able to use all cores. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The project is now in the phase where parallel calculations are possible.  We use all available cores, scheduling done by GNU parallel. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                }
                            ],
                            "name": "080203"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.8,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The project is now in the phase where parallel calculations are possible.  We use all available cores, scheduling done by GNU parallel. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The memory is the real bottleneck, so possibly we will not be able to use all cores. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The memory is the real bottleneck, so possibly we will not be able to use all cores. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The project is now in the phase where parallel calculations are possible.  We use all available cores, scheduling done by GNU parallel. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                }
                            ],
                            "name": "080202"
                        }
                    ],
                    "name": "0802"
                }
            ],
            "name": "08"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN Ecoinformatics (Australian Ecological Knowledge and Observation System)",
                                    "usagePatterns": "",
                                    "useCase": "Ingestion and federation of data from various government agencies, universities and NGOs; processing of textual, spatial and imagery data; databases; software development; web applications. Also interacting and exchanging data with other systems such as SHaRED (NeCTAR), Soils to Satellites (ANDS and ALA) and the TERN Multi-Scale Plot Network (MSPN), however these are separate projects acquiring their own allocations. Some VMs are envisaged as offering long-lived services whereas some will be temporary and used for development, testing or short-term processing. "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. Most of the resource (the extra we are requesting) will be for a trial deployment of a dynamic torque cluster on the SA node to augment shared HPC (cloudbursting). "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 1.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. We need some group resource to do this until the SA cloud node is in production. "
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 6.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN-AusPlots",
                                    "usagePatterns": "We will have few users with large datasets.",
                                    "useCase": "TERN AusPlots is a research facility that collects ecological (transect) data through the use of a tablet app.  The services which house the collected data are run in Nectar (in the Aekos project) and now AusPlots needs its own space for existing services, and soon to be developed PhotoPoint data collection (which is much more compute and data intensive than the current Tomcat and Postgres apps."
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN Ecoinformatics (Australian Ecological Knowledge and Observation System)",
                                    "usagePatterns": "",
                                    "useCase": "Ingestion and federation of data from various government agencies, universities and NGOs; processing of textual, spatial and imagery data; databases; software development; web applications. Also interacting and exchanging data with other systems such as SHaRED (NeCTAR), Soils to Satellites (ANDS and ALA) and the TERN Multi-Scale Plot Network (MSPN), however these are separate projects acquiring their own allocations. Some VMs are envisaged as offering long-lived services whereas some will be temporary and used for development, testing or short-term processing. "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. We need some group resource to do this until the SA cloud node is in production. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "PLEA",
                                    "usagePatterns": "The project comprises a web server for data collection and analysis. Volumes or RDSI storage allocation will be used for a relational database. Object Store and RDSI storage will be used for image (photo and video) data storage.  ",
                                    "useCase": "Off the coast of Point Lookout (Stradbroke Island, Queensland) lie a number of rocky reef systems that support and attract a diverse range of marine flora and fauna, as well as recreational and commercial activities such as fishing and SCUBA diving. These activities along with environmental impacts such as floods may be affecting the ecological health of the area. This study will assess the health of these systems in comparison to a similar study carried out in 2001. The aim of the project is to repeat the 2001 ecological assessment of the Point Lookout (North Stradbroke Island) marine environment using a community based, cost effective, scientifically sound survey program. At each site transect surveys will be conducted to assess the populations of fish, invertebrates, coral state, bottom type and impacts. Five transect sites will be used: two at Flat Rock, two at Shag Rock and one at Manta Bommie. Detailed maps of Shag Rock and Manta Bommie will be created. Photos and video will be acquired to document the species found at these locations. The Principal Investigator, who also holds the QLD RDSI allocation is Dr Chris Roelfsema (c.roelfsema@uq.edu.au).  "
                                },
                                {
                                    "coreQuota": 5.2,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution. The mammal occurrence related to climate change will visualise 100 mammal occurrence model output data for about 60 years "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.6,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution. The mammal occurrence related to climate change will visualise 100 mammal occurrence model output data for about 60 years "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 1.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN-Adel",
                                    "usagePatterns": "The project provides a User Accessible portal - AEKOS that contains large amounts of ecological data, binary and images. The initial estimates for the amounts of data in the system have been revised and we now require extra data capacity. ",
                                    "useCase": "As per our initial request that established the TERN-Adel tenancy.  The issue we now face is that we are being requested to provide individual AEKOS environments to our data providers in order for them to test and verify their datasets inside AEKOS prior to publication in production. The data ingestion processes that we need to run to create AEKOS data is extremely memory and processor intensive and generates a significant amount of data as an output. Our existing allocation is no longer able to cope with the demand we are receiving.  "
                                }
                            ],
                            "name": "0501"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "utas.edu.au",
                                    "name": "NERP Web Fire Mapping",
                                    "usagePatterns": "A small number of users with moderate-sized data sets - initially one user managing a suite of spatial data, but more users may be added if necessary.  After initial map rendering, CPU requirements will be low.",
                                    "useCase": "Provide spatial data storage, map tile rendering and web mapping service serving for the National Environmental Research Program (Landscapes and Policy Hub)."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "jcu.edu.au",
                                    "name": "Exposing JCU research",
                                    "usagePatterns": "VM users will be small however, we would hope that web access to the portal would increase with time.",
                                    "useCase": "web portal for collaboration and dissemination of research outputs that will include reports, publications, datasets, databases, etc..  This will be maintained by JCU's eResearch Centre and we will assist researchers with development on this VM."
                                },
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 10.0,
                                    "institution": "griffith.edu.au",
                                    "name": "TerraNova Climate Change Adaptation Information Hub",
                                    "usagePatterns": "One server will have many users and small data sets.  The map visualization server will have a few large data sets and will be accessed through a publicly accessible web interface.",
                                    "useCase": "Climate hub contains a repository that links climate practitioners to researchers.  It also contains tools to allow Australian climate practitioners to explore future climate scenarios through interactive map interfaces.  One server functions as a repository, the other runs map server and provides interfaces for examining future climate scenario data.  The repository can be a medium VM."
                                },
                                {
                                    "coreQuota": 72.0,
                                    "instanceQuota": 72.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Biodiversity Climate Change VL",
                                    "usagePatterns": "There will be many users accessing predominately many small data sets.  Having said that some climate data constitutes very large data sets.  We are hoping to have 100TB of attached RDSI storage to the instance.",
                                    "useCase": "The BCCVL is a NeCTAR funded Virtual lab that is being designed to provide Species Distribution and Functional Response Modellers with the appropriate data sets and computation environments to advance their research capabilities. This extension is due to a limited public release scheduled for the 9th of December.  We are hoping to have enough cores so that experiments run during this period complete in a reasonable timeframe."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CLiMAS_1",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the web service side of a citizen science / communication site on the potential impacts of climate change on Australian fauna. This project was previously on QERN."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CLiMAS_2",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the compute side of a citizen science / communication site on the potential impacts of climate change on Australian fauna. This project was previously on QERN. NOTE: this is associated with CLiMAS_1 VM request!!!"
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF Climate Data",
                                    "usagePatterns": "",
                                    "useCase": "This is a web-service vm to serve up climate change models associated with a 40TB storage allocation within QCloud. These climate change scenarios are used heavily by researchers at 5 universities and have been limited due to limited access."
                                },
                                {
                                    "coreQuota": 11.2,
                                    "instanceQuota": 2.8,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_IMAS_Predators",
                                    "usagePatterns": "The project will have moderately lager data sets (e.g remote sensing data from the Southern Ocean encompassing several decades), but relatively few simultaneous users - typically no more than 2",
                                    "useCase": "Our VM is used to run complex, Bayesian statistic models, usually in relation to animal movement and habitat analyses"
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "usc.edu.au",
                                    "name": "Marine climate-change dynamics",
                                    "usagePatterns": "Usage patterns will vary. Analyses will initially be relatively small, running intermittently on one or two VMs, but as the project scales up, usage will escalate. There will not be more than three users.",
                                    "useCase": "VMs will be used to run global-scale analyses of the trajectories of climate-change metrics in the global ocean (sea-surface temperature, pH, salinity, aragonite, calcite and oxygen concentrations, etc.). All data underlying the analyses are publicly available."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.4,
                                    "institution": "mq.edu.au",
                                    "name": "ecosystem Modelling and Scaling (eMAST",
                                    "usagePatterns": "We will establish a metadata, wiki and map service to distribute summary information on the data and products eMAST produces. The single instance will run software, available to the public, that will serve low volume metadata across Australia.",
                                    "useCase": "Users need to be able to easily discover ecosystem Modelling and Scaling Infrastructure (eMAST) data which includes estimates of Australia's climate, carbon and water cycle. Using a mixture of metadata and geospatial tools, this service will share information on eMAST's products and services."
                                }
                            ],
                            "name": "050101"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "csiro.au",
                                    "name": "Atlas of Living Australia",
                                    "usagePatterns": "Small number of users, large datasets.",
                                    "useCase": "The Atlas uses large SOLR indexes to power its spatial search. The requested allocation would give the Atlas an environment to test the capabilities of SolrCloud which provides index replication functionality."
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "csiro.au",
                                    "name": "Atlas of Living Australia",
                                    "usagePatterns": "Small number of users, large datasets.",
                                    "useCase": "The Atlas uses large SOLR indexes to power its spatial search. The requested allocation would give the Atlas an environment to test the capabilities of SolrCloud which provides index replication functionality."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 8.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "ecological informatics",
                                    "usagePatterns": "large data sets with a small number of users",
                                    "useCase": "1. To develop forcasting rule set models for early-warning of cyanobacteria based on long-term historic lake data which is very time-consuming 2. It belongs to partial research of an ARC Linkage project"
                                }
                            ],
                            "name": "050199"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "anu.edu.au",
                                    "name": "BushFM",
                                    "usagePatterns": "continuously running.  Strorage on NFS mount",
                                    "useCase": "James Cook University and ANU are hosting a TERN funded Environmental Acoustics project called BushFM.  Bush.fm version 1 was on QERN and is now being migrated across to QRISCloud with the Project number for BushFM storage that is Q0055."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uq.edu.au",
                                    "name": "Marxan conservation planning software development",
                                    "usagePatterns": "We're using RDSI storage so don't require additional object storage or storage volumes. We'd like to have the Marxan RDSI allocation statically linked to the primary Marxan VM's that are online continuously: marxan.net and roflcraft.net (being our production server, and our slave/experimental server). We'll have a large number of users and a large number of datasets from very small to very large. We anticipate the production system will go online early in 2014, and the computation performed by the system will incrementally ramp up between now and then. We'd like to make an additional request for increased computation resources when the production system is online. With the current request, we'd like to use about half the cores for continuously operating VM's, and we'd like to use the other half of the cores for cloud bursting with the Nimrod API. We're working with Minh Huynh at UQ RCC to use Nimrod cloud bursting API. The cloud bursting VM's will be used in this way: - At run time in the production system, Nimrod will spawn a large number of small VM's, - Once the computational tasks are completed by the large number of small VM's, Nimrod will terminate the large number of small VM instances, and return the computational resources to the NeCTAR pool, - This pattern of spawning a large number of small VM's and terminating the VM's when computation is complete will occur periodically on demand. The nature of the new calibration algorithms we're developing require more computational resources than we've used in the past for Marxan. This gives the software new and enhanced functionality at the expense of requiring more computational resources on demand for short periods of time.",
                                    "useCase": "I want to use the research cloud to assist me with cloud migration of the Marxan software. Marxan is the most widely used decision support software for conservation planning globally. It is used in over 100 countries to build marine and terrestrial conservation plans and national parks systems.  It dominates the world market for conservation land-use planning and new extensions are making it even more popular.   Currently, the only supported deployment model of the software is through download and install by users. We have initiated an internal project to migrate the software to the QCIF research cloud (Q-Cloud) to enhance the utility of the software for ourselves and our researcher collaborators. The cloud migration will improve the ability for us to inform conservation planning decisions in Australia and globally and increase its utility worldwide. Among our global users, there is a large number of government, educational and research organisations in Queensland who use Marxan to support their core business, including DEHP, DNRM, DNPRSR, QPWS, UQ, QUT, JCU, CQU, Griffith uni, and Bond uni. There&#39;s also a large number of government, research, and educational users nationwide. These users will benefit through improved delivery of decision support, enhanced scalability of computation, improved software performance, and better ease of use. The software will be made available to select researchers in Queensland and around the world. Their ability to conduct large scale Marxan analysis will be enhanced, which will boost research, publications, citations, and impact factor, and improve the ability to influence conservation decisions globally. This is part of a broader initiative to globalise the impact of the University of Queenslands conservation science.  Marxan is likely to be one of the major impact stories when the ARC roll out impact assessment as part of the way the federal government fund universities. I have several test servers currently in use, and am conducting research and development that require larger scale computational resources. The architecture I'm developing uses R-Studio server as the front end for highly optimised C code. www.uq.edu.au/marxan"
                                }
                            ],
                            "name": "050104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "CSS_Shallow",
                                    "usagePatterns": "LArge data set and a small number of users (2-4).",
                                    "useCase": "These instances will be used to process image analysis from a large collection of underwater imager to handle, post-produce images, model 3D reconstructions and perform automated image annotation."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.2,
                                    "institution": "mq.edu.au",
                                    "name": "ecosystem Modelling and Scaling (eMAST",
                                    "usagePatterns": "We will establish a metadata, wiki and map service to distribute summary information on the data and products eMAST produces. The single instance will run software, available to the public, that will serve low volume metadata across Australia.",
                                    "useCase": "Users need to be able to easily discover ecosystem Modelling and Scaling Infrastructure (eMAST) data which includes estimates of Australia's climate, carbon and water cycle. Using a mixture of metadata and geospatial tools, this service will share information on eMAST's products and services."
                                },
                                {
                                    "coreQuota": 7.8,
                                    "instanceQuota": 0.9,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution. The mammal occurrence related to climate change will visualise 100 mammal occurrence model output data for about 60 years "
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 0.9,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution."
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution."
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution. The mammal occurrence related to climate change will visualise 100 mammal occurrence model output data for about 60 years "
                                }
                            ],
                            "name": "050102"
                        }
                    ],
                    "name": "0501"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "rmit.edu.au",
                                    "name": "\"Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory. "
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "rmit.edu.au",
                                    "name": "Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "I've requested this allocation for another 12 months as we expect to be using this allocation  throughout this time. We expect to have a small number of users (~4) but some of us will be running large jobs that may take 30-50 core hours to run each. These may involve input and output data that take up to 5 Gig per machine (though many runs will be less then this). This will be for running simulations using the tzar framework for publications we are working on and also for testing and developing tzar further to be a component in the Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. ",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large Simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory."
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "usc.edu.au",
                                    "name": "Marine climate-change dynamics",
                                    "usagePatterns": "Usage patterns will vary. Analyses will initially be relatively small, running intermittently on one or two VMs, but as the project scales up, usage will escalate. There will not be more than three users.",
                                    "useCase": "VMs will be used to run global-scale analyses of the trajectories of climate-change metrics in the global ocean (sea-surface temperature, pH, salinity, aragonite, calcite and oxygen concentrations, etc.). All data underlying the analyses are publicly available."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "canberra.edu.au",
                                    "name": "GlobalWeb Food Web DB",
                                    "usagePatterns": "This instance will host Window Server 2008. As a consequence, we require a 30GB root disk allocation. NeCTAR support have indicated that this is available on the Monash node.",
                                    "useCase": "Food webs are maps of the feeding relationships between plants and animals. Describing these networks in detail requires considerable investment of time and money, and as a result, most researchers describe a very small number of food webs. However questions in food web ecology such as Are there particular rules that govern how food webs assemble? Do particular types of habitats have particular food webs? What will climate change do to food web structure and Are some food webs more vulnerable to invasive species? are all best addressed using large collections of food webs from many different places. The GlobalWeb Initiative collects for the first time all of the worlds published food webs to facilitate research. A web-based searchable database will be the repository of these food webs and will be of international importance.  This initiative has already been supported by a Future Fellowship (ARC FT120103010 to Thompson) and is the basis of several additional bids. A core group of researchers from seven nations have lent their support to the first phase of analyses of this unique dataset. The Global Web Initiative will showcase UCs and Australias e-research capabilities and directly lead to international collaborations, funding applications and high-profile publications. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO ALA-NPEI",
                                    "usagePatterns": "Many users, small datasets",
                                    "useCase": "These instances will support the investigation and use of the SISS stack to support data sharing between the Atlas of Living Australia and the NPEI project being ran by the Bureau of Meteorology."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "rmit.edu.au",
                                    "name": "Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "I've requested this allocation for 6 months as we expect to be using this allocation sporadically throughout this time. We expect to have a small number of users (~4) but some of us will be running large jobs that may take 30-50 core hours to run each. These may involve input and output data that take up to 10 Gig per machine (though many runs will be less then this). This will be for running simulations using the tzar framework for publications we are working on and also for testing and developing tzar further to be a component in the Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. ",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "jcu.edu.au",
                                    "name": "Exposing JCU research",
                                    "usagePatterns": "VM users will be small however, we would hope that web access to the portal would increase with time.",
                                    "useCase": "web portal for collaboration and dissemination of research outputs that will include reports, publications, datasets, databases, etc..  This will be maintained by JCU's eResearch Centre and we will assist researchers with development on this VM."
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "csiro.au",
                                    "name": "Atlas of Living Australia",
                                    "usagePatterns": "Small number of users, large datasets.",
                                    "useCase": "The Atlas uses large SOLR indexes to power its spatial search. The requested allocation would give the Atlas an environment to test the capabilities of SolrCloud which provides index replication functionality."
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "csiro.au",
                                    "name": "Atlas of Living Australia",
                                    "usagePatterns": "Small number of users, large datasets.",
                                    "useCase": "The Atlas uses large SOLR indexes to power its spatial search. The requested allocation would give the Atlas an environment to test the capabilities of SolrCloud which provides index replication functionality."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CLiMAS_1",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the web service side of a citizen science / communication site on the potential impacts of climate change on Australian fauna. This project was previously on QERN."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CLiMAS_2",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the compute side of a citizen science / communication site on the potential impacts of climate change on Australian fauna. This project was previously on QERN. NOTE: this is associated with CLiMAS_1 VM request!!!"
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "Crop Trait Mining Informatics Platform",
                                    "usagePatterns": "Initially a pilot project so will begin with 10s of users and moderate sized data sets (by plant improvement and genetic resources standards). Expected to gradually grow and either become the primary instance or a mirror of the primary instance of a global information system to facilitate the access to and use of germplasm in plant improvement.",
                                    "useCase": "Pilot phase will be a single instance for development and data aggregation purposes within Australia. It will involve installing Genesys I and uploading data from several GRDC funded research projects. Classes of data stored include plant genetic resources passport information together with associated characterization, phenotypic and environmental data. A GRDC funded use case to identify spring radiation frost tolerance in wheat will validate the value of a collaborative, global approach to sharing, storing and making these data classes publically available to facilitate their use in addressing food security/productivity, climate change and associated challenges. The project is anticipated to evolve into an integrated, easily accessible research data resource, together with tools to facilitate the identification and use of plant germplasm, and be part of a wider global system for plant improvement."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "une.edu.au",
                                    "name": "RevegUNE",
                                    "usagePatterns": "Database",
                                    "useCase": "PhD project that will investigate germination and establishment requirements of about 160 common native species of tree, shrub, forb and grass in crop lands in Moree Area  NSW. UNE-Border Rivers Gwydir Catchment Management Authorithy."
                                }
                            ],
                            "name": "050202"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "une.edu.au",
                                    "name": "RevegUNE",
                                    "usagePatterns": "Database",
                                    "useCase": "PhD project that will investigate germination and establishment requirements of about 160 common native species of tree, shrub, forb and grass in crop lands in Moree Area  NSW. UNE-Border Rivers Gwydir Catchment Management Authorithy."
                                },
                                {
                                    "coreQuota": 28.8,
                                    "instanceQuota": 28.8,
                                    "institution": "uq.edu.au",
                                    "name": "Marxan conservation planning software development",
                                    "usagePatterns": "We're using RDSI storage so don't require additional object storage or storage volumes. We'd like to have the Marxan RDSI allocation statically linked to the primary Marxan VM's that are online continuously: marxan.net and roflcraft.net (being our production server, and our slave/experimental server). We'll have a large number of users and a large number of datasets from very small to very large. We anticipate the production system will go online early in 2014, and the computation performed by the system will incrementally ramp up between now and then. We'd like to make an additional request for increased computation resources when the production system is online. With the current request, we'd like to use about half the cores for continuously operating VM's, and we'd like to use the other half of the cores for cloud bursting with the Nimrod API. We're working with Minh Huynh at UQ RCC to use Nimrod cloud bursting API. The cloud bursting VM's will be used in this way: - At run time in the production system, Nimrod will spawn a large number of small VM's, - Once the computational tasks are completed by the large number of small VM's, Nimrod will terminate the large number of small VM instances, and return the computational resources to the NeCTAR pool, - This pattern of spawning a large number of small VM's and terminating the VM's when computation is complete will occur periodically on demand. The nature of the new calibration algorithms we're developing require more computational resources than we've used in the past for Marxan. This gives the software new and enhanced functionality at the expense of requiring more computational resources on demand for short periods of time.",
                                    "useCase": "I want to use the research cloud to assist me with cloud migration of the Marxan software. Marxan is the most widely used decision support software for conservation planning globally. It is used in over 100 countries to build marine and terrestrial conservation plans and national parks systems.  It dominates the world market for conservation land-use planning and new extensions are making it even more popular.   Currently, the only supported deployment model of the software is through download and install by users. We have initiated an internal project to migrate the software to the QCIF research cloud (Q-Cloud) to enhance the utility of the software for ourselves and our researcher collaborators. The cloud migration will improve the ability for us to inform conservation planning decisions in Australia and globally and increase its utility worldwide. Among our global users, there is a large number of government, educational and research organisations in Queensland who use Marxan to support their core business, including DEHP, DNRM, DNPRSR, QPWS, UQ, QUT, JCU, CQU, Griffith uni, and Bond uni. There&#39;s also a large number of government, research, and educational users nationwide. These users will benefit through improved delivery of decision support, enhanced scalability of computation, improved software performance, and better ease of use. The software will be made available to select researchers in Queensland and around the world. Their ability to conduct large scale Marxan analysis will be enhanced, which will boost research, publications, citations, and impact factor, and improve the ability to influence conservation decisions globally. This is part of a broader initiative to globalise the impact of the University of Queenslands conservation science.  Marxan is likely to be one of the major impact stories when the ARC roll out impact assessment as part of the way the federal government fund universities. I have several test servers currently in use, and am conducting research and development that require larger scale computational resources. The architecture I'm developing uses R-Studio server as the front end for highly optimised C code. www.uq.edu.au/marxan"
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 8.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "ecological informatics",
                                    "usagePatterns": "large data sets with a small number of users",
                                    "useCase": "1. To develop forcasting rule set models for early-warning of cyanobacteria based on long-term historic lake data which is very time-consuming 2. It belongs to partial research of an ARC Linkage project"
                                }
                            ],
                            "name": "050209"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "csiro.au",
                                    "name": "TERN Australian Coastal Ecosystems Facility",
                                    "usagePatterns": "Please ignore the core hours field above - as far as I can tell, it does not really apply for how we plan to use these servers. We will be hosting 10 to 50 datasets, with most being regularly added to. About half will be file-based, and we hope to use object data storage for those: the initial space requirement is around 2TB, expected to grow by 100GB/month.  The remaining datasets will require databases: MySQL, PostgreSQL/PostGIS and MongoDB. These will need to be able to be backed up regularly. Open-source data-services and GUI applications will be used to make this data available to the general public. All data requests will be made via HTTP, routed to the appropriate services via Apache. Load will be highly variable, but should not be sustained at high levels. There will be a limited number of concurrent users, fewer than 10 with login rights.",
                                    "useCase": "This request is for infrastructure to be associated with several projects, including: * Australian Coastal Ecosystems Facility (ACEF),  which is part of TERN (see http://acef.tern.org.au/?q=data) * The South-east Queensland Integrated Terrestrial to Ocean Research (SEQITOR), which is funded by ANDS (see  http://blog.seqitor.org.au/)  * eReefs (http://ereefs.org.au/) These servers will store and serve a combination of spatial time-series data, geo-referenced video data, remote sensing imagery, and 4D model outputs.  The VM size specified above is not required for all 6 servers, but is the best choice the form would allow. The actual planned use and size requirement for each one is: Server 1. Map Rendering and tile cache (GeoServer / THREDDS). XXL VM please! Server 2. Real-time sensor storage and aggregation (SensorCloud). XL Server 3. Map Layer aggregation (GeoNetwork). XL Server 4. Web-GUI Applications. XL Server 5. Build and test environment. M. Server 6. Monitoring and control of other servers. M."
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "uq.edu.au",
                                    "name": "CMLR",
                                    "usagePatterns": "Right now less than 10 users with data sets constantly increased every 2 or 3 months.",
                                    "useCase": "CMLR needs to build up a centralised web-based database system for researchers of monitoring data (flora, water quality, sediment and etc...). Currently it contains more than 10 years of collected data from multiple projects. The data volume set is increasing every month as researchers collect new data from field. Some other small databases will also be integrated into the system as well, such as a research journal database."
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Acoustic_Work_Bench",
                                    "usagePatterns": "I think initially about 100 users and large data sets 100TB total but users will only access a small part of them. The data storage has been requested separately through Nectar. (core hours is a guess)",
                                    "useCase": "Collection of audible range terrestrial bio-acoustic (sound) recordings used for species monitoring, bio-diversity assessment and animal behaviour studies."
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "CMLR",
                                    "usagePatterns": "Right now less than 10 users with data sets constantly increased every 2 or 3 months.",
                                    "useCase": "CMLR needs to build up a centralised web-based database system for researchers of monitoring data (flora, water quality, sediment and etc...). Currently it contains more than 10 years of collected data from multiple projects. The data volume set is increasing every month as researchers collect new data from field. Some other small databases will also be integrated into the system as well, such as a research journal database."
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 3.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Acoustic_Work_Bench",
                                    "usagePatterns": "I think initially about 100 users and large data sets 100TB total but users will only access a small part of them. The data storage has been requested separately through Nectar. (core hours is a guess)",
                                    "useCase": "Collection of audible range terrestrial bio-acoustic (sound) recordings used for species monitoring, bio-diversity assessment and animal behaviour studies."
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "uts.edu.au",
                                    "name": "Intersect_Microbial_community_analysis",
                                    "usagePatterns": "3 users, data sets 10-30GB each, mix of I/O and CPU intensive work.",
                                    "useCase": "Analyzing microbial communities using 16S amplicon data generated by Illumina MiSeq instruments. Will use QIIME VM. "
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.3,
                                    "institution": "uts.edu.au",
                                    "name": "Intersect_Microbial_community_analysis",
                                    "usagePatterns": "3 users, data sets 10-30GB each, mix of I/O and CPU intensive work.",
                                    "useCase": "Analyzing microbial communities using 16S amplicon data generated by Illumina MiSeq instruments. Will use QIIME VM. "
                                }
                            ],
                            "name": "050206"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_OzTrack",
                                    "usagePatterns": "Amendment, 2013-11-18: As part of testing the current deliverables for this NeCTAR e-Research Tools project, we need to set up a UAT environment. I have updated the requested Instance/Core/Volume numbers to reflect the addition of one Large instance with an attached 100GB persistent volume. ---- NeCTAR cloud instances will host the OzTrack Java web application, GeoServer instance, PostgreSQL database, and a pool of Rserve instances used to compute R-based home range analyses and movement models. Potential configuration: one large instance for the Java servlet container and database; and four medium instances, each running Rserve, connected via TCP to main instance. Storage breakdown for current OzTrack deployment (oztrack.org): animal tracking data 1.0 GiB; external environmental layers, e.g. land use and bathymetry grids, 5.7 GiB; daily/weekly/monthly back-ups of animal tracking data 4.4 GiB. Animal tracking data is likely to remain small, but plans for pre-seeding tile caches of environmental layers in GeoServer mean that we are requesting 100 GiB of storage. As for the object store, we are not sure at this stage whether it will be required. We are considering using it for backups and potentially for storing environmental data files. We have applied for an allocation in order to experiment with it and decide whether it is appropriate.",
                                    "useCase": "The OzTrack project is supported by the NeCTAR e-Research Tools program for 2012-13. The goal of this project is to develop a set of eResearch software tools to support the compute, storage and analysis of animal tracking data being generated through telemetry devices. See http://oztrack.org/ and http://www.itee.uq.edu.au/eresearch/projects/oztrack/"
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 5.0,
                                    "institution": "monash.edu",
                                    "name": "ClimateWorks_CART",
                                    "usagePatterns": "Expected users 100 per deployed instance. Database expected to grow at a rate of 1 GB per year, per deployed instance Object Storage expected to grow at a rate of 1 GB per year, per deployed instance.",
                                    "useCase": "ClimateWorks Australia is a non-profit collaboration hosted by Monash University in partnership with The Myer Foundation that aims to facilitate substantial reductions in Australia's greenhouse gas emissions over the next five years. CART, the Climate Abatement Research Tool, is used by Climate Works Australia to develop Low Carbon Growth Plans (LCGPs). It provides a scalable web-based data management platform to support data collection, data storage (warehouse), data processing (i.e. cost curve development) and to progress tracking efforts (reporting).  A number of groups (UN, Monash Finance, Tohoku University (Japan)) have expressed interest in customizing CART for their use.  We need a dynamic environment where we can provision such requests. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Derived Digital Elevation Models (DEM)",
                                    "usagePatterns": "large data set with small number of users. ",
                                    "useCase": "Digital Elevation Model (DEM), smoothed DEM (DEMS) and hydrologically enforced DEM (DEMH) derived from 1 second (~30 m) Shuttle Radar Topography Mission (SRTM) data collected in 2000. The DEMs were produced by Geoscience Australia and are all version 1.0. The DEM was released in late 2009. The DEM-S was released in July 2010 and the DEMH was released in October 2011. Users should refer to the User Guide provided by Geoscience Australia with these datasets. Files are in ArcGIS-grid Geographic format. The digital elevation Models (DEM) covers all of continental Australia and near coastal islands land areas including all islands defined by the available SRTM 1 second elevation and Surface Water bodies Data Base datasets. These datasets are made available freely by Commonwealth of Australia (Geoscience Australia). This product is released under the Creative Commons Attribution 3.0 Australia Licence. http://creativecommons.org/licenses/by/3.0/au/deed.en . Creative Commons Attribution Australia Licence means data can be shared (copied, distributed, transmitted) or adapted provided you acknowledge the author or licensee. Consult the Creative Commons website for more information. http://creativecommons.org.au This data collection is used by a large number of researchers in environmental sciences, but currently is not available online from Geoscience Australia due to its large size (0.5TB).  Researchers use it for example to model floodplains, including inundation volume and surface water/ground water interactions, define catchment boundaries, etc. The collection is on Q0062 (located in Brisbane) We need this server to serve this collection on the web for wider community use. "
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 2.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_OzTrack",
                                    "usagePatterns": "Amendment, 2013-11-18: As part of testing the current deliverables for this NeCTAR e-Research Tools project, we need to set up a UAT environment. I have updated the requested Instance/Core/Volume numbers to reflect the addition of one Large instance with an attached 100GB persistent volume. ---- NeCTAR cloud instances will host the OzTrack Java web application, GeoServer instance, PostgreSQL database, and a pool of Rserve instances used to compute R-based home range analyses and movement models. Potential configuration: one large instance for the Java servlet container and database; and four medium instances, each running Rserve, connected via TCP to main instance. Storage breakdown for current OzTrack deployment (oztrack.org): animal tracking data 1.0 GiB; external environmental layers, e.g. land use and bathymetry grids, 5.7 GiB; daily/weekly/monthly back-ups of animal tracking data 4.4 GiB. Animal tracking data is likely to remain small, but plans for pre-seeding tile caches of environmental layers in GeoServer mean that we are requesting 100 GiB of storage. As for the object store, we are not sure at this stage whether it will be required. We are considering using it for backups and potentially for storing environmental data files. We have applied for an allocation in order to experiment with it and decide whether it is appropriate.",
                                    "useCase": "The OzTrack project is supported by the NeCTAR e-Research Tools program for 2012-13. The goal of this project is to develop a set of eResearch software tools to support the compute, storage and analysis of animal tracking data being generated through telemetry devices. See http://oztrack.org/ and http://www.itee.uq.edu.au/eresearch/projects/oztrack/"
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "sydney.edu.au",
                                    "name": "VL201 Industrial Ecology",
                                    "usagePatterns": "Initial usage is 15, with a maximum of 8 concurrent, final numbers could be up to 65 users nationally. Datasets are comprised of large files of statistical data from various sources e.g. ABS, Water usage, Waste data. The file sizes are significant with only 5 datasets  using the initial 10gb allocated to the project.",
                                    "useCase": "The Industrial Ecology project currently has a Medium environment as a pilot and there are insufficient RAM to run the processes to reach the initial datafeed milestones or consequent milestones that require further processing. Emails from the business sponsor and technical lead of the project can be forwarded as evidence of lack of RAM if required. Basic requirements from Technical lead are below: We need to operate a web server, a storage for a potentially large number of different MRIOs and the computing capability to run the analysis. If we run all of that stuff on the cloud, we need more crunch. If not, we should explore different solutions soon. For example that mass-storage facility at UQ. The homepage that we are running here for our global model has its own, dedicated machine with 8 cores, 1 TB of storage and 64 GB or RAM. I reckon for this project we would need at least the same. Kind regards Neal"
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "intersect.org.au",
                                    "name": "Intersect Demonstrations",
                                    "usagePatterns": "Intermittent use during demonstrations/ trials. ",
                                    "useCase": "Intersect has a variety of software packages that have been developed using federal funding and are freely available to Australian researchers. We would like to install example instances of this software so researchers may trial the software easily before deciding to install their own instance. These two packages are Genomic Data Analysis and HIEv Environmental Data Management."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "sydney.edu.au",
                                    "name": "SYD-IndEcologyVL",
                                    "usagePatterns": "Large data sets from a medium group of users which is growing on a weekly basis, ie, two new PHDs in May - June.",
                                    "useCase": "The team requires a storage allocation of 5tb to enable testign and work to continue as the existing 480gb has been consumed and it is estimated that at least 3tb will be in use by go-live December 2013. Hi Neal We're hitting constant disk-space problems - this problem has now become urgent - we need more space. Thanks Manfred [cid:E0E9A103-BEE5-4644-9E87-BDD1D77D24A7] Read our article in Nature: http://www.nature.com/nature/journal/v486/n7401/full/nature11145.html Begin forwarded message: From: Arne Geschke > Subject: disc full Date: May 31, 2013 6:50:12 AM GMT+10:00 To: Manfred Lenzen >, Arunima Malik >, Daniel Moran >, Joe Lane > Hi all, I've changed the upload code to adjust the rights of the uploaded file so that everybody can now delete their work directory. So you shouldn't need my help anymore to delete it. I've noticed that the disc is - again - completely full. Dan has got one job that is currently processing on scarlett, I don't know what will happen when that hits the upload queue. I can stop the uploader to avoid any problems. In that case, the data would remain on scarlett until the uploader is restarted. Dan, if you like, you can keep that data of this run on scarlett if that helps. Please advise/delete data from your directory. Thanks, Arne Dr Arne Geschke | Postdoctoral Fellow Integrated Sustainability Analysis School of Physics | Faculty of Science THE UNIVERSITY OF SYDNEY Rm 501, School of Physics A28 | Physics Road | The University of Sydney | NSW | 2006 T +61 2 9036 7505 | F +61 2 9351 7726 E arne.geschke@sydney.edu.au | W http://www.isa.org.usyd.edu.au "
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "sydney.edu.au",
                                    "name": "SYD-IndEcologyVL",
                                    "usagePatterns": "Large data sets from a medium group of users which is growing on a   weekly basis, ie, two new PHDs in May - June.   ",
                                    "useCase": "The team requires a storage allocation of 5tb to enable testing and  > work to continue as the existing 480gb has been consumed and it is  > estimated that at least 3tb will be in use by go-live December 2013. >  > Hi Neal > We&#39;re hitting constant disk-space problems - this problem has now  > become urgent - we need more space. > Thanks > Manfred > [cid:E0E9A103-BEE5-4644-9E87-BDD1D77D24A7] > Read our article in Nature: > http://www.nature.com/nature/journal/v486/n7401/full/nature11145.html > Begin forwarded message: > From: Arne Geschke &gt; > Subject: disc full > Date: May 31, 2013 6:50:12 AM GMT+10:00 > To: Manfred Lenzen &gt;, Arunima Malik &gt;, Daniel Moran &gt;, Joe  > Lane &gt; Hi all, I&#39;ve changed the upload code to adjust the  > rights of the uploaded file so that everybody can now delete their  > work directory. So you shouldn&#39;t need my help anymore to delete it. > I&#39;ve noticed that the disc is - again - completely full. Dan has  > got one job that is currently processing on scarlett, I don&#39;t know  > what will happen when that hits the upload queue. I can stop the  > uploader to avoid any problems. In that case, the data would remain on  > scarlett until the uploader is restarted. Dan, if you like, you can  > keep that data of this run on scarlett if that helps. > Please advise/delete data from your directory. > Thanks, > Arne > Dr Arne Geschke | Postdoctoral Fellow > Integrated Sustainability Analysis > School of Physics | Faculty of Science THE UNIVERSITY OF SYDNEY Rm  > 501, School of Physics A28 | Physics Road | The University of Sydney |  > NSW | 2006 T +61 2 9036 7505 | F +61 2 9351 7726 E  > arne.geschke@sydney.edu.au | W http://www.isa.org.usyd.edu.au >  >  > The usage pattern is: > Large data sets from a medium group of users which is growing on a  > weekly basis, ie, two new PHDs in May - June. >  > And the geographic requirements are: > Can be any available node, however, proximity to Massive would be  > preferred. >  > The tenant has specified the breakdown of their Fields Of Research as: >  > 0103 NUMERICAL AND COMPUTATIONAL MATHEMATICS (30%) >  >  > 0502 ENVIRONMENTAL SCIENCE AND MANAGEMENT (40%) >  >  > 0806 INFORMATION SYSTEMS (30%) >  >  > Yours, > the rcDashBoard. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Soils2Sats",
                                    "usagePatterns": "As per current",
                                    "useCase": "Migration and consolidation of S2S infrastructure - currently split across CSIRO and TERN run NeCTAR tenancys Ive put in an Allocation Request for 2 extra instances for this tenancy. What we would like is a Small VM for the s2sDev-Portal and a Medium VM for the s2sProd-Portal added to the existing Soils2Sats project. This is so that we can consolidate the whole Soils2Satellites project infrastructure under the one tenancy. Currently the infrastructure is split across an ALA managed tenancy for our web portal and our TERN managed tenancy for the services and db. However the Allocation Request form doesnt really let me explain my request very well  is there any other channel I can use to clarify this request?"
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.6,
                                    "institution": "csiro.au",
                                    "name": "IMOS AODAAC",
                                    "usagePatterns": "The system needs to be tested distributed across two VMs. We're using the necTAR RC systems for development and testing for the production instance hosted by IMOS. We expect episodic use by 3 developers, interspersed with longer periods of routine operation with a light ongoing testing load. The small persistent storage request is for the databases and some (read-only) test files.  Ephemeral storage ought be sufficient for the transient results of the testing envisaged.  Load testing will occasionally be heavy but relatively rare.",
                                    "useCase": "We are developing a system for extracting and aggregating subsets of large gridded data sets served by distributed OPeNDAP servers. The V1 system sits behind the IMOS Portal (See http://espace.library.uq.edu.au/view/UQ:155380 and p5 of http://imos.org.au/fileadmin/user_upload/shared/AODN/AODN_Newsletter_Nov2012.pdf). Development is ongoing, with V2 system due for transfer to IMOS in late 2013.  We need to test distributed mode of operation, with db and system population/query services on one node, with subset and aggregation work on another. ."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "PLEA",
                                    "usagePatterns": "The project comprises a web server for data collection and analysis. Volumes or RDSI storage allocation will be used for a relational database. Object Store and RDSI storage will be used for image (photo and video) data storage.  ",
                                    "useCase": "Off the coast of Point Lookout (Stradbroke Island, Queensland) lie a number of rocky reef systems that support and attract a diverse range of marine flora and fauna, as well as recreational and commercial activities such as fishing and SCUBA diving. These activities along with environmental impacts such as floods may be affecting the ecological health of the area. This study will assess the health of these systems in comparison to a similar study carried out in 2001. The aim of the project is to repeat the 2001 ecological assessment of the Point Lookout (North Stradbroke Island) marine environment using a community based, cost effective, scientifically sound survey program. At each site transect surveys will be conducted to assess the populations of fish, invertebrates, coral state, bottom type and impacts. Five transect sites will be used: two at Flat Rock, two at Shag Rock and one at Manta Bommie. Detailed maps of Shag Rock and Manta Bommie will be created. Photos and video will be acquired to document the species found at these locations. The Principal Investigator, who also holds the QLD RDSI allocation is Dr Chris Roelfsema (c.roelfsema@uq.edu.au).  "
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN Ecoinformatics (Australian Ecological Knowledge and Observation System)",
                                    "usagePatterns": "",
                                    "useCase": "Ingestion and federation of data from various government agencies, universities and NGOs; processing of textual, spatial and imagery data; databases; software development; web applications. Also interacting and exchanging data with other systems such as SHaRED (NeCTAR), Soils to Satellites (ANDS and ALA) and the TERN Multi-Scale Plot Network (MSPN), however these are separate projects acquiring their own allocations. Some VMs are envisaged as offering long-lived services whereas some will be temporary and used for development, testing or short-term processing. "
                                },
                                {
                                    "coreQuota": 4.5,
                                    "instanceQuota": 4.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN Ecoinformatics (Australian Ecological Knowledge and Observation System)",
                                    "usagePatterns": "",
                                    "useCase": "Ingestion and federation of data from various government agencies, universities and NGOs; processing of textual, spatial and imagery data; databases; software development; web applications. Also interacting and exchanging data with other systems such as SHaRED (NeCTAR), Soils to Satellites (ANDS and ALA) and the TERN Multi-Scale Plot Network (MSPN), however these are separate projects acquiring their own allocations. Some VMs are envisaged as offering long-lived services whereas some will be temporary and used for development, testing or short-term processing. "
                                }
                            ],
                            "name": "0502"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 10.0,
                                    "institution": "intersect.org.au",
                                    "name": "Intersect_Snap",
                                    "usagePatterns": "",
                                    "useCase": "We are currently working on the Snap Deploy project which is Nectar funded. We would like additional resources to allow us to test our application with the Nectar Cloud. I cannot select a FOR of research as our application is to allow the automatic deployments of multiple projects from various disciplines. "
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "unsw.edu.au",
                                    "name": "A web portal to study the spread of debris in the global ocean",
                                    "usagePatterns": "The online web portal does not need high availability. There is also no need for persistency and/or backups. We do, however, hope to be able to use more than two cores. More cores (ideally four to eight) allows us to speed up the key component of the portal: the matrix multiplication that gives us the evolution of tracer in time, by using Atlas and OpenMP frameworks. We do not, however, need the memory that comes with each core. We don't expect high traffic to the web portal under normal operation, but envision that there could be peaks due to high exposure related to events in the media (such as another Fukushima). At these times, and particularly when media embeds the tool on their web sites, we expect that it will be very popular and provide high exposure.",
                                    "useCase": "We have recently been funded by the ARC Centre of Excellence on Climate System Science to develop a web portal where users can interact with animations of the spreading of tracer through the ocean. We envision the web portal to develop into the to-go-to place for people interested in ocean circulation pathways. The portal will be used by both researchers and members of the general public who want to study the pathways of marine debris and contaminants, as well as learn about the role of the ocean in global climate. We hope that we can use the Research Cloud at NeCTAR to host this high profile portal.  After the Fukushima nuclear disaster in March 2011, one of the most pressing issues of general public concern was where the nuclear waste would end up. Similar reactions followed the Deep Water Horizon oil spill in the Gulf of Mexico in April 2010 and (on a more local scale) the beaching of the container ship Rena in New Zealand in October 2011. Our online web portal can provide the general public with answers to these questions. Recently, we have developed a novel way to study spreading of tracer on the surface of the global ocean. In this method, which relies on iterating a simple vector-matrix multiplication, data from observed buoys in the global ocean are used to calculate the spreading of tracer from any point in the ocean over time. A manuscript detailing this method and applying it to the global garbage patches has been published in the A* peer-reviewed journal Environmental Research Letters. A video abstract of the method is available from http://www.youtube.com/watch?v=M4UK9Yt6A-s. The manuscript has gotten extensive media attention, both nationally and internationally, and the video abstract has been watched over 8,000 times in two weeks. The method to calculate the evolution of tracer in the ocean is computationally extremely efficient, which opens up the possibility to let the general public directly interact with it. Which is why the ARC Centre of Excellence on Climate System Science has funded its development and we are now seeking NeCTAR help for the hosting. Further questions on the research of the portal can be directed to Erik van Sebille (E.vanSebille@unsw.eud.au), questions on the technical side can be directed to David Fuchs (D.Fuchs@unsw.edu.au)"
                                },
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 14.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM MUtopia",
                                    "usagePatterns": "Currently, our system is hosting moderately large precinct models (thousands of buildings with hundreds of parameters each), plus some large assets (such as uploaded 3D models). The scale and number of these projects is expected to grow, potentially substantially, over the next year as we acquire more clients to work with and develop more sophisticated models. However CPU usage is typically light, with the occasional demanding simulation. We would have a primary instance as a production server, and a secondary, identical server for development staging. The final server would be split between hosting our development tools and smaller, temporary demo instances in VMs. Object storage would be utilised to store assets, uploaded/downloadable by the user, as well as for redundant database backup.",
                                    "useCase": "The MUtopia team is developing a large web application for the modelling, simulation and visualisation of urban development projects. The servers will host the web application and its databases for access by researchers and clients, public and private, current and future. They should improve performance over our current arrangement, and provide greater redundancy and data security. The project will be ongoing beyond this year, but this should be sufficient for that time. I'm not sure how to estimate core hours for a 24/7 web server. See http://mutopia.unimelb.edu.au/ for more information."
                                },
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 0.9,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM MUtopia",
                                    "usagePatterns": "Currently, our system is hosting moderately large precinct models (thousands of buildings with hundreds of parameters each), plus some large assets (such as uploaded 3D models). The scale and number of these projects is expected to grow, potentially substantially, over the next year as we acquire more clients to work with and develop more sophisticated models. However CPU usage is typically light, with the occasional demanding simulation. We would have a primary instance as a production server, and a secondary, identical server for development staging. The final server would be split between hosting our development tools and smaller, temporary demo instances in VMs. Object storage would be utilised to store assets, uploaded/downloadable by the user, as well as for redundant database backup.",
                                    "useCase": "The MUtopia team is developing a large web application for the modelling, simulation and visualisation of urban development projects. The servers will host the web application and its databases for access by researchers and clients, public and private, current and future. They should improve performance over our current arrangement, and provide greater redundancy and data security. The project will be ongoing beyond this year, but this should be sufficient for that time. I'm not sure how to estimate core hours for a 24/7 web server. See http://mutopia.unimelb.edu.au/ for more information."
                                }
                            ],
                            "name": "050204"
                        }
                    ],
                    "name": "0502"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_Sense-T",
                                    "usagePatterns": "",
                                    "useCase": "This is for the Sense-T project: www.sense-t.org.au It will be used to host Infosphere Streams so we can process incoming sensor data in real-time."
                                },
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "utas.edu.au",
                                    "name": "UTas-Sense-T",
                                    "usagePatterns": "",
                                    "useCase": "This is for the Sense-T project: www.sense-t.org.au It will be used to host Infosphere Streams so we can process incoming sensor data in real-time."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uq.edu.au",
                                    "name": "Marxan conservation planning software development",
                                    "usagePatterns": "We're using RDSI storage so don't require additional object storage or storage volumes. We'd like to have the Marxan RDSI allocation statically linked to the primary Marxan VM's that are online continuously: marxan.net and roflcraft.net (being our production server, and our slave/experimental server). We'll have a large number of users and a large number of datasets from very small to very large. We anticipate the production system will go online early in 2014, and the computation performed by the system will incrementally ramp up between now and then. We'd like to make an additional request for increased computation resources when the production system is online. With the current request, we'd like to use about half the cores for continuously operating VM's, and we'd like to use the other half of the cores for cloud bursting with the Nimrod API. We're working with Minh Huynh at UQ RCC to use Nimrod cloud bursting API. The cloud bursting VM's will be used in this way: - At run time in the production system, Nimrod will spawn a large number of small VM's, - Once the computational tasks are completed by the large number of small VM's, Nimrod will terminate the large number of small VM instances, and return the computational resources to the NeCTAR pool, - This pattern of spawning a large number of small VM's and terminating the VM's when computation is complete will occur periodically on demand. The nature of the new calibration algorithms we're developing require more computational resources than we've used in the past for Marxan. This gives the software new and enhanced functionality at the expense of requiring more computational resources on demand for short periods of time.",
                                    "useCase": "I want to use the research cloud to assist me with cloud migration of the Marxan software. Marxan is the most widely used decision support software for conservation planning globally. It is used in over 100 countries to build marine and terrestrial conservation plans and national parks systems.  It dominates the world market for conservation land-use planning and new extensions are making it even more popular.   Currently, the only supported deployment model of the software is through download and install by users. We have initiated an internal project to migrate the software to the QCIF research cloud (Q-Cloud) to enhance the utility of the software for ourselves and our researcher collaborators. The cloud migration will improve the ability for us to inform conservation planning decisions in Australia and globally and increase its utility worldwide. Among our global users, there is a large number of government, educational and research organisations in Queensland who use Marxan to support their core business, including DEHP, DNRM, DNPRSR, QPWS, UQ, QUT, JCU, CQU, Griffith uni, and Bond uni. There&#39;s also a large number of government, research, and educational users nationwide. These users will benefit through improved delivery of decision support, enhanced scalability of computation, improved software performance, and better ease of use. The software will be made available to select researchers in Queensland and around the world. Their ability to conduct large scale Marxan analysis will be enhanced, which will boost research, publications, citations, and impact factor, and improve the ability to influence conservation decisions globally. This is part of a broader initiative to globalise the impact of the University of Queenslands conservation science.  Marxan is likely to be one of the major impact stories when the ARC roll out impact assessment as part of the way the federal government fund universities. I have several test servers currently in use, and am conducting research and development that require larger scale computational resources. The architecture I'm developing uses R-Studio server as the front end for highly optimised C code. www.uq.edu.au/marxan"
                                }
                            ],
                            "name": "059999"
                        }
                    ],
                    "name": "0599"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF eSpaces",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate a custom webserve JCU's staff use to Qcloud, enabling others to use the same."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CCDAM",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the primary digital asset management system used by JCU to enable others to do the same. "
                                },
                                {
                                    "coreQuota": 13.0,
                                    "instanceQuota": 1.5,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution. The mammal occurrence related to climate change will visualise 100 mammal occurrence model output data for about 60 years "
                                },
                                {
                                    "coreQuota": 5.0,
                                    "instanceQuota": 1.5,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution."
                                },
                                {
                                    "coreQuota": 5.0,
                                    "instanceQuota": 5.0,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution."
                                },
                                {
                                    "coreQuota": 5.0,
                                    "instanceQuota": 5.0,
                                    "institution": "uq.edu.au",
                                    "name": "TERN continental scale model output ",
                                    "usagePatterns": "The project will have large users to search for data available via TERN and visualize the model output. ",
                                    "useCase": "The Virtual machine will be used as a catalog for all the data collections published by Terrestrial Ecosystem Research network, enable the publication of the continental scale gridded models and mammal occurrence distribution. The mammal occurrence related to climate change will visualise 100 mammal occurrence model output data for about 60 years "
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 0.9,
                                    "institution": "monash.edu",
                                    "name": " OzFlux Data Portal",
                                    "usagePatterns": "144 users and 2G datasets growth /year",
                                    "useCase": " OzFlux is part of the Australian Terrestrial Ecosystem Research Network (TERN). The OzFlux network consists of nearly 30 flux towers in Australia and New Zealand, many of which are also members of the Australian Supersite Network (ASN). OzFlux is also a member of the global FluxNet community of over 500 flux stations that is designed to provide continuous, long-term micrometeorological measurements to monitor the state of ecosystems globally. The OzFlux purpose is: to understand mechanisms controlling exchanges of carbon, water vapour and energy between terrestrial ecosystems and the atmosphere over a range of time and space scales to provide data on carbon and water balances of key ecosystems for model testing to provide information to validate estimates of net primary productivity, evaporation, and energy absorption using remotely sensed radiance data to provide data to validate new developments in micrometeorological theory for fluxes and air flows in complex terrain to provide high precision CO2 concentrations measurements (at Cape Grim) for use in regional, continental and global atmospheric inverse studies of the carbon cycle. Data from the OzFlux network of flux towers is available from this portal.  The data is organised into collections with each collection representing at least one site. The virtual resources will be used to host this web data portal."
                                }
                            ],
                            "name": "05"
                        }
                    ],
                    "name": "05"
                }
            ],
            "name": "05"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 30.0,
                                    "institution": "vu.edu.au",
                                    "name": "VU eResearch",
                                    "usagePatterns": "Many would be cloud hosting of web portals, collaborations sites, datasets, databases and collections.  Some we expect to be servers for data collection around specific projects or events, eg. from mobile phone data.  A few may use instances for computation or analysis tools.",
                                    "useCase": "We'd like a development allocation for central eResearch support at VU.  We have 25 identified potential research instances across a range of research groups and would use the allocation to develop these.  Potential projects include: engagement with HUNI project (VU node); SportVL early establishment; Footscary Community Arts Centre project;  IPE project collaborative site; Mobile and location data collection; Sensor network data collection and development; Online historical collections; Community Identity Displacement Research Network; Football heritage and culture; History of ideas;  Health Reform and CaseMix data; Logistics Modeling Lab; Economic modelling compute on demand; In Teachers Hands education study. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "MONITOR-1",
                                    "usagePatterns": "",
                                    "useCase": "Monitor for testing "
                                },
                                {
                                    "coreQuota": 30.0,
                                    "instanceQuota": 30.0,
                                    "institution": "vu.edu.au",
                                    "name": "VU-eResearch",
                                    "usagePatterns": "Many would be cloud hosting of web portals, collaborations sites, datasets, databases and collections.  Some we expect to be servers for data collection around specific projects or events, eg. from mobile phone data.  A few may use instances for computation or analysis tools.",
                                    "useCase": "We'd like a development allocation for central eResearch support at VU.  We have 25 identified potential research instances across a range of research groups and would use the allocation to develop these.  Potential projects include: engagement with HUNI project (VU node); SportVL early establishment; Footscary Community Arts Centre project;  IPE project collaborative site; Mobile and location data collection; Sensor network data collection and development; Online historical collections; Community Identity Displacement Research Network; Football heritage and culture; History of ideas;  Health Reform and CaseMix data; Logistics Modeling Lab; Economic modelling compute on demand; In Teachers Hands education study. "
                                }
                            ],
                            "name": "10"
                        }
                    ],
                    "name": "10"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "monash.edu",
                                    "name": "Electronic and Mechanical Properties of Next-Generation Nanoscale Materials",
                                    "usagePatterns": "No of users: 1-2  Data sets: small ",
                                    "useCase": "We, at the Computational Materials Laboratory at Monash University (http://users.monash.edu/~nikhilm/home.html), are a large group working on structure and electronic, chemical, and mechanical properties of nanoscale materials. We employ tools a wide variety of modelling tools such as first principles simulations, molecular dynamics and phase-field simulations.  We currently have many ongoing projects and the needs for different projects vary in terms of number of cores, time and memory. As advised by Ms Lennie Au, here we request a  short testing time on NECTAR to benchmark our needs against available resources. This will allow us to precisely identify the sort of calculations we can do using NECTAR in the most efficient manner. Once benchmarking is done, I plan to submit an appropriately large allocation request.  It should also be noted that we do not intend to use NECTAR resources for calculations that can be efficiently done elsewhere (for example, NCI). Rather, we our objective is to identify NECTAR resources that can complement other resources. This motivates this allocation request for benchmarking purposes. "
                                }
                            ],
                            "name": "100708"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "usc.edu.au",
                                    "name": "Joanne Macdonald",
                                    "usagePatterns": "",
                                    "useCase": "Run python scripts that call information from public databases, then bioinformatically assess for matches with query inputs. The process is CPU intensive but on site tests have shown it have a very small RAM and disk utilisation. This will be an ongoing project with new processes being run frequently"
                                },
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "usc.edu.au",
                                    "name": "USC-FIoV:",
                                    "usagePatterns": "",
                                    "useCase": "* project description, significance and expected outcomes We are developing next-generation diagnostic assays that use molecular circuitry to enable field-identification of viruses. The devices are constructed via nucleic acid networks, designed as a logic circuits, to link viral genetic signatures to an output text display of diagnoses. Viruses are a moving target for diagnostics, not just because of their infection time-course, but through their microevolution that helps them evade host defences. We have developed bioinformatics software for maximum capture of evolving viruses, which is able to identify regions of similarity in highly related sequences that are divergent from non-related sequences. The specificity of the software is controlled by user-inputs, and can be used to determine consensus sequences at the level of both species and subtype/strain as required.  Significance: Diagnostic technologies for individual detection of potential disease biomarkers are well established. Significant developments in recent years have occurred in the fields of (i) multiplexing, and (ii) rapid/low-resource detection. (i) Multiplex developments include high- and low-density spatial arrangements such as microarrays, or the use of multiple tags such as mass-tagged PCR, electrospray ionisation, or luminex beads. Our molecular circuits use a novel adaptation of spatial arrays to provide text outputs for autonomous diagnosis of multiple viral signature sequences.  (ii) The most striking low resource development in diagnostics has been the lateral flow dipstick, which operates similarly to an at-home pregnancy test. The sensitivity of the standard device is limited, however, the subsequently reported nucleic acid lateral flow device utilizes PCR-based sensitivity with rapid duplex detection. Combined with isothermal amplification, this set-up holds the most promise for rapid end-point detection in low resource settings. Variations include detection by fluorescence, which improves sensitivity at the expense of requiring portable read-out devices, extension to low-density micro-arrays, or portable lab-on-a-chip devices that combine sample preparation, amplification, and detection in a single user-friendly device. Our proposal moves beyond these singleplex or duplex field-amenable endpoint devices by providing a method to detect multiple viral disease markers and also quantifying their amount, yet still in a field-amenable assay format. Outcome: A robust molecular diagnostic technology able to detect and quanitify multiple viruses, undergoing reasonable levels of microevolution, in a single field-amenable device. * Impact and innovation of the research Our circuitry is purely driven by molecules: no electronics are incorporated, and no electronic power is required for operation. By replacing electronic circuitry with molecular circuitry, we reduce electrical requirements and contribute to reduction in carbon footprints. In addition, previous field-amenable diagnostic assays have focused on the detection of only a few agents in a single device. Our use of molecular automata allows the development of a single field-based device that can identify and analyse the significance of multiple biomarker presence. Finally, our innovative custom bioinformatics software enables rapid and automated development of devices that encompasses viral microevolution, enabling comprehensive viral detection. * National benefit of the research National research priority: Safeguarding Australia  (i) The ability to detect viral infections, and quantitatively assess viral loads, all onsite in a clinic, field, or airport, would have a significant impact on disease treatment and epidemiology, protecting Australia from invasive diseases and pests. Infectious but asymptomatic virus carriers, as well as those in the early stage of infection that do not yet show disease, could be screened and isolated. This has major ramifications in airports, cruise ships, animal export industries, and general border control, and could result in major economic gains. Additionally, while this research is entirely focused on viral infections, the generic principles of our integrated diagnostic platforms are entirely amenable to other infections, such as bacteria and parasites. (ii) A hand-held device that quickly analyses for biosecurity threats and does not require electricity to operate would be a transformational defence technology, allowing soldiers to assess and contain bioterrorism attacks before extensive disease spread.   Targeted research areas: This proposal directly addresses several targeted research areas, including the development of bioinformatics methodology for rapidly evolving viruses and the use of nucleic acid computing networks to provide an alternative green technology that replace electronics and wires with molecules. Our nucleic acid networks will perform autonomous pattern recognition and data mining for assessment of disease state and diagnosis of viral infections. Additionally, our proposal indirectly addresses indigenous health and wellbeing, in the sense that any point-of-care inexpensive rapid assay for the detection of viruses of national significance will provide an enabling technology for communities in low-resource settings to take ownership of their personal and community human and animal health. Rapid on-site diagnosis allows for faster decisions on treatment and isolation, which can reduce morbidity and disease spread. "
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "uq.edu.au",
                                    "name": "NBCF Breast Cancer Project",
                                    "usagePatterns": "The resources will mainly be used for analysis of large genomic datasets.  This typically requires unique reference datafiles that are frequently accessed by our bioinformatic tools; these reference datafile sets can frequently be in excess of 20GB each (e.g., 3 different human genome reference sets, each of which ~20GB each).  Each genome reference set will require persistent storage. The requirement for object storage relates to archiving our patient sequencing datasets, for ongoing access by our national research term.  We currently have an application with QCloud for allocation of storage for this project. ",
                                    "useCase": "We have been funded by the NBCF for 5 years to pursue next-generation diagnostic testing and screening on clinical tumour biopsies from women diagnosed with breast cancer.  The next phase of our project requires extensive next-generation sequencing analysis of these genetic and epigenetic changes, and this analysis will take place over the length of our 5 year project grant.  The data generated will be used collaboratively by multiple users across Australia, with the goal of improving patient outcomes for women currently suffering from disease."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "NBCF Breast Cancer Project",
                                    "usagePatterns": "The resources will mainly be used for analysis of large genomic datasets.  This typically requires unique reference datafiles that are frequently accessed by our bioinformatic tools; these reference datafile sets can frequently be in excess of 20GB each (e.g., 3 different human genome reference sets, each of which ~20GB each).  Each genome reference set will require persistent storage. The requirement for object storage relates to archiving our patient sequencing datasets, for ongoing access by our national research term.  We currently have an application with QCloud for allocation of storage for this project. ",
                                    "useCase": "We have been funded by the NBCF for 5 years to pursue next-generation diagnostic testing and screening on clinical tumour biopsies from women diagnosed with breast cancer.  The next phase of our project requires extensive next-generation sequencing analysis of these genetic and epigenetic changes, and this analysis will take place over the length of our 5 year project grant.  The data generated will be used collaboratively by multiple users across Australia, with the goal of improving patient outcomes for women currently suffering from disease."
                                }
                            ],
                            "name": "100703"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 1.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.3,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.3,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.3,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Evanescent_Scattering",
                                    "usagePatterns": "A single user with relatively small datasets; I will be using a direct solver requiring a large amount of memory and a more-substantial CPU than we have on-hand in the group. The expanded allocation request is due to memory issues encountered when attempting to solve a moderate scale-up of an initial series of test cases.",
                                    "useCase": "Modelling evanescent scattering by a dielectric sphere via FEA using COMSOL Mutiphysics 4.3b. This is in order to compare results against analytical solutions previously computed using Mathematica/MATLAB, and to resolve uncertainty regarding the far-field scattering behaviour of analytically intractable cases."
                                }
                            ],
                            "name": "100712"
                        }
                    ],
                    "name": "1007"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "sydney.edu.au",
                                    "name": "Clonal Diversity",
                                    "usagePatterns": "Large data sets with a small number of users.",
                                    "useCase": "This project aims to investigate the feasibility of next generation sequencing for evaluating clonal diversity following gene transfer to haematopoietic stem cells, using both barcoded lentiviral gene transfer vectors and analysis of integration sites. The cloud instance will be used to process and analyse large datasets using filtering, clustering and alignment techniques. "
                                }
                            ],
                            "name": "100401"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_HVPA",
                                    "usagePatterns": "Sequencing data pipeline and database",
                                    "useCase": "Genome variation file processing and database"
                                },
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 14.4,
                                    "institution": "usc.edu.au",
                                    "name": "Joanne Macdonald",
                                    "usagePatterns": "",
                                    "useCase": "Run python scripts that call information from public databases, then bioinformatically assess for matches with query inputs. The process is CPU intensive but on site tests have shown it have a very small RAM and disk utilisation. This will be an ongoing project with new processes being run frequently"
                                },
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 14.4,
                                    "institution": "usc.edu.au",
                                    "name": "USC-FIoV:",
                                    "usagePatterns": "",
                                    "useCase": "* project description, significance and expected outcomes We are developing next-generation diagnostic assays that use molecular circuitry to enable field-identification of viruses. The devices are constructed via nucleic acid networks, designed as a logic circuits, to link viral genetic signatures to an output text display of diagnoses. Viruses are a moving target for diagnostics, not just because of their infection time-course, but through their microevolution that helps them evade host defences. We have developed bioinformatics software for maximum capture of evolving viruses, which is able to identify regions of similarity in highly related sequences that are divergent from non-related sequences. The specificity of the software is controlled by user-inputs, and can be used to determine consensus sequences at the level of both species and subtype/strain as required.  Significance: Diagnostic technologies for individual detection of potential disease biomarkers are well established. Significant developments in recent years have occurred in the fields of (i) multiplexing, and (ii) rapid/low-resource detection. (i) Multiplex developments include high- and low-density spatial arrangements such as microarrays, or the use of multiple tags such as mass-tagged PCR, electrospray ionisation, or luminex beads. Our molecular circuits use a novel adaptation of spatial arrays to provide text outputs for autonomous diagnosis of multiple viral signature sequences.  (ii) The most striking low resource development in diagnostics has been the lateral flow dipstick, which operates similarly to an at-home pregnancy test. The sensitivity of the standard device is limited, however, the subsequently reported nucleic acid lateral flow device utilizes PCR-based sensitivity with rapid duplex detection. Combined with isothermal amplification, this set-up holds the most promise for rapid end-point detection in low resource settings. Variations include detection by fluorescence, which improves sensitivity at the expense of requiring portable read-out devices, extension to low-density micro-arrays, or portable lab-on-a-chip devices that combine sample preparation, amplification, and detection in a single user-friendly device. Our proposal moves beyond these singleplex or duplex field-amenable endpoint devices by providing a method to detect multiple viral disease markers and also quantifying their amount, yet still in a field-amenable assay format. Outcome: A robust molecular diagnostic technology able to detect and quanitify multiple viruses, undergoing reasonable levels of microevolution, in a single field-amenable device. * Impact and innovation of the research Our circuitry is purely driven by molecules: no electronics are incorporated, and no electronic power is required for operation. By replacing electronic circuitry with molecular circuitry, we reduce electrical requirements and contribute to reduction in carbon footprints. In addition, previous field-amenable diagnostic assays have focused on the detection of only a few agents in a single device. Our use of molecular automata allows the development of a single field-based device that can identify and analyse the significance of multiple biomarker presence. Finally, our innovative custom bioinformatics software enables rapid and automated development of devices that encompasses viral microevolution, enabling comprehensive viral detection. * National benefit of the research National research priority: Safeguarding Australia  (i) The ability to detect viral infections, and quantitatively assess viral loads, all onsite in a clinic, field, or airport, would have a significant impact on disease treatment and epidemiology, protecting Australia from invasive diseases and pests. Infectious but asymptomatic virus carriers, as well as those in the early stage of infection that do not yet show disease, could be screened and isolated. This has major ramifications in airports, cruise ships, animal export industries, and general border control, and could result in major economic gains. Additionally, while this research is entirely focused on viral infections, the generic principles of our integrated diagnostic platforms are entirely amenable to other infections, such as bacteria and parasites. (ii) A hand-held device that quickly analyses for biosecurity threats and does not require electricity to operate would be a transformational defence technology, allowing soldiers to assess and contain bioterrorism attacks before extensive disease spread.   Targeted research areas: This proposal directly addresses several targeted research areas, including the development of bioinformatics methodology for rapidly evolving viruses and the use of nucleic acid computing networks to provide an alternative green technology that replace electronics and wires with molecules. Our nucleic acid networks will perform autonomous pattern recognition and data mining for assessment of disease state and diagnosis of viral infections. Additionally, our proposal indirectly addresses indigenous health and wellbeing, in the sense that any point-of-care inexpensive rapid assay for the detection of viruses of national significance will provide an enabling technology for communities in low-resource settings to take ownership of their personal and community human and animal health. Rapid on-site diagnosis allows for faster decisions on treatment and isolation, which can reduce morbidity and disease spread. "
                                }
                            ],
                            "name": "100402"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.6,
                                    "instanceQuota": 3.6,
                                    "institution": "uts.edu.au",
                                    "name": "Intersect_Microbial_community_analysis",
                                    "usagePatterns": "3 users, data sets 10-30GB each, mix of I/O and CPU intensive work.",
                                    "useCase": "Analyzing microbial communities using 16S amplicon data generated by Illumina MiSeq instruments. Will use QIIME VM. "
                                },
                                {
                                    "coreQuota": 3.6,
                                    "instanceQuota": 0.9,
                                    "institution": "uts.edu.au",
                                    "name": "Intersect_Microbial_community_analysis",
                                    "usagePatterns": "3 users, data sets 10-30GB each, mix of I/O and CPU intensive work.",
                                    "useCase": "Analyzing microbial communities using 16S amplicon data generated by Illumina MiSeq instruments. Will use QIIME VM. "
                                }
                            ],
                            "name": "100499"
                        }
                    ],
                    "name": "1004"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_genes4all_clients",
                                    "usagePatterns": "Each instance will be a web-server that will serve data derived from a database server located elsewhere in the cloud (I had to file a separate allocation request).",
                                    "useCase": "Modern genomics has enabled a large number of researchers to produce vast amounts of data for their favourite organism. This is especially true for species of industrial and/or ecological importance (instead of the classic lab models: mouse, human, fruitfly etc). The bottleneck is not the processing of the data (dealt via HPC) but the dissemination and visualization in a standardized, pain-free format. Further, curation and interactivity allows the web users to edit and curate the metadata in a crowdsourcing approach. Our work has produced such a system, written in PHP (Drupal), JavaScript (ExtJS and JQuery) and some perl code. We are now seeking to deploy it as domain-specific web-servers (e.g. only for specific species or projects). Each instance will host a webserver that will serve domain-specific data in a common GUI. Users will be using port 80 to access the interface.  Administrators (one or two individuals) will be responsible for maintaining the web server but we don't expect frequent updates. Data will be derived from a database server located elsewhere on the NECTaR cloud and/or within CSIRO. An example can be seen at http://insectacentral.org. Previous work has been cited 38 times since 2008 (DOI: 10.1093/nar/gkm853). I'm more than happy to provide more information if this is required."
                                }
                            ],
                            "name": "100201"
                        }
                    ],
                    "name": "1002"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "anu.edu.au",
                                    "name": "ANU-RDSI-Netcomms",
                                    "usagePatterns": "Performance software such as iPerf , OWAMP, Smokeping etc for testing domestic and international connectivity.",
                                    "useCase": "ANU Netcomms maintains and supports both the ANU and NCI networking infrastructure and require a VM for connectivity and occasional performance testing."
                                }
                            ],
                            "name": "100605"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "unsw.edu.au",
                                    "name": "ARGUS FPGA Design",
                                    "usagePatterns": "Small number of users with relatively small data sets but long processing time. FPGA place/routing can take from 30mins to 10+ hours.",
                                    "useCase": "Remote building platform for the ARGUS project to enable other academics to queue FPGA tests/benchmarks which will be automatically built, simulated and the results returned."
                                }
                            ],
                            "name": "100699"
                        }
                    ],
                    "name": "1006"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 21.0,
                                    "instanceQuota": 21.0,
                                    "institution": "cqu.edu.au",
                                    "name": "CQU Sandbox",
                                    "usagePatterns": "Mostly a test bed - so usage will vary",
                                    "useCase": "This VM will be used as a \"test\" bed for CQUniversity. We have a Sys-admin assigned to look after CQU research cloud resources and therefore, need to test before assisting our researcher needs."
                                }
                            ],
                            "name": "1099"
                        }
                    ],
                    "name": "1099"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "TeachingCloudComputing-Team1",
                                    "usagePatterns": "These will be used for hosting CouchDB-based twitter data sets.",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team2",
                                    "usagePatterns": "Twitter data analytics",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team3",
                                    "usagePatterns": "Twitter data analytics",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team4",
                                    "usagePatterns": "Twitter data analytics",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team5",
                                    "usagePatterns": "Twitter data analytics",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team6",
                                    "usagePatterns": "Twitter data analytics",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team7",
                                    "usagePatterns": "Twitter data analytics",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_TeachingCloudComputing-Team8",
                                    "usagePatterns": "Twitter data analytics - in all of these requests I expect a team of 4-6 users to be harvesting twitter data, pushing it into CouchDB and doing a range of analytics with it (using MapReduce etc). I do not expect these projects to generate LARGE data sets and they won't be developing services for others to access etc.",
                                    "useCase": "This will be used for a teaching assignment in Cloud Computing at UniMelb."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "TeachingCloudComputing-Team9",
                                    "usagePatterns": "They will be harvesting, analysing twitter data",
                                    "useCase": "Teaching cloud computing - more students have registered so need more VMs please."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "TeachingCloudComputing-Team10",
                                    "usagePatterns": "they will be harvesting, analysing twitter data",
                                    "useCase": "teaching cloud computing and have more students than originally planned so need some more VMs"
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "TeachingCloudComputing-Team9",
                                    "usagePatterns": "They will be harvesting, analysing twitter data",
                                    "useCase": "Teaching cloud computing - more students have registered so need more VMs please."
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "TeachingCloudComputing-Team10",
                                    "usagePatterns": "they will be harvesting, analysing twitter data",
                                    "useCase": "teaching cloud computing and have more students than originally planned so need some more VMs"
                                }
                            ],
                            "name": "100599"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Twitter_keyword_track",
                                    "usagePatterns": "",
                                    "useCase": "to track the stream tweet from Twitter by twitter API. set up Hadoop on this service to do the sentiment analysis with the tweet data. It is the final year project for my coursework. My supervisor is Dr Aaron Harwood "
                                }
                            ],
                            "name": "100503"
                        }
                    ],
                    "name": "1005"
                }
            ],
            "name": "10"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "unsw.edu.au",
                                    "name": "FAIMS",
                                    "usagePatterns": "1 user (me) most of the time until we start really getting under weigh. The data sets will be large and varied, whatever we can beg from tDar and OpenContext and Heurist in the main. I'll be testing data extraction and import, so a decent pipe will be nice. But very low utilization expected most of the time based on research needs. Number of core hours unknown. Reqeuest help estimating. Number of instances: We have 8 core servers, each requiring at least a medium allocation (we've experienced processor overload on our smalls) running our website, issue tracker, wiki, authentication server, primary data repository, development data repository, primary mobile app server, and development app server. With our new grant, we also will need to spin up up to 15 2-core instances to support our researchers. I've increased our volume storage request to provide room for our mandated backup regimen. ",
                                    "useCase": "This is to support the NeCTAR funded FAIMS project. Our current VM is a local-tenancy test VM that cannot support sample instances of tDAR, openContext, Heurist, and the 8 other databases I need to investigate and experiment with.  We need a front-end tiny web-host for our blog and workshop management software. And either 1 machine or a set of machines to run the various archaeological warehouses to test with. We will also likely need one or more machines to serve as storage backends to the mobile tool we're developing."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "unsw.edu.au",
                                    "name": "FAIMS",
                                    "usagePatterns": "1 user (me) most of the time until we start really getting under weigh. The data sets will be large and varied, whatever we can beg from tDar and OpenContext and Heurist in the main. I'll be testing data extraction and import, so a decent pipe will be nice. But very low utilization expected most of the time based on research needs. Number of core hours unknown. Reqeuest help estimating. Number of instances unknown, also need help there. 2 to start with, 1 web facing with wordpress and ocs, and 1 research-facing that can run 8 different data warehouses.  Object storage size is highly variable, as a function of what archaeologist buy-in we get.",
                                    "useCase": "This is to support the NeCTAR funded FAIMS project. Our current VM is a local-tenancy test VM that cannot support sample instances of tDAR, openContext, Heurist, and the 8 other databases I need to investigate and experiment with.  We need a front-end tiny web-host for our blog and workshop management software. And either 1 machine or a set of machines to run the various archaeological warehouses to test with. We will also likely need one or more machines to serve as storage backends to the mobile tool we're developing."
                                }
                            ],
                            "name": "210199"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQ Online Archaeological Reference Collections",
                                    "usagePatterns": "This application will have very quite computational requirements, but will require running a database backed web application continuously for users to access. Due to the niche nature of the records there won't be a great number of users. The database itself will be small, but the related photos will require significantly more storage. It's intended to use the Nectar Object Store to hold them. Initially there won't be many, but that will grow to several high resolution photos for each of the several thousand records. ",
                                    "useCase": "The requested VM is for hosting a digital reference collection for use by archaeologists at UQ, and beyond. It will be made up of two sets of records, one of botanical records of different plant parts and one of molluscs/shells. Both will contain taxonomic information, detailed descriptions of the physical specimens housed at UQ, along with photos of the specimens. These reference collections will be used during archaeological research to help determine which types of plants and shells have been found at different locations. Many of the found specimens will be burnt or decayed, and so different descriptors are required than in more generic reference resources. The existing references are either not specific to the Australian region, don't provide the required level of detail, or are only available offline in books. The data and photos are being provided and updated by staff in the Archaeology division at UQ, with the project led by Andrew Fairbairn <a.fairbairn@uq.edu.au>. A test version of the database is available at http://archaeobotanyref-uat.qc.to/. The database has been developed by the eResearch group at UQ using the Django web framework and Postgresql database. Source code is available at https://github.com/omad/molluscs-reference.   "
                                }
                            ],
                            "name": "210102"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "German Missionaries",
                                    "usagePatterns": "small data set, small number of users",
                                    "useCase": "The contact history of indigenous people in Australia is strongly shaped by missions and reserves, the breaking up of families and removals of children from their parents. Missionaries played a prominent role in modelling and managing such regimes so that the history of missions is highly contested. This server will host a web CMS about German Missionaries and  gives detailed insight into their backgrounds, their aspirations and frustrations. It provides materials from private collections and overseas archives and frames these in their wider historical context. "
                                }
                            ],
                            "name": "210101"
                        }
                    ],
                    "name": "2101"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "qut.edu.au",
                                    "name": "QCIF_Paperminer",
                                    "usagePatterns": "",
                                    "useCase": "The objectives of this project and the PaperMiner portal are: Develop a small set of data mining and indexing techniques to meaningfully organise the National Library of Australia Trove Newspapers Online dataset for useful knowledge discovery; Store and manage the indexes and provide the linkages back to the NLA Trove api for record retrieval; Provide a web based portal through which registered users can perform queries against the indexes, parse results, view summaries in both textual and graphical forms, and initiate NLA Trove record retrieval. "
                                }
                            ],
                            "name": "21"
                        }
                    ],
                    "name": "21"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "unsw.edu.au",
                                    "name": "FAIMS",
                                    "usagePatterns": "1 user (me) most of the time until we start really getting under weigh. The data sets will be large and varied, whatever we can beg from tDar and OpenContext and Heurist in the main. I'll be testing data extraction and import, so a decent pipe will be nice. But very low utilization expected most of the time based on research needs. Number of core hours unknown. Reqeuest help estimating. Number of instances: We have 8 core servers, each requiring at least a medium allocation (we've experienced processor overload on our smalls) running our website, issue tracker, wiki, authentication server, primary data repository, development data repository, primary mobile app server, and development app server. With our new grant, we also will need to spin up up to 15 2-core instances to support our researchers. I've increased our volume storage request to provide room for our mandated backup regimen. ",
                                    "useCase": "This is to support the NeCTAR funded FAIMS project. Our current VM is a local-tenancy test VM that cannot support sample instances of tDAR, openContext, Heurist, and the 8 other databases I need to investigate and experiment with.  We need a front-end tiny web-host for our blog and workshop management software. And either 1 machine or a set of machines to run the various archaeological warehouses to test with. We will also likely need one or more machines to serve as storage backends to the mobile tool we're developing."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "unsw.edu.au",
                                    "name": "FAIMS",
                                    "usagePatterns": "1 user (me) most of the time until we start really getting under weigh. The data sets will be large and varied, whatever we can beg from tDar and OpenContext and Heurist in the main. I'll be testing data extraction and import, so a decent pipe will be nice. But very low utilization expected most of the time based on research needs. Number of core hours unknown. Reqeuest help estimating. Number of instances unknown, also need help there. 2 to start with, 1 web facing with wordpress and ocs, and 1 research-facing that can run 8 different data warehouses.  Object storage size is highly variable, as a function of what archaeologist buy-in we get.",
                                    "useCase": "This is to support the NeCTAR funded FAIMS project. Our current VM is a local-tenancy test VM that cannot support sample instances of tDAR, openContext, Heurist, and the 8 other databases I need to investigate and experiment with.  We need a front-end tiny web-host for our blog and workshop management software. And either 1 machine or a set of machines to run the various archaeological warehouses to test with. We will also likely need one or more machines to serve as storage backends to the mobile tool we're developing."
                                }
                            ],
                            "name": "210202"
                        }
                    ],
                    "name": "2102"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 28.8,
                                    "instanceQuota": 3.0,
                                    "institution": "monash.edu",
                                    "name": "AutoDesk Maya Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly.",
                                    "useCase": "My research group (http://infotech.monash.edu/research/groups/3dg/)  is engaged in a wide range of projects, many with an emphasis on the visualisation of the past (history and archaeology) and cultural heritage.  To date my team and I have gotten by rendering 3D visualisation research on our laptops at relatively low resolution, mainly for display on the internet. As the animations were neither long (in time) or large (in screen size), this was more less manageable. Recently there has been an increasing trend towards large scale landscapes and environments, as well as new formats for display (the new CAVE installation at Monash, for example, which requires images at resolutions of 27,000 pixels). We are currently quite compromised in what we can achieve using 3-4 Mac Book Pro laptops.  The preeminent projects where a rendering farm is required are the Visualising Angkor Project and the Monash Country Lines Archive. 1. The Visualising Angkor project explores the 3D generation and animation of landscapes, people, soundscapes and architecture in a medieval century Cambodian metropolis. The resulting scenes draw upon a wide range of archaeological and historical data, from bas-reliefs to Chinese eye-witness accounts and extensive mapping undertaken by the Greater Angkor Project and the EFEO. In comparison to the familiar historical staples of Rome, Greece and Egypt, the virtual image of Angkor remains unexplored. The recent inclusion of Angkor as a subject of study in the Australian national High School history curriculum is timely, but it also presents some interesting challenges. 2. The Monash Country Lines Archive (MCLA) is a collaborative Monash University project between the Monash Indigenous Centre (MIC), Faculty of Arts and the Faculty of Information Technology with a team of Monash researchers, digital animators and post-graduate students from the Monash Indigenous Centre, Faculty of Arts and the Faculty of Information Technology. The Monash Country Line Archive demands intellectual engagement in regards to issues associated with how best to construct a living archive that is a decolonised space in which communities are happy to see their material stored. It also provides an exciting place for scholars to work and share knowledge. "
                                }
                            ],
                            "name": "210302"
                        }
                    ],
                    "name": "2103"
                }
            ],
            "name": "21"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 10.0,
                                    "institution": "monash.edu",
                                    "name": "SAXS Data Reduction Cloud",
                                    "usagePatterns": "",
                                    "useCase": "Provide online access to \"scatterBrain\", Small Angle X-ray Scattering data reduction software developed in house at the Australian Synchrotron. The application will run on a virtual desktop (e.g., NX) on each virtual machine. Data will be accessed across the network, with no significant requirement for local storage on the NeCTAR cloud."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 0.5,
                                    "institution": "monash.edu",
                                    "name": "The Proteome Browser",
                                    "usagePatterns": "around 100 users / month and 5G datasets growth / year",
                                    "useCase": "The Proteome Browser was established to assist in the Chromosome-centric Human Proteome Project (C-HPP) of the Human Proteome Organisation (HUPO), particularly the Australia/New Zealand Chromosome 7 project, and hence has both national and international significance by providing the baseline for that project.  It is also an integral part of a current application for an ARC Centre of Excellence in Human Proteomics. It integrates several distinct data sources into unique hierarchical data types to assist in identifying areas of missing information in the human proteome and also rapidly address high level research questions.  As it is continually updated from the source data, it is a valuable resource to monitor the progress of the C-HPP and provides an overview of the quality of data available from human proteins. The virtual resources will be used to host the this web portal."
                                }
                            ],
                            "name": "110106"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 60.0,
                                    "instanceQuota": 60.0,
                                    "institution": "monash.edu",
                                    "name": "CharacterisationVL - [Additional resource]",
                                    "usagePatterns": "",
                                    "useCase": "Collaboration research with interstate universities for Characterisation virtual laboratory"
                                }
                            ],
                            "name": "1101"
                        }
                    ],
                    "name": "1101"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "anu.edu.au",
                                    "name": "AIR-POLLUTION",
                                    "usagePatterns": "Intensive use for a couple of weeks.  Then terminate.",
                                    "useCase": "Current models use more than 7GB of RAM and we have only got 8GB available.  Want to test this on a VM with 16GB of RAM."
                                },
                                {
                                    "coreQuota": 179.2,
                                    "instanceQuota": 179.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "InfectiousDiseases",
                                    "usagePatterns": "Existing machines (128vCPUs) are essentially under constant full load, reflecting our group's expansion and increased capabilities to leverage the NECTAR infrastructure. 128vCPUs are currently split as: 4 x 16 core machines 8 x 8 core machines all instances deployed and heavily used now for well over 12 months. With capacity reached and projects being queued, we are seeking expansion to 256vCPUs, envisaged to be deployed as a further 4 x 16 and 8 x 8 cores, but 8 x 16vCPUs would also be suitable. The calculation of core hours (6727680) is 3 years x 365 days x 24 hours x 256 cores.",
                                    "useCase": "We currently use 128 vCPUs at close to full load 24x7 running numerical mathematical and computational models of disease transmission and host infection. Here we apply to double capacity to 256vCPUs for this existing allocation (InfectiousDiseases). Our primary research questions lie in:  - development of public health policy for future influenza pandemic events;  - investigation of the biological process of influenza infection and transmission based on evaluation of virological and immunological data derived from animal models;  - analysis of historical time-series of influenza infections in diverse populations, to infer the role of pre-existing and acquired humoral and celluar immunity;  - evaluation of novel vaccination strategies against childhood diseases such as pertussis;  - numerical investigations into the non-linear properties of infectious disease models with novel formulations for exposure-mediated immune boosting and seasonal forcing, leading to bistable and/or chaotic behaviour; Multiple users (>5 post-docs, fellowship holders and graduate students from my research group) log in to the existing machines which are fully integrated (users, filesystem etc) into our research group's core IT infrastructure."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uwa.edu.au",
                                    "name": "BCEES",
                                    "usagePatterns": "We envisage we will have 1 to 2 instances with what is probably quite a small data set (up to 1 gigabyte) and only a handful of users. ",
                                    "useCase": "Our study was innovative in using a VM for an epidemiological study and recording all data directly onto this VM. We used the ARCS VM services, hosted by CSIRO, for this NHMRC-funded study.     The study has completed collecting data and we are writing papers, so we still need access but the ARCS services have closed now and we need some space to host our VM and the data on the VM. Thank you. "
                                }
                            ],
                            "name": "111706"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "jcu.edu.au",
                                    "name": "ACCLAiM - Australian Collaboration for Clinical Assessment in Medicine ",
                                    "usagePatterns": "Small numbers of online videos requiring download by various users - storage requirements between 70 and 150MB per video, approximately 5GB per year required at this stage.",
                                    "useCase": "The Australian Collaboration for Clinical Assessment in Medicine (ACCLAiM) commenced in 2010 and is a collaborative venture between medical schools in Australia and New Zealand, lead by James Cook University.  The project focuses on benchmarking graduate outcomes in the clinical domain.  The collaboration aims to develop shared clinical assessment items (OSCE stations) for use by medical schools in a secure online database that is easy to use.  The online system will provide online assessor training for each station, incorporating examiner discussion and feedback. The web-based collaborative workspace requires NeCTAR compute resources to enable secure online hosting, and requires a nominal amount (50GB short-term, ~100GB long-term) of backend data storage that will primarily be used to host secure video content.  Until issues around volume storage are resolved, the standard 60GB ephemeral disk associated with 1 medium VM instance should be sufficient in the short term."
                                }
                            ],
                            "name": "111711"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.8,
                                    "instanceQuota": 1.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "International Dental Graduate Study",
                                    "usagePatterns": "The project is expected to have at least 6 core users, of which at least 2 expect to use the cloud intensively.  It will have both small data sets and a few large data sets. 1. Large datasets     a. two survey datasets with about 1000 records     b. one qualitative dataset with about 500,000 words 2. Small datasets     About 15-20 smaller datasets of about 200 records Can also hold other generic data from ABS - population, geographic data etc. This will depend on the capacity of the cloud, and what we got to offer. ",
                                    "useCase": "The IDG Study is a national study of all overseas qualified dentists in Australia. Team members are mainly based in the Universities of Adelaide (4 members) and Sydney (2 members). Other members of the team are outside the university enviornment and are based in state public dental services (2 in Adelaide), , dental councils (1 member in Melbourne) and associations (2 in Sydney).  The purpose of the cloud is to: 1. Communicate effectively with team members 2. Build an external mode (or something like a website) - through which we can disseminate information to the broader dentist community.      a. Where resources can be updated regularly     b. Where team members can share daily updates on some important issues 3. Data tables can be made available to the public and can be extracted from the cloud, but with some built in protocols.  4. Have the capacity to store large quantities of qualitative and quantitative data.. that can also be visualised externally to make sense.   "
                                }
                            ],
                            "name": "111709"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd Moderator Assitant",
                                    "usagePatterns": "~100 users, not a big data set",
                                    "useCase": "Moderator Assistant is a project funded by the Young and Well CRC to develop a tool that helps moderators of mental health peer-support groups. This website will be used to annotate data and generate feedback by interns and users at the CRC and the Inspire Foundation "
                                }
                            ],
                            "name": "111714"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.4,
                                    "institution": "unisa.edu.au",
                                    "name": "UniSA_Cystic_Fibrosis_Genetic_Therapy_Community_Engagement",
                                    "usagePatterns": "1 dataset (the animation), 1 instance with one user.",
                                    "useCase": "We need to render a short (~3min) educational clip intended for distribution over the internet. The goal is to explain in easy to understand terms what research is being conducted at the Women's and Children's Hospital towards a genetic therapy cure for Cystic Fibrosis. In this way we hope to educate the general public in an easily accessible format which will garner greater public support for our work, and hopefully increased donations to the Cure4CF foundation that provides some of our funding."
                                }
                            ],
                            "name": "111799"
                        }
                    ],
                    "name": "1117"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.4,
                                    "instanceQuota": 1.4,
                                    "institution": "sydney.edu.au",
                                    "name": "OpenClinica",
                                    "usagePatterns": "",
                                    "useCase": "Instalation of OpenClinica to run a small study which will measure the effect of eggs on blood glucose control."
                                }
                            ],
                            "name": "111104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "sydney.edu.au",
                                    "name": "OpenClinica",
                                    "usagePatterns": "",
                                    "useCase": "Instalation of OpenClinica to run a small study which will measure the effect of eggs on blood glucose control."
                                }
                            ],
                            "name": "111102"
                        }
                    ],
                    "name": "1111"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unsw.edu.au",
                                    "name": "Breast cancer RNA-seq",
                                    "usagePatterns": "Large data sets with small number of users",
                                    "useCase": "I am a PhD student in the field of breast cancer and would like to analyse RNA-seq data from cell lines and tumours. I am studying the relationship between 2 transcription factors ELF5 and FOXA1 in breast cancer and the expression of the various ELF5 isoforms in breast cancer. I recently completed an NGS workshop in Canberra and would like to utilise tools such as bowtie and cufflinks.  Please note: I'm not entirely sure of the numbers I've entered and am pleased to be advised."
                                }
                            ],
                            "name": "111201"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uq.edu.au",
                                    "name": "NBCF Breast Cancer Project",
                                    "usagePatterns": "The resources will mainly be used for analysis of large genomic datasets.  This typically requires unique reference datafiles that are frequently accessed by our bioinformatic tools; these reference datafile sets can frequently be in excess of 20GB each (e.g., 3 different human genome reference sets, each of which ~20GB each).  Each genome reference set will require persistent storage. The requirement for object storage relates to archiving our patient sequencing datasets, for ongoing access by our national research term.  We currently have an application with QCloud for allocation of storage for this project. ",
                                    "useCase": "We have been funded by the NBCF for 5 years to pursue next-generation diagnostic testing and screening on clinical tumour biopsies from women diagnosed with breast cancer.  The next phase of our project requires extensive next-generation sequencing analysis of these genetic and epigenetic changes, and this analysis will take place over the length of our 5 year project grant.  The data generated will be used collaboratively by multiple users across Australia, with the goal of improving patient outcomes for women currently suffering from disease."
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 1.5,
                                    "institution": "csiro.au",
                                    "name": "CSIRO_NGSANE",
                                    "usagePatterns": "Large number of users with small fixed data set (provided toy data). ",
                                    "useCase": "Cluster with storage to showcase our sequence data analysis framework, NGSANE (https://github.com/BauerLab/ngsane), for the bioinformatics publication currently under review. One reviewer requested that our software should be testable without lengthy dependency installation and toy data download."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "NBCF Breast Cancer Project",
                                    "usagePatterns": "The resources will mainly be used for analysis of large genomic datasets.  This typically requires unique reference datafiles that are frequently accessed by our bioinformatic tools; these reference datafile sets can frequently be in excess of 20GB each (e.g., 3 different human genome reference sets, each of which ~20GB each).  Each genome reference set will require persistent storage. The requirement for object storage relates to archiving our patient sequencing datasets, for ongoing access by our national research term.  We currently have an application with QCloud for allocation of storage for this project. ",
                                    "useCase": "We have been funded by the NBCF for 5 years to pursue next-generation diagnostic testing and screening on clinical tumour biopsies from women diagnosed with breast cancer.  The next phase of our project requires extensive next-generation sequencing analysis of these genetic and epigenetic changes, and this analysis will take place over the length of our 5 year project grant.  The data generated will be used collaboratively by multiple users across Australia, with the goal of improving patient outcomes for women currently suffering from disease."
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "monash.edu",
                                    "name": "Monash-Galaxy-Prod",
                                    "usagePatterns": "The project is likely to have 10 - 50 users and some users will use more than 500GB of Volume space. May even use one TB.",
                                    "useCase": "This allocation will be for the Monash Galaxy Production server. My other allocation is being used for  training users and post grad students in the use of Galaxy and bioinformatics analyses. Once trained, the users will be moved onto this allocation for their work. It is envisaged that this Galaxy server will be used by researchers from SOBS staff but will be open to anyone from the University."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "adelaide.edu.au",
                                    "name": "SACGF",
                                    "usagePatterns": "We will have only a few users (2-5), we have our own storage, but we require the cloud for worker nodes in a Torque cluster. I have amended the request to include some persistant storage.  While we won't need this in the longer term, it turns out that we will need it in the short term in order to test the setup on sample projects.",
                                    "useCase": "We are a genome sequencing facility and our research is in the area of genomics/bioinformatics.  For our computing we have been using Torque queues both on our own dedicated server as well as eResearch SA servers.  eResearch SA is now moving over to Nectar cloud computing, so we want to implement our workflows/pipelines on a Nectar-based setup.  It is likely that we will implement this using  a dynamic Torque set-up.  The present allocation is designed to allow us to develop, implement and test  this on some sample projects.  If successful, we would then request allocations on a more permanent basis."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_HVPA",
                                    "usagePatterns": "Sequencing data pipeline and database",
                                    "useCase": "Genome variation file processing and database"
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 3.0,
                                    "institution": "monash.edu",
                                    "name": "Monash-Galaxy-Prod",
                                    "usagePatterns": "The project is likely to have 10 - 50 users and some users will use more than 500GB of Volume space. May even use one TB.",
                                    "useCase": "This allocation will be for the Monash Galaxy Production server. My other allocation is being used for  training users and post grad students in the use of Galaxy and bioinformatics analyses. Once trained, the users will be moved onto this allocation for their work. It is envisaged that this Galaxy server will be used by researchers from SOBS staff but will be open to anyone from the University."
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 0.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "SACGF",
                                    "usagePatterns": "We will have only a few users (2-5), we have our own storage, but we require the cloud for worker nodes in a Torque cluster. I have amended the request to include some persistant storage.  While we won't need this in the longer term, it turns out that we will need it in the short term in order to test the setup on sample projects.",
                                    "useCase": "We are a genome sequencing facility and our research is in the area of genomics/bioinformatics.  For our computing we have been using Torque queues both on our own dedicated server as well as eResearch SA servers.  eResearch SA is now moving over to Nectar cloud computing, so we want to implement our workflows/pipelines on a Nectar-based setup.  It is likely that we will implement this using  a dynamic Torque set-up.  The present allocation is designed to allow us to develop, implement and test  this on some sample projects.  If successful, we would then request allocations on a more permanent basis."
                                }
                            ],
                            "name": "111203"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Peter_Mac_Research_Dropbox",
                                    "usagePatterns": "I anticipate predominantly large data sets but with not many users.",
                                    "useCase": "Peter Mac has many collaborations with other local, national and international institutes and we currently often use dropbox (www.dropbox.com) to share data and collaborate. The free offering is very limited however. A use case is that Tony Papenfuss has now joined Peter Mac and he requires an easy way to access data at multiple locations (home, WEHI and Peter Mac) while also sharing the data with his collaborators and labs at both Peter Mac and WEHI."
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_PeterMac_Bioinformatics_Pipelines",
                                    "usagePatterns": "Around 4 users. Very large data sets (next-generation sequencing data). ",
                                    "useCase": "The cloud instances will be used to analyse cancer genomics data generated by Next Generation Sequencers.  Recent technological advances in cancer research have led to substantial development in bioinformatics tools and methods. We have built VMs that hold the best-practice analysis pipelines. The VMs will be used by both internal users (under this project) and external users (who will launch our images under other projects). I hope you have the capacity to provide the required support. "
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 1.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_PeterMac_Bioinformatics_Pipelines",
                                    "usagePatterns": "Around 4 users. Very large data sets (next-generation sequencing data). ",
                                    "useCase": "The cloud instances will be used to analyse cancer genomics data generated by Next Generation Sequencers.  Recent technological advances in cancer research have led to substantial development in bioinformatics tools and methods. We have built VMs that hold the best-practice analysis pipelines. The VMs will be used by both internal users (under this project) and external users (who will launch our images under other projects). I hope you have the capacity to provide the required support. "
                                }
                            ],
                            "name": "1112"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics Compute",
                                    "usagePatterns": "The instance will be used for short periods of 1-3 weeks by the applicant - Kevin Gillinder. And will be purely for data analysis. Analysed files will be then transferred to our Cancer Genomics project for longer-term storage and visualisation on genome browsers.",
                                    "useCase": "We are a genomics laboratory requiring compute to perform bioinfomatic analysis of our sequencing data collections. This application is for a VM to run intermittently and perform compute intensive tasks, for example mapping sequencing reads to the human and mouse genomes."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more object and storage space."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.4,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more volume storage space."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF Cancer Genomics",
                                    "usagePatterns": "This project will have a small number of core users, based at UQ, within Australia, and at international collaborating institutions. Restricted access will be provided to the larger scientific community. The data will be large files ranging in size from 1GB up to around 20GB each.",
                                    "useCase": "We are a genomics labs providing biomedical data for international collaborations and research use. We would like to setup a web server to provide online storage of large datasets for download and usage on the UCSC genome browser and the Genomics Virtual Lab UCSC mirror. Edit: 14/11 We are currently running a UCSC mirror, but would to extend it to custom genome versions, and to providing a co-location backup/archive facility. To do this, we require more volume storage space."
                                }
                            ],
                            "name": "111206"
                        }
                    ],
                    "name": "1112"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "International Dental Graduate Study",
                                    "usagePatterns": "The project is expected to have at least 6 core users, of which at least 2 expect to use the cloud intensively.  It will have both small data sets and a few large data sets. 1. Large datasets     a. two survey datasets with about 1000 records     b. one qualitative dataset with about 500,000 words 2. Small datasets     About 15-20 smaller datasets of about 200 records Can also hold other generic data from ABS - population, geographic data etc. This will depend on the capacity of the cloud, and what we got to offer. ",
                                    "useCase": "The IDG Study is a national study of all overseas qualified dentists in Australia. Team members are mainly based in the Universities of Adelaide (4 members) and Sydney (2 members). Other members of the team are outside the university enviornment and are based in state public dental services (2 in Adelaide), , dental councils (1 member in Melbourne) and associations (2 in Sydney).  The purpose of the cloud is to: 1. Communicate effectively with team members 2. Build an external mode (or something like a website) - through which we can disseminate information to the broader dentist community.      a. Where resources can be updated regularly     b. Where team members can share daily updates on some important issues 3. Data tables can be made available to the public and can be extracted from the cloud, but with some built in protocols.  4. Have the capacity to store large quantities of qualitative and quantitative data.. that can also be visualised externally to make sense.   "
                                }
                            ],
                            "name": "110599"
                        }
                    ],
                    "name": "1105"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 24.0,
                                    "instanceQuota": 24.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-CIAP",
                                    "usagePatterns": "At least initially we envision a small number or users (&lt; 1000 total, < 20 concurrent ) and relatively big data sets (e.g.28GB) We also need about 250 GB of volume storage.",
                                    "useCase": "This is an extension of the existing allocation for the NeCTAR Image Processing and Analysis toolkit. Some instances would be used to host online web interface and   database for the toolkit. (~ 3 instances ~ 4 cores) Other instances will be used as workers to preform actual computations. These my be provisioned on demand depending on the load. The worker instances may be either medium or large - this is still uncertain at the moment."
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "monash.edu",
                                    "name": "CharacterisationVL - [Additional resource]",
                                    "usagePatterns": "",
                                    "useCase": "Collaboration research with interstate universities for Characterisation virtual laboratory"
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "utas.edu.au",
                                    "name": "UTas Climate Change and Health Adaptions",
                                    "usagePatterns": "At this stage intermitent 20 people accessing in course.",
                                    "useCase": "A/Prof  Paul Turner paul.turner@utas.edu.au A/Prof Erica Bell Erica.Bell@utas.edu.au Climate Change and Rural  Health Risk Assessment project  online portal including quiz tool."
                                }
                            ],
                            "name": "11"
                        }
                    ],
                    "name": "11"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.8,
                                    "instanceQuota": 1.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "MoViT - Mobile Vision Tester",
                                    "usagePatterns": "The server will host collections of images that will migrate to the iPad before testing begins. These could be large (several GBs) per user, with only 2 to 3 users initially. These images will only be accessed intermittently (say monthly). Each user will also upload about 20 (say) small text files per day; log files of tests conducted on the iPads.  It is envisaged that during 2013 the server will only have a handful of users. We hope to recruit users during this period, and envisage other groups from around the world joining the project.",
                                    "useCase": "MoViT is an iPad platform for visual psychophysics developed a the University of Melbourne. As the iPad has limited local storage an filesystem, this virtual server will be used for storing configuration files and images for vision tests and for saving log files from vision tests. The app will be deployed for testing people with normal health and vision, in addition to cohorts with migraine, schizophrenia and autism both through unimelb and UWA. Ethics approval for the relevant projects has already been obtained through the host universities HRECs.  Upon completion of these pilot projects, it will be made available worldwide for other users to employ in research studies. We are hoping to provide a stable server for about 5 years (say) while we evaluate the utility of the app, its appeal in the broader psychophysics community, and build a user base. Once it becomes a vital piece of infrastructure for vision research, or traffic/usage becomes heavy, we will transition to a \"user pays\" model for providing the server."
                                }
                            ],
                            "name": "111303"
                        }
                    ],
                    "name": "1113"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "ballarat.edu.au",
                                    "name": "Sport and Recreation Spatial",
                                    "usagePatterns": "Reasonably large datasets, Small number of users. ",
                                    "useCase": "A spatial information infrastructure including Mapserver/Mapcache for the delivery of spatial information for the Sport and Recreation Spatial Project. http://www.sportandrecreationspatial.com.au/ Spatial Analysis of Sports Participation, Health and Well-being factors from Large National survey datasets"
                                }
                            ],
                            "name": "110699"
                        }
                    ],
                    "name": "1106"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 150.0,
                                    "instanceQuota": 150.0,
                                    "institution": "monash.edu",
                                    "name": "NeuroImagingClusterLarge",
                                    "usagePatterns": "I anticipate very spiky usage. There will be a very low level of everyday activity involving provisioning of VMs and tweaking of cluster configuration. This will probably only involve 5 hosts. The spikes will require the full allocation for a period of 1 to 2 weeks, as a new cohort is processed. This may happen 2 to 3 times per year. The datasets will be moderately large. Processing output is approximately 300-400Mb per subject. Only one user will be accessing the data.",
                                    "useCase": "A previous small project (NeuroImagingCluster) has successfully demonstrated the feasibility of creating a cluster to run FreeSurfer analysis on a cohort of 20 subjects. I built the cluster using Enis Agfan's cloudman software. I would now like to scale up to deal with large cohorts with ~1000 participants."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_CAI_Mouse_Brain_ANDS",
                                    "usagePatterns": "The large storage is needed for the pre-tiling of the images to speed up delivery time for the end-user. Increased CPU usage will happen as part of the pre-tiling process but won't last for too long a time. Otherwise CPUs will be stressed only when new, un-tiled data is added so that the image-server has to perform the tiling on-the-fly. Likewise RAM requirements will peak during pre-tiling and whenever we add data sets that are not already pre-tiled but generated on the go.",
                                    "useCase": "Primarily the nodes will run Tissue Stack, a software system that allows the viewing of 3D data sets as 2D cross sections."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_CAI_Mouse_Brain_ANDS",
                                    "usagePatterns": "The large storage is needed for the pre-tiling of the images to speed up delivery time for the end-user. Increased CPU usage will happen as part of the pre-tiling process but won't last for too long a time. Otherwise CPUs will be stressed only when new, un-tiled data is added so that the image-server has to perform the tiling on-the-fly. Likewise RAM requirements will peak during pre-tiling and whenever we add data sets that are not already pre-tiled but generated on the go.",
                                    "useCase": "Primarily the nodes will run Tissue Stack, a software system that allows the viewing of 3D data sets as 2D cross sections."
                                }
                            ],
                            "name": "1109"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "monash.edu",
                                    "name": "Monash_NeuroImaging_FusionForge",
                                    "usagePatterns": "The system includes databases, and source code repositories. It is relatively light weight. The computational and storage demands are not great. The main requirements are sensible, publicly visible hostnames, and constant availability.  Datasets are effectively small, with small numbers of users. ",
                                    "useCase": "FusionForge is a web-based software development support platform, derived from sourceforge. It provides version control tools, mailing lists, project web pages etc. I hope to migrate an old gforge server to the new fusionforge platform. I use version control tools to do collaborative software development, and fusionforge provides a quick and easy way to create repositories that can be shared."
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "monash.edu",
                                    "name": "Characterisation VL Development and Testing",
                                    "usagePatterns": "Mixed interactive and scheduled usage.",
                                    "useCase": "This will be a first development environment for the Characterisation VL (CVL). The numbers I gave are a first estimate. Happy to refine this as we learn more about the system and develop our requirements. I may put in further requests on behalf of users of the CVL. "
                                },
                                {
                                    "coreQuota": 38.4,
                                    "instanceQuota": 38.4,
                                    "institution": "monash.edu",
                                    "name": "CharacterisationVL - [Additional resource]",
                                    "usagePatterns": "Large data sets",
                                    "useCase": "Characterisation VM"
                                }
                            ],
                            "name": "110999"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 2.4,
                                    "institution": "uq.edu.au",
                                    "name": "Neural Netetwork Complex Dynamics Parameter Search",
                                    "usagePatterns": "This project will most likely continually run one virtual machine with the requested persistent volume storage attached.  This virtual machine will be used for debugging test experiments and storing data.  For larger experiments, a number of virtual machines will be spawned for the duration of a simulation and terminated after the job has been run.  This dynamic allocation approach aims to minimize the amount of resources that are continually being used, while greatly increasing the computational capacity for simulating neural networks.",
                                    "useCase": "The Complex and Intelligent Systems group (led by Prof J Wiles in ITEE) simulates a range of computational models of biological phenomena. The group is seeking computational resources to support a number of these projects.  I am currently conducting neural network simulations to study complex network activity across wide ranges of input parameters.  I intend to use these resources for two aspects of my project.  1) to increase the number of parameter sets I am able to simulate simultaneously. I am currently limited to 8 simultaneous simulations on my work desktop.   and 2) to increase the size of simulated networks.  I am currently limited to ~250 neurons, while some complex network dynamics only emerge with greater than ~1000 neurons.  The brain is able to continually change its activity state in response to internal representations and external stimuli.  This body of work aims to elucidate how various neural architectures are able to self-regulate these changes in activity states. Specifically, we would like to study dynamical switching mechanisms in neural networks that exhibit complex self-sustained activity.  Some of the neural regions of interest have over a million neurons and we expect that simulations siees will continue to improve with the development of modelling techniques coupled to increases in computational power."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_LeukoSeq",
                                    "usagePatterns": "Primarily will require one VM running as a Webserver 24/7. The webserver will host a custom portal for accessing clinical genomics results by our research team and clinical collaborators around the world.  07/11/13: Amended to add ability to use up to 4 XXL nodes for data analysis. This is required to enable full migration from pQERN/QERN where compute is currently based.",
                                    "useCase": "Leukodystrophies are inherited diseases that result from the pathological reduction of myelin, the dielectric material that insulates axons in the human brain. Children suffering from a leukodystrophy have substantial morbidity and mortality, with more than a third dying by age eight. Unfortunately, despite the fact that the incidence of these diseases is relatively high (at least 1 in 7,000 births), more than half of all leukodystrophy patients are never fully diagnosed  i.e. the inherited mutation at the root of their affliction remains unknown. In this research project we are harnessing the power next generation sequencing technology to identify novel gene variants that cause leukodystrophy, the first step required to develop treatments for this disease. To achieve this we are currently in the process sequencing the genomes and/or exomes (i.e. the subset of the genome that contains protein coding genes) of approximately 300 children affected by these illnesses and, where possible, their unaffected family members. In total we will be sequencing the exomes of over 1000 individuals in the next 12 months.  "
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 3.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF_LeukoSeq",
                                    "usagePatterns": "Primarily will require one VM running as a Webserver 24/7. The webserver will host a custom portal for accessing clinical genomics results by our research team and clinical collaborators around the world.  07/11/13: Amended to add ability to use up to 4 XXL nodes for data analysis. This is required to enable full migration from pQERN/QERN where compute is currently based.",
                                    "useCase": "Leukodystrophies are inherited diseases that result from the pathological reduction of myelin, the dielectric material that insulates axons in the human brain. Children suffering from a leukodystrophy have substantial morbidity and mortality, with more than a third dying by age eight. Unfortunately, despite the fact that the incidence of these diseases is relatively high (at least 1 in 7,000 births), more than half of all leukodystrophy patients are never fully diagnosed  i.e. the inherited mutation at the root of their affliction remains unknown. In this research project we are harnessing the power next generation sequencing technology to identify novel gene variants that cause leukodystrophy, the first step required to develop treatments for this disease. To achieve this we are currently in the process sequencing the genomes and/or exomes (i.e. the subset of the genome that contains protein coding genes) of approximately 300 children affected by these illnesses and, where possible, their unaffected family members. In total we will be sequencing the exomes of over 1000 individuals in the next 12 months.  "
                                }
                            ],
                            "name": "110903"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.3,
                                    "institution": "uq.edu.au",
                                    "name": "RNA-seq of iPS-derived human neurons",
                                    "usagePatterns": "This project will have a small number of users (primarily the main applicant), with large datasets, a relatively small number of big reference files (genome, annotation, mapping indexes) and a relatively small (<=100) number of output files for each sample (~60 samples total planned). The main bottlenecks include mapping RNA-seq reads (using STAR, which is quite fast, but loads the entire genome index into memory (~40Gb for human or mouse, the two species which will be interrogated for this project)) and assembling the reads into transcripts using cufflinks, which uses ~50Gb RAM for prolonged periods of time (up to 2 weeks per dataset)).   ==== Update 12.02.14 -> Have checked reference file requirements (which will be placed into object storage); currently am using 300Gb on barrine. -> In terms of volume storage, have looked at /ebi/bscratch and use ~1-1.5Tb per series of experiments (which is what would be run in parallel at a given time for the project). In order to trial running my analysis on one dataset at a time on the VMs, I would likely need ~150Gb of space; more would, of course, be better. ",
                                    "useCase": "This project focuses on characterising the differentiation of induced pluripotent stem (iPS) cells into neurons.  iPS cells are stem cells made from skin cells, which can be taken from normal people or those afflicted by a certain disease. These stem cells can then be differentiated into other cell types, for example brain cells. These cell types can be used for basic research to replace model animals, and drugs can be tested on these \"brain cells in a dish\", with the most effective drug that alleviates the patient's symptoms \"in a dish\" chosen to be administered to the patient in vivo.  While this is the hope for the application of stem cells in basic research and personalised medicine for drug testing, the reality is that we don't know how similar these neurons in the dish are to the ones we've got in our brains, which is what this project is attempting to find out. By sequencing the RNA of iPS derived stem cells and the neurons we are differentiating from them (at several time points of differentiation) we are attempting to understand what changes occur, at the molecular level, in these cells as they go from being a stem cell to being a neuron, and how similar these changes are to ones we know normally happen during neurological development.  The results of this work will be presented at local and international conferences, and subsequently written up as manuscripts for publication. ==== This research is being partly funded by NHMRC grant 1021005 \"ReprogrammingofAtaxiaTelangiectasiafibroblaststogenerateiPScells\" and APP1043023 \"Investigation of processed snoRNAs as cryptic regulators of the imprinted Prader-Willi syndrome locus\". This work represents a collaboration between bioinformaticians at the IMB, stem cell biologists at AIBN,  and stem cell biologists and clinicians at QIMR. "
                                }
                            ],
                            "name": "110902"
                        }
                    ],
                    "name": "1109"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 6.4,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_QEH_Rhinology",
                                    "usagePatterns": "For now, I will be the only user in the department running and accessing the instances. The input files/datasets will be huge (in GBs), since they will be files produced by pyrosequencers, for example 454 Roche or Illumina sequencers, hence the need for big storage space in the form of Volumes and Object storage. At the beginning, I do not expect to run more than one large instance at a time in case a pipeline is running. Once we have all the VM images/snapshots ready and pipelines and scripts set in place, I can then request from NECTAR admins for more resources or storage space in case this is needed, but this won't be needed initially. Some genomics work is known to be RAM-limited rather than CPU-limited, due to the huge datasets, and this is the reason (high RAM) I may need to do run an xx-large instance, but again this will not be needed until I have set up my development environment on snapshots and running test runs of the pipeline (and I become comfortable with the NECTAR openstack system). In general, I estimate the workload per year will be fairly very low compared to researchers from other fields, since microbiome research constitutes only a percentage of all of our research.",
                                    "useCase": "We are the department Otorhinolaryngology, Head & Neck Surgery (Department of Surgery, University of Adelaide), based in the Queen Elizabeth Hospital (QEH) campus in SA. Our main research is Rhinology, revolving around chronic rhinosinusitis and endoscopic sinus surgery. Our department has recently started doing microbiome research in human sinuses, to study the role of the sinus microbiome in chronic rhinosinusitis. For example, we have recently published a paper on 16s rRNA amplicon sequencing to identify fungi in the sinuses) We will also be doing RNA-seq work in the near future to study the immunological response of the host to the microbiome. For us to start doing this work in-house, we need a cluster or a cloud. I had previously tried running our own in-house pipeline using QIIME v1.6 (Quantitative insights into Microcial Ecology) on a machine in our department and it constantly fails during the denoising step, because it is so computer-intensive and I hope NECTAR is going to help us develop our own tools and produce our own results instead of outsourcing them. The types of programs we will be running on the instances will be mostly bioinformatics/genomics scripts/programs for example: - denoising raw NGS datasets, such as pyronoise, ampliconnoise or QIIME - BLAST - cluserting algorithms, for Operational Taxonomic Units (OTUs) clustering - taxonomic classification of OTUs, for example using a Naive Bayes Classifier - genome sequence alignment, such as TopHat or Velvet - Python - Postgresql "
                                }
                            ],
                            "name": "110315"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uq.edu.au",
                                    "name": "Qld breast cancer",
                                    "usagePatterns": "We will be using the SlidePath which provides virtual microscopy and digital pathology software solutions for clinical Research. It runs Digital SlideBox, a virtual microscopy e-learning solution; Tissue IA/OpTMA, an automated tissue microarray data storage and management system that allows users to review and score cores online; Digital Image Hub, a Web-enabled digital pathology PACS system that allows users to review and annotate slides over the Internet; and Distiller, a Web-based information management tool for creation and management of data hierarchies. In order to use this software package to process and analyse our software we require 2 large virtual servers, 4 cores with 16 GB of RAM each. The system also uses machine based image analysis of various parameters associated with the images including nuclear, cytoplasmic and membrane staining of biomarkers. This enables analysis of a large number of cores and approximately 20,000 have been analysed to date. This enables correlation with clinical and outcome data to determine the usefulness of the various biomarkers and the correlation and interaction with patient outcome.",
                                    "useCase": "Analysis of breast cancer database. We run a database containing information on breast cancers and correlation with survival information. Images from both whole slides and tissue microarrays are attached to the samples with information on biomarkers. The data base comprises approximately 8,000 breast cancer patients with attached biomarker information, treatment, outcome and images of tumour. This database enables us to associate outcome data to clinical images, this is relatively unique and has the potential to be used for ongoing validation of breast cancer, for example we can evaluate oestrogen and progesterone receptor status in HER2 positive breast carcinomas and correlate that with the patients treatment outcomes."
                                }
                            ],
                            "name": "110316"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_HVPA",
                                    "usagePatterns": "Sequencing data pipeline and database",
                                    "useCase": "Genome variation file processing and database"
                                }
                            ],
                            "name": "110311"
                        }
                    ],
                    "name": "1103"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 6.0,
                                    "institution": "monash.edu",
                                    "name": "SAXS Data Reduction Cloud",
                                    "usagePatterns": "",
                                    "useCase": "Provide online access to \"scatterBrain\", Small Angle X-ray Scattering data reduction software developed in house at the Australian Synchrotron. The application will run on a virtual desktop (e.g., NX) on each virtual machine. Data will be accessed across the network, with no significant requirement for local storage on the NeCTAR cloud."
                                }
                            ],
                            "name": "111504"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "XAS",
                                    "usagePatterns": "Data with be accessible by many users and data sets are predicted to be relatively small ( < 1GB) However, data packs and measurements are expected to be numerous (~30-40 ) on each case.  ",
                                    "useCase": "XAS analysis using mainly the EXAFSPAK suite or anything else that might be proved more sufficient during the study  "
                                }
                            ],
                            "name": "111599"
                        }
                    ],
                    "name": "1115"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 76.8,
                                    "instanceQuota": 76.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "InfectiousDiseases",
                                    "usagePatterns": "Existing machines (128vCPUs) are essentially under constant full load, reflecting our group's expansion and increased capabilities to leverage the NECTAR infrastructure. 128vCPUs are currently split as: 4 x 16 core machines 8 x 8 core machines all instances deployed and heavily used now for well over 12 months. With capacity reached and projects being queued, we are seeking expansion to 256vCPUs, envisaged to be deployed as a further 4 x 16 and 8 x 8 cores, but 8 x 16vCPUs would also be suitable. The calculation of core hours (6727680) is 3 years x 365 days x 24 hours x 256 cores.",
                                    "useCase": "We currently use 128 vCPUs at close to full load 24x7 running numerical mathematical and computational models of disease transmission and host infection. Here we apply to double capacity to 256vCPUs for this existing allocation (InfectiousDiseases). Our primary research questions lie in:  - development of public health policy for future influenza pandemic events;  - investigation of the biological process of influenza infection and transmission based on evaluation of virological and immunological data derived from animal models;  - analysis of historical time-series of influenza infections in diverse populations, to infer the role of pre-existing and acquired humoral and celluar immunity;  - evaluation of novel vaccination strategies against childhood diseases such as pertussis;  - numerical investigations into the non-linear properties of infectious disease models with novel formulations for exposure-mediated immune boosting and seasonal forcing, leading to bistable and/or chaotic behaviour; Multiple users (>5 post-docs, fellowship holders and graduate students from my research group) log in to the existing machines which are fully integrated (users, filesystem etc) into our research group's core IT infrastructure."
                                }
                            ],
                            "name": "110804"
                        }
                    ],
                    "name": "1108"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_QEH_Rhinology",
                                    "usagePatterns": "For now, I will be the only user in the department running and accessing the instances. The input files/datasets will be huge (in GBs), since they will be files produced by pyrosequencers, for example 454 Roche or Illumina sequencers, hence the need for big storage space in the form of Volumes and Object storage. At the beginning, I do not expect to run more than one large instance at a time in case a pipeline is running. Once we have all the VM images/snapshots ready and pipelines and scripts set in place, I can then request from NECTAR admins for more resources or storage space in case this is needed, but this won't be needed initially. Some genomics work is known to be RAM-limited rather than CPU-limited, due to the huge datasets, and this is the reason (high RAM) I may need to do run an xx-large instance, but again this will not be needed until I have set up my development environment on snapshots and running test runs of the pipeline (and I become comfortable with the NECTAR openstack system). In general, I estimate the workload per year will be fairly very low compared to researchers from other fields, since microbiome research constitutes only a percentage of all of our research.",
                                    "useCase": "We are the department Otorhinolaryngology, Head & Neck Surgery (Department of Surgery, University of Adelaide), based in the Queen Elizabeth Hospital (QEH) campus in SA. Our main research is Rhinology, revolving around chronic rhinosinusitis and endoscopic sinus surgery. Our department has recently started doing microbiome research in human sinuses, to study the role of the sinus microbiome in chronic rhinosinusitis. For example, we have recently published a paper on 16s rRNA amplicon sequencing to identify fungi in the sinuses) We will also be doing RNA-seq work in the near future to study the immunological response of the host to the microbiome. For us to start doing this work in-house, we need a cluster or a cloud. I had previously tried running our own in-house pipeline using QIIME v1.6 (Quantitative insights into Microcial Ecology) on a machine in our department and it constantly fails during the denoising step, because it is so computer-intensive and I hope NECTAR is going to help us develop our own tools and produce our own results instead of outsourcing them. The types of programs we will be running on the instances will be mostly bioinformatics/genomics scripts/programs for example: - denoising raw NGS datasets, such as pyronoise, ampliconnoise or QIIME - BLAST - cluserting algorithms, for Operational Taxonomic Units (OTUs) clustering - taxonomic classification of OTUs, for example using a Naive Bayes Classifier - genome sequence alignment, such as TopHat or Velvet - Python - Postgresql "
                                }
                            ],
                            "name": "1107"
                        }
                    ],
                    "name": "1107"
                }
            ],
            "name": "11"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "sydney.edu.au",
                                    "name": "ANIM.OS Websocket Server",
                                    "usagePatterns": "Java app on server is running a scheduler that triggers events on client machines. Server sends short simple control messages (strings) to clients and receives simple string requests from client. Server is also providing video and other generative media (HTML5 canvas-based content). ",
                                    "useCase": "A fast centralised websocket server that will allow multiple users in a single geographic location to collaborate on multi-device, synchronised multimedia exhibitions, presentations and performances. Can be thought of as a mixed multimedia version of the MIT project \"Junkyard Jumbotron\"."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "sydney.edu.au",
                                    "name": "Solstice",
                                    "usagePatterns": "The project's main requirement is bandwidth in serving audio files to users mobile devices. Vivid runs from May 24th to June 10th with the project running from dusk to midnight each night. File serving needs to be relatively fast but we will handle synch issues by buffering pre-rendered audio on devices in good time. However, clearly there will be a capacity issue at the server or at specific mobile access points. We intend to gauge and handle this within the possible limits. Requests to download will be interrupted at capacity. An ultra minimal default file will be used as higher capacity is reached. ",
                                    "useCase": "Novel electronic media artwork. Audio files will be served from the VM to mobile devices and played in synch to create multi-device generative musical structures. A node.js websocket server on the VM will serve requests. Compressed audio files will be downloaded onto devices as needed. Client-side javascript / HTML5 audio tag will play audio in synch with a laser show being projected on the AMP building, Circular Quay, Sydney. This is a component of the Solstice large-scale electronic media artwork, hosted by the Vivid Festival and AMP's amplify festival. The project is developed by researchers in electronic arts and music based at the Faculty of Architecture, Design and Planning, and the Conservatorium of Music, University of Sydney. The work involves electronic media arts research that will result in HERDC countable research outputs and will constitute a major Non-Traditional Research Output. Usage patterns and audience response will be surveyed.  To be clear: multiple audience members distributed throughout the Circular Quay and Sydney Harbour area will be simultaneously accessing files on the server."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "monash.edu",
                                    "name": "C4D Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly. ",
                                    "useCase": "We have a need to render a large amount high resolution 3D computer animation for a visualisation project. The research involves developmental simulations that mimic biological growth and morphogenesis. Simulation software we have developed generates very complex geometric models that are too large for realtime visualisation, hence the need for distributed rendering."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 2.0,
                                    "institution": "monash.edu",
                                    "name": "C4D Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly. ",
                                    "useCase": "We have a need to render a large amount high resolution 3D computer animation for a visualisation project. The research involves developmental simulations that mimic biological growth and morphogenesis. Simulation software we have developed generates very complex geometric models that are too large for realtime visualisation, hence the need for distributed rendering."
                                }
                            ],
                            "name": "190203"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_CAARP",
                                    "usagePatterns": "This project will have approximately 40 users a week and a data set currently of 16MB, looking to increase slowly.",
                                    "useCase": "The Cinema and Audiences Research Project (CAARP) aims to promote research into, and a deeper appreciation of, the history of film exhibition and cinema going in Australia. The CAARP database holds information about film-related events, capturing where and when individual screenings took place and the relationship between film distribution, exhibition venues and cinema-going in different periods and locales. The unique structure of the database allows for a historical perspective to be recorded and viewed from within a single screen. . The database provides a framework for research and analysis concerned with the history of film exhibition and distribution in Australia, and currently houses information about more than 11 000 films, 1 700 companies, 2 000 venues, and in excess of 400 000 film screenings. The NeCTAR RC will become the new home for this database and will allow the HuNI VL harvest data from it."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Bonza",
                                    "usagePatterns": "Approximately 20 users a week with 11MB in data, but this is increasing slowly over time. There are additional images close to 120MB also.",
                                    "useCase": "Similar to the CAARP project, this project will also be harvested by the HuNI VL. The RC application will provide a consistent platform for maintenance of this database as for others relating to the HuNI VL."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_CAARP",
                                    "usagePatterns": "This project will have approximately 40 users a week and a data set currently of 16MB, looking to increase slowly.",
                                    "useCase": "The Cinema and Audiences Research Project (CAARP) aims to promote research into, and a deeper appreciation of, the history of film exhibition and cinema going in Australia. The CAARP database holds information about film-related events, capturing where and when individual screenings took place and the relationship between film distribution, exhibition venues and cinema-going in different periods and locales. The unique structure of the database allows for a historical perspective to be recorded and viewed from within a single screen. . The database provides a framework for research and analysis concerned with the history of film exhibition and distribution in Australia, and currently houses information about more than 11 000 films, 1 700 companies, 2 000 venues, and in excess of 400 000 film screenings. The NeCTAR RC will become the new home for this database and will allow the HuNI VL harvest data from it."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Bonza",
                                    "usagePatterns": "Approximately 20 users a week with 11MB in data, but this is increasing slowly over time. There are additional images close to 120MB also.",
                                    "useCase": "Similar to the CAARP project, this project will also be harvested by the HuNI VL. The RC application will provide a consistent platform for maintenance of this database as for others relating to the HuNI VL."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_CAARP",
                                    "usagePatterns": "This project will have approximately 40 users a week and a data set currently of 16MB, looking to increase slowly.",
                                    "useCase": "The Cinema and Audiences Research Project (CAARP) aims to promote research into, and a deeper appreciation of, the history of film exhibition and cinema going in Australia. The CAARP database holds information about film-related events, capturing where and when individual screenings took place and the relationship between film distribution, exhibition venues and cinema-going in different periods and locales. The unique structure of the database allows for a historical perspective to be recorded and viewed from within a single screen. . The database provides a framework for research and analysis concerned with the history of film exhibition and distribution in Australia, and currently houses information about more than 11 000 films, 1 700 companies, 2 000 venues, and in excess of 400 000 film screenings. The NeCTAR RC will become the new home for this database and will allow the HuNI VL harvest data from it."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Bonza",
                                    "usagePatterns": "Approximately 20 users a week with 11MB in data, but this is increasing slowly over time. There are additional images close to 120MB also.",
                                    "useCase": "Similar to the CAARP project, this project will also be harvested by the HuNI VL. The RC application will provide a consistent platform for maintenance of this database as for others relating to the HuNI VL."
                                }
                            ],
                            "name": "190201"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "sydney.edu.au",
                                    "name": "ANIM.OS Websocket Server",
                                    "usagePatterns": "Java app on server is running a scheduler that triggers events on client machines. Server sends short simple control messages (strings) to clients and receives simple string requests from client. Server is also providing video and other generative media (HTML5 canvas-based content). ",
                                    "useCase": "A fast centralised websocket server that will allow multiple users in a single geographic location to collaborate on multi-device, synchronised multimedia exhibitions, presentations and performances. Can be thought of as a mixed multimedia version of the MIT project \"Junkyard Jumbotron\"."
                                }
                            ],
                            "name": "190205"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 2.0,
                                    "institution": "monash.edu",
                                    "name": "AutoDesk Maya Renderfarm",
                                    "usagePatterns": "The renderfarm software operates on a client-server model, jobs are submitted to the server that farms out specific rendering tasks (frames or subframes) to the clients. This requires only temporary local storage for the clients, as each rendered frame is sent back to the server for assembly.",
                                    "useCase": "My research group (http://infotech.monash.edu/research/groups/3dg/)  is engaged in a wide range of projects, many with an emphasis on the visualisation of the past (history and archaeology) and cultural heritage.  To date my team and I have gotten by rendering 3D visualisation research on our laptops at relatively low resolution, mainly for display on the internet. As the animations were neither long (in time) or large (in screen size), this was more less manageable. Recently there has been an increasing trend towards large scale landscapes and environments, as well as new formats for display (the new CAVE installation at Monash, for example, which requires images at resolutions of 27,000 pixels). We are currently quite compromised in what we can achieve using 3-4 Mac Book Pro laptops.  The preeminent projects where a rendering farm is required are the Visualising Angkor Project and the Monash Country Lines Archive. 1. The Visualising Angkor project explores the 3D generation and animation of landscapes, people, soundscapes and architecture in a medieval century Cambodian metropolis. The resulting scenes draw upon a wide range of archaeological and historical data, from bas-reliefs to Chinese eye-witness accounts and extensive mapping undertaken by the Greater Angkor Project and the EFEO. In comparison to the familiar historical staples of Rome, Greece and Egypt, the virtual image of Angkor remains unexplored. The recent inclusion of Angkor as a subject of study in the Australian national High School history curriculum is timely, but it also presents some interesting challenges. 2. The Monash Country Lines Archive (MCLA) is a collaborative Monash University project between the Monash Indigenous Centre (MIC), Faculty of Arts and the Faculty of Information Technology with a team of Monash researchers, digital animators and post-graduate students from the Monash Indigenous Centre, Faculty of Arts and the Faculty of Information Technology. The Monash Country Line Archive demands intellectual engagement in regards to issues associated with how best to construct a living archive that is a decolonised space in which communities are happy to see their material stored. It also provides an exciting place for scholars to work and share knowledge. "
                                }
                            ],
                            "name": "190202"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "Twitter Data Collection",
                                    "usagePatterns": "These machines will run a web accessible front-end for data collection specification, and a mySQL/php script backend responsible for data collection. While admin/ssh access will be restricted to two accounts, there will be a number of users across several institutions using the front end (25-50 users). The machines will run a mySQL database for live metrics and data collection, which will periodically be backed up to local storage.",
                                    "useCase": "These machines will be responsible for social media data collection for a variety of current projects, including but not limited to crisis communication, political conversation, scientific communication, dissemination of news and conversation around entertainment (to include television, movies, games and social networks). This will feed into ongoing research on the above subjects focused on creating metrics to measure audience engagement and participation. For more details see http://www.mappingonlinepublics.com and http://www.tvmetrics.net"
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "cqu.edu.au",
                                    "name": "Access Grid International Website Hosting",
                                    "usagePatterns": "Most of the content that will be accessed is in the simple form of webpages.  Additional to this, the site will also provide the Access Grid Software downloads (similar to http://www.accessgrid.org/software).  As an example, the latest Windows Bundle installer for the Access Grid Software is 75MB in size.",
                                    "useCase": "I would like to request a VM which will help support the Australian (and International) Access Grid Community. As you might not be aware, ANL withdrew support for the open source Access Grid project.  From this, the International community, as well as leveraging open services such as sourceforge, combined to provide resources to continue the Access Grid Project. One of these services included the International Access Grid Web Site (See http://www.accessgrid.org/ - which currently runs on Drupal).  WestGrid (Canada) currently provides this service on a temporary basis, until a new permanent location/website could be arranged.  The server infrastructure currently hosting the website is reaching end of life and WestGrid can no longer provide this service and host this website. Though the Access Grid is slowly losing support  particularly internationally, there is still significant usage (and even new uptake  especially with AMSI [Australian Mathematical Sciences Institute]) and it would be good if this Research and Teaching Tool could continue to be supported. There are a number of us within the international community who would be happy to setup and support the servers, we just need the infrastructure to host the international website. "
                                }
                            ],
                            "name": "1902"
                        }
                    ],
                    "name": "1902"
                }
            ],
            "name": "19"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "qut.edu.au",
                                    "name": "QUT_IT - Extend end-date",
                                    "usagePatterns": "Increasingly large datasets with a small number of users. These instances will largely be left alone to gather data from the Twitter API on a continuous basis, and users will only access them to change content tracking parameters and/or download the gathered data. ",
                                    "useCase": "This is an extension request for the current project QUT_TT. Our We currently have an allocation of 10 instances, running to the end of the year; we would like to extend this project, ideally until the end of 2013. If possible, it would also be very useful to increase our instance allocation, ideally to 20 instances. Our originally submitted use case is below: I lead a team of social media researchers at QUT who are focussing on large-scale, long-term research into the uses of Twitter. For our longitudinal work, we intend to set up several instances of the open-source Twitter tracking solution yourTwapperkeeper (https://github.com/jobrieniii/yourTwapperKeeper). These will connect to the Twitter API to continuously ingest tweets matching certain criteria (keywords, hashtags, etc.). Two trial yTK installs are currently running on my standard allocation NeCTAR instances. These instances will be used simply to ingest and store data, which will be downloaded at regular intervals (weekly / monthly) by the researchers. NeCTAR instances are ideal for this because they are stable and independent; we have found that instances based on our university servers are interrupted too often by general server and network maintenance activities. Each individual instance will only need to be small, as ingestion of data from the Twitter API requires limited computing power, but it will need to be operational 24/7 and the databases holding the ingested tweets may grow large. "
                                }
                            ],
                            "name": "200102"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uws.edu.au",
                                    "name": "ADELTA",
                                    "usagePatterns": "",
                                    "useCase": "ADELTA Adelta is the Australian Directory of Electronic Literature and Text-based Art The directory emerges from the Creative Nation ARC discovery funded project: Writers and Writing in the New Media Arts led by Assoc/Prof Anna Gibbs and Dr Maria Angel from the Writing and Society Research Group in the School of Humanities and Communication Arts at the University of Western Sydney. This project is building and/or adapting open source software (probably the Islandora repository platform) to house an interactive directory of Australian writers and writing in the New Media Arts. This work is taking place in the context of work on the Consortium of Electronic Literature (CELL). This consortium addresses the development of collaboration for the purposes of researching, publishing and archiving electronic works, and is currently working on the interoperability of international databases of electronic literature. Founding partners of CELL are the University of Western Sydney (adelta), ELMCIP, Po.Ex, NT2, Electronic Book Review, University of Siegen (Likumed), Hermeneia, and Archiveit.org/Library of Congress. Technical work on this project is being done by the UWS eResearch team, led by Peter Sefton, with technical work done by Lloyd Harischandra and Ifeanyi Egwutuoha "
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uws.edu.au",
                                    "name": "ADELTA",
                                    "usagePatterns": "",
                                    "useCase": "ADELTA Adelta is the Australian Directory of Electronic Literature and Text-based Art The directory emerges from the Creative Nation ARC discovery funded project: Writers and Writing in the New Media Arts led by Assoc/Prof Anna Gibbs and Dr Maria Angel from the Writing and Society Research Group in the School of Humanities and Communication Arts at the University of Western Sydney. This project is building and/or adapting open source software (probably the Islandora repository platform) to house an interactive directory of Australian writers and writing in the New Media Arts. This work is taking place in the context of work on the Consortium of Electronic Literature (CELL). This consortium addresses the development of collaboration for the purposes of researching, publishing and archiving electronic works, and is currently working on the interoperability of international databases of electronic literature. Founding partners of CELL are the University of Western Sydney (adelta), ELMCIP, Po.Ex, NT2, Electronic Book Review, University of Siegen (Likumed), Hermeneia, and Archiveit.org/Library of Congress. Technical work on this project is being done by the UWS eResearch team, led by Peter Sefton, with technical work done by Lloyd Harischandra and Ifeanyi Egwutuoha "
                                },
                                {
                                    "coreQuota": 6.0,
                                    "instanceQuota": 3.0,
                                    "institution": "qut.edu.au",
                                    "name": "Twitter Data Collection",
                                    "usagePatterns": "These machines will run a web accessible front-end for data collection specification, and a mySQL/php script backend responsible for data collection. While admin/ssh access will be restricted to two accounts, there will be a number of users across several institutions using the front end (25-50 users). The machines will run a mySQL database for live metrics and data collection, which will periodically be backed up to local storage.",
                                    "useCase": "These machines will be responsible for social media data collection for a variety of current projects, including but not limited to crisis communication, political conversation, scientific communication, dissemination of news and conversation around entertainment (to include television, movies, games and social networks). This will feed into ongoing research on the above subjects focused on creating metrics to measure audience engagement and participation. For more details see http://www.mappingonlinepublics.com and http://www.tvmetrics.net"
                                }
                            ],
                            "name": "2001"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 10.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "HuNI_VL",
                                    "usagePatterns": "Small data sets and few users. Possible public access.",
                                    "useCase": "NeCTAR VL exemplar"
                                },
                                {
                                    "coreQuota": 10.0,
                                    "instanceQuota": 5.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "HuNI_VL",
                                    "usagePatterns": "Small data sets and few users. Possible public access.",
                                    "useCase": "NeCTAR VL exemplar"
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "qut.edu.au",
                                    "name": "QUT_IT - Extend end-date",
                                    "usagePatterns": "Increasingly large datasets with a small number of users. These instances will largely be left alone to gather data from the Twitter API on a continuous basis, and users will only access them to change content tracking parameters and/or download the gathered data. ",
                                    "useCase": "This is an extension request for the current project QUT_TT. Our We currently have an allocation of 10 instances, running to the end of the year; we would like to extend this project, ideally until the end of 2013. If possible, it would also be very useful to increase our instance allocation, ideally to 20 instances. Our originally submitted use case is below: I lead a team of social media researchers at QUT who are focussing on large-scale, long-term research into the uses of Twitter. For our longitudinal work, we intend to set up several instances of the open-source Twitter tracking solution yourTwapperkeeper (https://github.com/jobrieniii/yourTwapperKeeper). These will connect to the Twitter API to continuously ingest tweets matching certain criteria (keywords, hashtags, etc.). Two trial yTK installs are currently running on my standard allocation NeCTAR instances. These instances will be used simply to ingest and store data, which will be downloaded at regular intervals (weekly / monthly) by the researchers. NeCTAR instances are ideal for this because they are stable and independent; we have found that instances based on our university servers are interrupted too often by general server and network maintenance activities. Each individual instance will only need to be small, as ingestion of data from the Twitter API requires limited computing power, but it will need to be operational 24/7 and the databases holding the ingested tweets may grow large. "
                                }
                            ],
                            "name": "200104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "qut.edu.au",
                                    "name": "QUT_IT - Extend end-date",
                                    "usagePatterns": "Increasingly large datasets with a small number of users. These instances will largely be left alone to gather data from the Twitter API on a continuous basis, and users will only access them to change content tracking parameters and/or download the gathered data. ",
                                    "useCase": "This is an extension request for the current project QUT_TT. Our We currently have an allocation of 10 instances, running to the end of the year; we would like to extend this project, ideally until the end of 2013. If possible, it would also be very useful to increase our instance allocation, ideally to 20 instances. Our originally submitted use case is below: I lead a team of social media researchers at QUT who are focussing on large-scale, long-term research into the uses of Twitter. For our longitudinal work, we intend to set up several instances of the open-source Twitter tracking solution yourTwapperkeeper (https://github.com/jobrieniii/yourTwapperKeeper). These will connect to the Twitter API to continuously ingest tweets matching certain criteria (keywords, hashtags, etc.). Two trial yTK installs are currently running on my standard allocation NeCTAR instances. These instances will be used simply to ingest and store data, which will be downloaded at regular intervals (weekly / monthly) by the researchers. NeCTAR instances are ideal for this because they are stable and independent; we have found that instances based on our university servers are interrupted too often by general server and network maintenance activities. Each individual instance will only need to be small, as ingestion of data from the Twitter API requires limited computing power, but it will need to be operational 24/7 and the databases holding the ingested tweets may grow large. "
                                }
                            ],
                            "name": "200101"
                        }
                    ],
                    "name": "2001"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uon.edu.au",
                                    "name": "harlequin",
                                    "usagePatterns": "I have a smaller dataset (inflates to around 2 gig). There is one user (me). I have already uploaded most of the data I will need. I can use a single machine to do most of the work, but I need lots of RAM because I'm using R. ",
                                    "useCase": "I have a large amount of contemporary romance novels that I analyse using weighted gene correlation network analysis - effectively sequencing the genre. I typically use Amazon EC2 for this (my preliminary results are based on an EC2 run). I would, however, prefer to use Nectar. "
                                }
                            ],
                            "name": "200302"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "griffith.edu.au",
                                    "name": "QCIF AusNC Migration and Enhancement",
                                    "usagePatterns": "many users small datasets",
                                    "useCase": "The Australian Nationall Corpus (AusNC) is an innovative online discovery service that provides access to many varied samples of language in Australia - in all its forms and diversity: Audio files, written texts, digital AV, etc, from different places, times and language varieties.  The website (www.ausnc.org.au) is an advanced research tool for linguists, language technologists and other researchesrs in digital humanities and social sciences."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "griffith.edu.au",
                                    "name": "AusNC Migration and Enhancement",
                                    "usagePatterns": "This instance is for the compute and web site of the project. the other server request will be used as a Proxy and Cache service to help speed up the site.",
                                    "useCase": "The AusNC is an innovative online discovery service that provids access to many varied samles of lanuage in Australia.  The web site is an advanced research tool for linguists, lanuage technologists and other researchers in digital humanities and social sciences."
                                }
                            ],
                            "name": "2003"
                        }
                    ],
                    "name": "2003"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.5,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_TUGG",
                                    "usagePatterns": "The project will have many users interacting with an growing data set.  Contributions from those users will feed the data sets. The system is also a potential harvest end point for the HuNI VL.",
                                    "useCase": "The Ultimate Gig Guide (TUGG) tracks the history of the live music scene in Melbourne, as it evolved from organised dance hall events, to discos and the thriving pub music scene of today. The database consists of a variety of contemporary gig guides, capturing minute details of bands, performances and venues to create a unique dataset of Melbourne music history. The TUGG platform has the ability to manage content, expose it to researchers, students, and casual users, and eventually allow end users to contribute their own knowledge. It has been established through a university research grant and it represents a multi-institutional, interdisciplinary research effort. The database is built using lightweight, modern web frameworks: Django provides the database logic, while Twitter Bootstrap delivers a cross-browser, cross-device independent web interface. The Nectar Research Cloud has been a useful deployment host during the development phase, due to its quick allocation time. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.5,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_TUGG",
                                    "usagePatterns": "The project will have many users interacting with an growing data set.  Contributions from those users will feed the data sets. The system is also a potential harvest end point for the HuNI VL.",
                                    "useCase": "The Ultimate Gig Guide (TUGG) tracks the history of the live music scene in Melbourne, as it evolved from organised dance hall events, to discos and the thriving pub music scene of today. The database consists of a variety of contemporary gig guides, capturing minute details of bands, performances and venues to create a unique dataset of Melbourne music history. The TUGG platform has the ability to manage content, expose it to researchers, students, and casual users, and eventually allow end users to contribute their own knowledge. It has been established through a university research grant and it represents a multi-institutional, interdisciplinary research effort. The database is built using lightweight, modern web frameworks: Django provides the database logic, while Twitter Bootstrap delivers a cross-browser, cross-device independent web interface. The Nectar Research Cloud has been a useful deployment host during the development phase, due to its quick allocation time. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.5,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_TUGG",
                                    "usagePatterns": "The project will have many users interacting with an growing data set.  Contributions from those users will feed the data sets. The system is also a potential harvest end point for the HuNI VL.",
                                    "useCase": "The Ultimate Gig Guide (TUGG) tracks the history of the live music scene in Melbourne, as it evolved from organised dance hall events, to discos and the thriving pub music scene of today. The database consists of a variety of contemporary gig guides, capturing minute details of bands, performances and venues to create a unique dataset of Melbourne music history. The TUGG platform has the ability to manage content, expose it to researchers, students, and casual users, and eventually allow end users to contribute their own knowledge. It has been established through a university research grant and it represents a multi-institutional, interdisciplinary research effort. The database is built using lightweight, modern web frameworks: Django provides the database logic, while Twitter Bootstrap delivers a cross-browser, cross-device independent web interface. The Nectar Research Cloud has been a useful deployment host during the development phase, due to its quick allocation time. "
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_CAARP",
                                    "usagePatterns": "This project will have approximately 40 users a week and a data set currently of 16MB, looking to increase slowly.",
                                    "useCase": "The Cinema and Audiences Research Project (CAARP) aims to promote research into, and a deeper appreciation of, the history of film exhibition and cinema going in Australia. The CAARP database holds information about film-related events, capturing where and when individual screenings took place and the relationship between film distribution, exhibition venues and cinema-going in different periods and locales. The unique structure of the database allows for a historical perspective to be recorded and viewed from within a single screen. . The database provides a framework for research and analysis concerned with the history of film exhibition and distribution in Australia, and currently houses information about more than 11 000 films, 1 700 companies, 2 000 venues, and in excess of 400 000 film screenings. The NeCTAR RC will become the new home for this database and will allow the HuNI VL harvest data from it."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Bonza",
                                    "usagePatterns": "Approximately 20 users a week with 11MB in data, but this is increasing slowly over time. There are additional images close to 120MB also.",
                                    "useCase": "Similar to the CAARP project, this project will also be harvested by the HuNI VL. The RC application will provide a consistent platform for maintenance of this database as for others relating to the HuNI VL."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_CAARP",
                                    "usagePatterns": "This project will have approximately 40 users a week and a data set currently of 16MB, looking to increase slowly.",
                                    "useCase": "The Cinema and Audiences Research Project (CAARP) aims to promote research into, and a deeper appreciation of, the history of film exhibition and cinema going in Australia. The CAARP database holds information about film-related events, capturing where and when individual screenings took place and the relationship between film distribution, exhibition venues and cinema-going in different periods and locales. The unique structure of the database allows for a historical perspective to be recorded and viewed from within a single screen. . The database provides a framework for research and analysis concerned with the history of film exhibition and distribution in Australia, and currently houses information about more than 11 000 films, 1 700 companies, 2 000 venues, and in excess of 400 000 film screenings. The NeCTAR RC will become the new home for this database and will allow the HuNI VL harvest data from it."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Bonza",
                                    "usagePatterns": "Approximately 20 users a week with 11MB in data, but this is increasing slowly over time. There are additional images close to 120MB also.",
                                    "useCase": "Similar to the CAARP project, this project will also be harvested by the HuNI VL. The RC application will provide a consistent platform for maintenance of this database as for others relating to the HuNI VL."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_CAARP",
                                    "usagePatterns": "This project will have approximately 40 users a week and a data set currently of 16MB, looking to increase slowly.",
                                    "useCase": "The Cinema and Audiences Research Project (CAARP) aims to promote research into, and a deeper appreciation of, the history of film exhibition and cinema going in Australia. The CAARP database holds information about film-related events, capturing where and when individual screenings took place and the relationship between film distribution, exhibition venues and cinema-going in different periods and locales. The unique structure of the database allows for a historical perspective to be recorded and viewed from within a single screen. . The database provides a framework for research and analysis concerned with the history of film exhibition and distribution in Australia, and currently houses information about more than 11 000 films, 1 700 companies, 2 000 venues, and in excess of 400 000 film screenings. The NeCTAR RC will become the new home for this database and will allow the HuNI VL harvest data from it."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 1.0,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_Bonza",
                                    "usagePatterns": "Approximately 20 users a week with 11MB in data, but this is increasing slowly over time. There are additional images close to 120MB also.",
                                    "useCase": "Similar to the CAARP project, this project will also be harvested by the HuNI VL. The RC application will provide a consistent platform for maintenance of this database as for others relating to the HuNI VL."
                                }
                            ],
                            "name": "200203"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "HuNI_VL",
                                    "usagePatterns": "Small data sets and few users. Possible public access.",
                                    "useCase": "NeCTAR VL exemplar"
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 4.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "HuNI_VL",
                                    "usagePatterns": "Small data sets and few users. Possible public access.",
                                    "useCase": "NeCTAR VL exemplar"
                                }
                            ],
                            "name": "200209"
                        }
                    ],
                    "name": "2002"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "uon.edu.au",
                                    "name": "harlequin",
                                    "usagePatterns": "I have a smaller dataset (inflates to around 2 gig). There is one user (me). I have already uploaded most of the data I will need. I can use a single machine to do most of the work, but I need lots of RAM because I'm using R. ",
                                    "useCase": "I have a large amount of contemporary romance novels that I analyse using weighted gene correlation network analysis - effectively sequencing the genre. I typically use Amazon EC2 for this (my preliminary results are based on an EC2 run). I would, however, prefer to use Nectar. "
                                }
                            ],
                            "name": "200526"
                        }
                    ],
                    "name": "2005"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "griffith.edu.au",
                                    "name": "QCIF AusNC Migration and Enhancement",
                                    "usagePatterns": "many users small datasets",
                                    "useCase": "The Australian Nationall Corpus (AusNC) is an innovative online discovery service that provides access to many varied samples of language in Australia - in all its forms and diversity: Audio files, written texts, digital AV, etc, from different places, times and language varieties.  The website (www.ausnc.org.au) is an advanced research tool for linguists, language technologists and other researchesrs in digital humanities and social sciences."
                                },
                                {
                                    "coreQuota": 128.0,
                                    "instanceQuota": 128.0,
                                    "institution": "intersect.org.au",
                                    "name": "HCSvLab",
                                    "usagePatterns": "The project will have a few users with large datasets. This usage pattern is relevant now, during development, but it will likely change once the virtual laboratory goes into full production. The INDRI cluster we want to setup will potentially consist of 10-15 simultaneously running instances. We need additional instances for running other tools.",
                                    "useCase": "This is for testing and development of the HCSvLab NeCTAR Virtual Laboratory Project. We want to setup instances to test tools (e.g. DeMoLib, HTK and INDRI) that will be run over data sourced from the virtual laboratory. One particular use case will be to setup a cluster for indexing very large datasets using INDRI. The dataset is called ClueWeb09 and is 25TB (http://lemurproject.org/clueweb09/). The clustering will potentially be done using Hadoop. "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "griffith.edu.au",
                                    "name": "AusNC Migration and Enhancement",
                                    "usagePatterns": "This instance is for the compute and web site of the project. the other server request will be used as a Proxy and Cache service to help speed up the site.",
                                    "useCase": "The AusNC is an innovative online discovery service that provids access to many varied samles of lanuage in Australia.  The web site is an advanced research tool for linguists, lanuage technologists and other researchers in digital humanities and social sciences."
                                },
                                {
                                    "coreQuota": 128.0,
                                    "instanceQuota": 8.0,
                                    "institution": "intersect.org.au",
                                    "name": "HCSvLab",
                                    "usagePatterns": "The project will have a few users with large datasets. This usage pattern is relevant now, during development, but it will likely change once the virtual laboratory goes into full production. The INDRI cluster we want to setup will potentially consist of 10-15 simultaneously running instances. We need additional instances for running other tools.",
                                    "useCase": "This is for testing and development of the HCSvLab NeCTAR Virtual Laboratory Project. We want to setup instances to test tools (e.g. DeMoLib, HTK and INDRI) that will be run over data sourced from the virtual laboratory. One particular use case will be to setup a cluster for indexing very large datasets using INDRI. The dataset is called ClueWeb09 and is 25TB (http://lemurproject.org/clueweb09/). The clustering will potentially be done using Hadoop. "
                                }
                            ],
                            "name": "2004"
                        }
                    ],
                    "name": "2004"
                }
            ],
            "name": "20"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 200.0,
                                    "instanceQuota": 100.0,
                                    "institution": "utas.edu.au",
                                    "name": "Marine Virtual Laboratory",
                                    "usagePatterns": "There will be large dta sets with small number of users",
                                    "useCase": "Marine Virtual Laboratory (MARVL) is a nectar founded virtual laboratory project. The project requires to be deployed in nectar cloud. Therefore, we requires 100 instances (8 cores per instance) to run ocean models associated with other services and applications. We will manage these instances on behalf of our end users and an admin account is required.  The admin account should not attached to any AAF accounts."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "utas.edu.au",
                                    "name": "IMOS Chef Integration Testing",
                                    "usagePatterns": "Technically the project will not have any users but various IMOS developers may need access, there are 9 of us in total, if it is at all possible to have a non user based account so our jenkins server could access the project without using a developer's authentication that would be ace.",
                                    "useCase": "We use Chef to manage our infrastructure and we have reached a critical juncture with regard to integration testing. The purpose of this allocation would be to allow our CI server (jenkins) to spin up VMs and apply our node configurations to them via chef-client and test that each completes successfully, then destroy each VM."
                                },
                                {
                                    "coreQuota": 2.8,
                                    "instanceQuota": 1.4,
                                    "institution": "csiro.au",
                                    "name": "IMOS AODAAC",
                                    "usagePatterns": "The system needs to be tested distributed across two VMs. We're using the necTAR RC systems for development and testing for the production instance hosted by IMOS. We expect episodic use by 3 developers, interspersed with longer periods of routine operation with a light ongoing testing load. The small persistent storage request is for the databases and some (read-only) test files.  Ephemeral storage ought be sufficient for the transient results of the testing envisaged.  Load testing will occasionally be heavy but relatively rare.",
                                    "useCase": "We are developing a system for extracting and aggregating subsets of large gridded data sets served by distributed OPeNDAP servers. The V1 system sits behind the IMOS Portal (See http://espace.library.uq.edu.au/view/UQ:155380 and p5 of http://imos.org.au/fileadmin/user_upload/shared/AODN/AODN_Newsletter_Nov2012.pdf). Development is ongoing, with V2 system due for transfer to IMOS in late 2013.  We need to test distributed mode of operation, with db and system population/query services on one node, with subset and aggregation work on another. ."
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "utas.edu.au",
                                    "name": "Marine Virtual Laboratory",
                                    "usagePatterns": "There will be large dta sets with small number of users",
                                    "useCase": "Marine Virtual Laboratory (MARVL) is a nectar founded virtual laboratory project. The project requires to be deployed in nectar cloud. Therefore, we requires 100 instances (8 cores per instance) to run ocean models associated with other services and applications. We will manage these instances on behalf of our end users and an admin account is required.  The admin account should not attached to any AAF accounts."
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "utas.edu.au",
                                    "name": "Marine Virtual Laboratory",
                                    "usagePatterns": "There will be large dta sets with small number of users",
                                    "useCase": "Marine Virtual Laboratory (MARVL) is a nectar founded virtual laboratory project. The project requires to be deployed in nectar cloud. Therefore, we requires 100 instances (8 cores per instance) to run ocean models associated with other services and applications. We will manage these instances on behalf of our end users and an admin account is required.  The admin account should not attached to any AAF accounts."
                                }
                            ],
                            "name": "0405"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "swin.edu.au",
                                    "name": "NOPP WW3",
                                    "usagePatterns": "Single user with large data sets.",
                                    "useCase": "Evaluate and test new physics in a third-generation wave model (WAVEWATCH III) for years 2003, 2004, 2005, 2006."
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "unsw.edu.au",
                                    "name": "A web portal to study the spread of debris in the global ocean",
                                    "usagePatterns": "The online web portal does not need high availability. There is also no need for persistency and/or backups. We do, however, hope to be able to use more than two cores. More cores (ideally four to eight) allows us to speed up the key component of the portal: the matrix multiplication that gives us the evolution of tracer in time, by using Atlas and OpenMP frameworks. We do not, however, need the memory that comes with each core. We don't expect high traffic to the web portal under normal operation, but envision that there could be peaks due to high exposure related to events in the media (such as another Fukushima). At these times, and particularly when media embeds the tool on their web sites, we expect that it will be very popular and provide high exposure.",
                                    "useCase": "We have recently been funded by the ARC Centre of Excellence on Climate System Science to develop a web portal where users can interact with animations of the spreading of tracer through the ocean. We envision the web portal to develop into the to-go-to place for people interested in ocean circulation pathways. The portal will be used by both researchers and members of the general public who want to study the pathways of marine debris and contaminants, as well as learn about the role of the ocean in global climate. We hope that we can use the Research Cloud at NeCTAR to host this high profile portal.  After the Fukushima nuclear disaster in March 2011, one of the most pressing issues of general public concern was where the nuclear waste would end up. Similar reactions followed the Deep Water Horizon oil spill in the Gulf of Mexico in April 2010 and (on a more local scale) the beaching of the container ship Rena in New Zealand in October 2011. Our online web portal can provide the general public with answers to these questions. Recently, we have developed a novel way to study spreading of tracer on the surface of the global ocean. In this method, which relies on iterating a simple vector-matrix multiplication, data from observed buoys in the global ocean are used to calculate the spreading of tracer from any point in the ocean over time. A manuscript detailing this method and applying it to the global garbage patches has been published in the A* peer-reviewed journal Environmental Research Letters. A video abstract of the method is available from http://www.youtube.com/watch?v=M4UK9Yt6A-s. The manuscript has gotten extensive media attention, both nationally and internationally, and the video abstract has been watched over 8,000 times in two weeks. The method to calculate the evolution of tracer in the ocean is computationally extremely efficient, which opens up the possibility to let the general public directly interact with it. Which is why the ARC Centre of Excellence on Climate System Science has funded its development and we are now seeking NeCTAR help for the hosting. Further questions on the research of the portal can be directed to Erik van Sebille (E.vanSebille@unsw.eud.au), questions on the technical side can be directed to David Fuchs (D.Fuchs@unsw.edu.au)"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "csiro.au",
                                    "name": "MARVL",
                                    "usagePatterns": "Medium data sets with few uses.",
                                    "useCase": "Use to test installation system, data registration and interface for MARVL (a Nectar Virtual Lab). Also interested in testing Persistent Volumes on behalf of other potential CSIRO CMAR users of NeCTAR cloud."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 0.5,
                                    "institution": "griffith.edu.au",
                                    "name": "Ocean model",
                                    "usagePatterns": "Large data sets with a small number of users",
                                    "useCase": "I need a VM to conduct the project listed below. The project implements high-resolution ocean model to investigate coastal processes for the South-east Queensland. The project requires a large volume of storage space and access speed to analyse the data. Although there are some activities regrading regional ocean modelling in Australia such as CSIRO and universities, this level of detailed numerical modelling has not been conducted in Australia and will demonstrate capacity of high-resolution modelling necessary to understand coastal ocean, which is currently missing."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "csiro.au",
                                    "name": "MARVL",
                                    "usagePatterns": "Medium data sets with few uses.",
                                    "useCase": "Use to test installation system, data registration and interface for MARVL (a Nectar Virtual Lab). Also interested in testing Persistent Volumes on behalf of other potential CSIRO CMAR users of NeCTAR cloud."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Storm Surge Modelling and Emergency Response Management",
                                    "usagePatterns": "",
                                    "useCase": "To provide a real-time visualisation tool for the display of model outputs from the storm surge project. The QLD-funded project is to establish a real time storm surge forecast and distribution system for QLD on cloud. This VM will set up routine processes of the storm surge forecasts and to visualise in real time model outputs given a range of selectable parameters (cyclone intensity, landfall, angle of approach, tide, etc). This may be extended to calculate and display inland inundation levels."
                                }
                            ],
                            "name": "040503"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 8.0,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_Satellite_Ocean_Colour_Processing",
                                    "usagePatterns": "The raw satellite data would be uploaded to the persistent data storage allocation in the beginning of the project. This data would then be used to generate new datasets that would be downloaded to a local machine for final checking and storage. The dataset is relatively small (<500GB) and would have up to three users accessing it through this project.",
                                    "useCase": "I commenced my PhD research at the Institute for Marine and Antarctic Studies (IMAS), at the University of Tasmania, in February 2011. The central focus of my work is the use and assimilation of a long term, that is >10 years and continuing, biological and oceanographic dataset from the Southern Ocean south of Australia. I plan to use the nectar cloud in the development of regional ocean colour satellite algorithms specifically tuned for the Southern Ocean. I will be taking raw NASA satellite data and matching it to coincident in situ data, as well as batch processing satellite data through to final products. The final datasets will not be stored on the cloud. This work is too large to be done on a desktop machine and too small to be done on traditional super computing facilities, which is why I believe that the nectar cloud would be the perfect place."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.8,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_IMAS_Predators",
                                    "usagePatterns": "The project will have moderately lager data sets (e.g remote sensing data from the Southern Ocean encompassing several decades), but relatively few simultaneous users - typically no more than 2",
                                    "useCase": "Our VM is used to run complex, Bayesian statistic models, usually in relation to animal movement and habitat analyses"
                                }
                            ],
                            "name": "040501"
                        }
                    ],
                    "name": "0405"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "unsw.edu.au",
                                    "name": "CliMDDIR",
                                    "usagePatterns": " 1. Web service offering batch processing capabilities of climate simulations (1-2 cores). 2. Spatial database offering geographic search capabilities (1 core). 3. scheduler (rabbitmq) which serves jobs to the data processing engines (these sit outside of NECTAR cloud, 1 core). 4. FTP/RSYNC serving data to the end user (i.e. impact and adaptation researcher) and syncs output from the backend.",
                                    "useCase": "Web end to our ANDS funded CliMDDIR project (AP04). This project is a high profile project that brings together researchers from all disciplines as well as policy makers. Note related links below. Portal About: http://www.climddir.org/node/11 Partners: http://www.climddir.org/node/8#overlay-context=node/2 Service: http://climddir.org/service/ "
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "qut.edu.au",
                                    "name": "Semaphore",
                                    "usagePatterns": "Many users with medium-sized data sets.",
                                    "useCase": "This is for the Semaphore Q-Cloud Tool Migration project, funded as a QCIF infrastructure project."
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 3.0,
                                    "institution": "qut.edu.au",
                                    "name": "Semaphore",
                                    "usagePatterns": "Many users with medium-sized data sets.",
                                    "useCase": "This is for the Semaphore Q-Cloud Tool Migration project, funded as a QCIF infrastructure project."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "utas.edu.au",
                                    "name": "UTas Climate Change and Health Adaptions",
                                    "usagePatterns": "At this stage intermitent 20 people accessing in course.",
                                    "useCase": "A/Prof  Paul Turner paul.turner@utas.edu.au A/Prof Erica Bell Erica.Bell@utas.edu.au Climate Change and Rural  Health Risk Assessment project  online portal including quiz tool."
                                }
                            ],
                            "name": "040104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.4,
                                    "institution": "mq.edu.au",
                                    "name": "ecosystem Modelling and Scaling (eMAST",
                                    "usagePatterns": "We will establish a metadata, wiki and map service to distribute summary information on the data and products eMAST produces. The single instance will run software, available to the public, that will serve low volume metadata across Australia.",
                                    "useCase": "Users need to be able to easily discover ecosystem Modelling and Scaling Infrastructure (eMAST) data which includes estimates of Australia's climate, carbon and water cycle. Using a mixture of metadata and geospatial tools, this service will share information on eMAST's products and services."
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "unsw.edu.au",
                                    "name": "A web portal to study the spread of debris in the global ocean",
                                    "usagePatterns": "The online web portal does not need high availability. There is also no need for persistency and/or backups. We do, however, hope to be able to use more than two cores. More cores (ideally four to eight) allows us to speed up the key component of the portal: the matrix multiplication that gives us the evolution of tracer in time, by using Atlas and OpenMP frameworks. We do not, however, need the memory that comes with each core. We don't expect high traffic to the web portal under normal operation, but envision that there could be peaks due to high exposure related to events in the media (such as another Fukushima). At these times, and particularly when media embeds the tool on their web sites, we expect that it will be very popular and provide high exposure.",
                                    "useCase": "We have recently been funded by the ARC Centre of Excellence on Climate System Science to develop a web portal where users can interact with animations of the spreading of tracer through the ocean. We envision the web portal to develop into the to-go-to place for people interested in ocean circulation pathways. The portal will be used by both researchers and members of the general public who want to study the pathways of marine debris and contaminants, as well as learn about the role of the ocean in global climate. We hope that we can use the Research Cloud at NeCTAR to host this high profile portal.  After the Fukushima nuclear disaster in March 2011, one of the most pressing issues of general public concern was where the nuclear waste would end up. Similar reactions followed the Deep Water Horizon oil spill in the Gulf of Mexico in April 2010 and (on a more local scale) the beaching of the container ship Rena in New Zealand in October 2011. Our online web portal can provide the general public with answers to these questions. Recently, we have developed a novel way to study spreading of tracer on the surface of the global ocean. In this method, which relies on iterating a simple vector-matrix multiplication, data from observed buoys in the global ocean are used to calculate the spreading of tracer from any point in the ocean over time. A manuscript detailing this method and applying it to the global garbage patches has been published in the A* peer-reviewed journal Environmental Research Letters. A video abstract of the method is available from http://www.youtube.com/watch?v=M4UK9Yt6A-s. The manuscript has gotten extensive media attention, both nationally and internationally, and the video abstract has been watched over 8,000 times in two weeks. The method to calculate the evolution of tracer in the ocean is computationally extremely efficient, which opens up the possibility to let the general public directly interact with it. Which is why the ARC Centre of Excellence on Climate System Science has funded its development and we are now seeking NeCTAR help for the hosting. Further questions on the research of the portal can be directed to Erik van Sebille (E.vanSebille@unsw.eud.au), questions on the technical side can be directed to David Fuchs (D.Fuchs@unsw.edu.au)"
                                }
                            ],
                            "name": "040105"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 0.9,
                                    "institution": "monash.edu",
                                    "name": " OzFlux Data Portal",
                                    "usagePatterns": "144 users and 2G datasets growth /year",
                                    "useCase": " OzFlux is part of the Australian Terrestrial Ecosystem Research Network (TERN). The OzFlux network consists of nearly 30 flux towers in Australia and New Zealand, many of which are also members of the Australian Supersite Network (ASN). OzFlux is also a member of the global FluxNet community of over 500 flux stations that is designed to provide continuous, long-term micrometeorological measurements to monitor the state of ecosystems globally. The OzFlux purpose is: to understand mechanisms controlling exchanges of carbon, water vapour and energy between terrestrial ecosystems and the atmosphere over a range of time and space scales to provide data on carbon and water balances of key ecosystems for model testing to provide information to validate estimates of net primary productivity, evaporation, and energy absorption using remotely sensed radiance data to provide data to validate new developments in micrometeorological theory for fluxes and air flows in complex terrain to provide high precision CO2 concentrations measurements (at Cape Grim) for use in regional, continental and global atmospheric inverse studies of the carbon cycle. Data from the OzFlux network of flux towers is available from this portal.  The data is organised into collections with each collection representing at least one site. The virtual resources will be used to host this web data portal."
                                }
                            ],
                            "name": "0401"
                        }
                    ],
                    "name": "0401"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "csiro.au",
                                    "name": "GeophysicsVL - [Additional VMs]",
                                    "usagePatterns": "We will be running the workshop as a demonstration so theres a good bet approximately 25 requests for 4/8 core VMs are going to come in a relatively short period of time (essentially synchronised). The VMs will not be doing any snapshotting. You can count on us using one of two images. We will be running the workshop so that the processing time is minimised. Therefore we dont expect to push around datasets any larger than a few MB per user. ",
                                    "useCase": "This is a temporary extension to the resources allocated to the existing GeophysicsVL project. We are hosting a workshop on the 22nd of March and are expecting a surge of new users."
                                }
                            ],
                            "name": "0404"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "monash.edu",
                                    "name": "Underworld_NeCTAR_Cloud_Flow",
                                    "usagePatterns": "Medium to large datasets/simulation results per user, 20+ users. The project will manage the life-span of data retention in the Object Store.",
                                    "useCase": "A web based job launch and result aggregation tool is being implemented which aims to provide a low usage barrier for new users to leverage cloud resources, while still retaining the the flexibility for power users to scale to large simulations (utilising appropriate resources).  Simulation results generated using the gLucifer visualisation framework will be immediately available for users to view and interact with directly within their browsers via the web-based gLuciferViewer (an outcome of the Geology to Geophysics NeCTAR tool). This will accelerate and simplify the visual analysis necessary in geophysical modelling. Further to this, simulation metadata and provenance will be stored within the job launch tool alongside generated simulation data. The user workflow is - * Simulation are launched to cloud VM instances (or other) from the job launch tool. * Simulation and visualisation data are pushed into the cloud storage from various sources, either at job completion or during job execution.   * The cloud storage is then queried by Underworld Cloud workflow tool for visualisation and other simulation data via a web page hosted by a webserver hosted on the Research Cloud. "
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 80.0,
                                    "institution": "monash.edu",
                                    "name": "Underworld Student Lab",
                                    "usagePatterns": "small to medium datasets/simulation results per user, 20+ users. The project will manage the life-span of data retention in the Object Store.",
                                    "useCase": " Underworld is a 3D-parallel geodynamic modelling framework capable of deriving viscous / viscoplastic thermal, chemical and thermochemical models consistent with tectonic processes, such as mantle convection and lithospheric deformation over long time scales.  Underworld uses PETSc (optimised numerical solvers) and MPI (parallelism). The aim is to have a pool of resources available for students participating in Louis Moresi's Underworld workshop. This will drastically reduce the difficulty new users encounter owing to running across heterogeneous platforms, while still allowing the exploration of parallel simulation techniques.  Students will be provided with a VM image preloaded with Underworld and its associated prerequisites."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 2.0,
                                    "institution": "monash.edu",
                                    "name": "Underworld_NeCTAR_Cloud_Flow",
                                    "usagePatterns": "Medium to large datasets/simulation results per user, 20+ users. The project will manage the life-span of data retention in the Object Store.",
                                    "useCase": "A web based job launch and result aggregation tool is being implemented which aims to provide a low usage barrier for new users to leverage cloud resources, while still retaining the the flexibility for power users to scale to large simulations (utilising appropriate resources).  Simulation results generated using the gLucifer visualisation framework will be immediately available for users to view and interact with directly within their browsers via the web-based gLuciferViewer (an outcome of the Geology to Geophysics NeCTAR tool). This will accelerate and simplify the visual analysis necessary in geophysical modelling. Further to this, simulation metadata and provenance will be stored within the job launch tool alongside generated simulation data. The user workflow is - * Simulation are launched to cloud VM instances (or other) from the job launch tool. * Simulation and visualisation data are pushed into the cloud storage from various sources, either at job completion or during job execution.   * The cloud storage is then queried by Underworld Cloud workflow tool for visualisation and other simulation data via a web page hosted by a webserver hosted on the Research Cloud. "
                                },
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 10.0,
                                    "institution": "monash.edu",
                                    "name": "Underworld Student Lab",
                                    "usagePatterns": "small to medium datasets/simulation results per user, 20+ users. The project will manage the life-span of data retention in the Object Store.",
                                    "useCase": " Underworld is a 3D-parallel geodynamic modelling framework capable of deriving viscous / viscoplastic thermal, chemical and thermochemical models consistent with tectonic processes, such as mantle convection and lithospheric deformation over long time scales.  Underworld uses PETSc (optimised numerical solvers) and MPI (parallelism). The aim is to have a pool of resources available for students participating in Louis Moresi's Underworld workshop. This will drastically reduce the difficulty new users encounter owing to running across heterogeneous platforms, while still allowing the exploration of parallel simulation techniques.  Students will be provided with a VM image preloaded with Underworld and its associated prerequisites."
                                }
                            ],
                            "name": "040402"
                        }
                    ],
                    "name": "0404"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "flinders.edu.au",
                                    "name": "Near Surface Barchan Dune Flow",
                                    "usagePatterns": "The project will have one user simulating flow in a computational domain which has been parallelised. Output's from the simulations can be managed to remain reasonably small.",
                                    "useCase": "I intend to use the cloud to simulate near surface flow over barchan dunes using the computational fluid dynamic software OpenFOAM. This is the first time such a study has been conducted and it is hoped the increased resolution of near surface flow simulated will provide insight to their morphology and migration."
                                }
                            ],
                            "name": "040607"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF DSITIA Remote Sensing Trial",
                                    "usagePatterns": "Few users, large datasets. Intermittent processing, as this is a pilot. ",
                                    "useCase": "This is a pilot project to trial the feasibility of processing Qld Gov't Landsat satellite imagery archive with QCIF. The initial pilot will set up the systems and data for a single Landsat tile (path/row), for all available dates (approx. 600 dates). One such tile's worth of data is approximately 1.7Tb (just for comparison, there are 102 such tiles to cover all of Qld). The data is used for monitoring of Queensland's rangelands, in particular vegetation cover of different types, in accordance with Qld state government's various land management needs. If the pilot is successful, this would assist in decision making about medium term needs for high performance computing infrastructure within Qld DSITIA. "
                                }
                            ],
                            "name": "040699"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "stormsurge-cemdss",
                                    "usagePatterns": "Seeking legal advice: currently believe this data will be available to Emergency Management Qld for internal disaster management purposes and not available for re-use.",
                                    "useCase": "This data set has great value to emergency management and disaster planning due to its unique nature, the intensive and high resolution modelling effort utilising eResearch infrastructure at Griffith University and QCIF. Peer review and publications are in progress and will be supplied at the end of the project. We need this VM to mount RDSI storage already allocated to this project in Queensland. The allocation # is: Q0046. We will mount the rdsi storgae on this server: https://ssrisk.qcloud.qcif.edu.au/Q0046 "
                                }
                            ],
                            "name": "040604"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "flinders.edu.au",
                                    "name": "Near Surface Barchan Dune Flow",
                                    "usagePatterns": "The project will have one user simulating flow in a computational domain which has been parallelised. Output's from the simulations can be managed to remain reasonably small.",
                                    "useCase": "I intend to use the cloud to simulate near surface flow over barchan dunes using the computational fluid dynamic software OpenFOAM. This is the first time such a study has been conducted and it is hoped the increased resolution of near surface flow simulated will provide insight to their morphology and migration."
                                }
                            ],
                            "name": "040601"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.2,
                                    "institution": "monash.edu",
                                    "name": " OzFlux Data Portal",
                                    "usagePatterns": "144 users and 2G datasets growth /year",
                                    "useCase": " OzFlux is part of the Australian Terrestrial Ecosystem Research Network (TERN). The OzFlux network consists of nearly 30 flux towers in Australia and New Zealand, many of which are also members of the Australian Supersite Network (ASN). OzFlux is also a member of the global FluxNet community of over 500 flux stations that is designed to provide continuous, long-term micrometeorological measurements to monitor the state of ecosystems globally. The OzFlux purpose is: to understand mechanisms controlling exchanges of carbon, water vapour and energy between terrestrial ecosystems and the atmosphere over a range of time and space scales to provide data on carbon and water balances of key ecosystems for model testing to provide information to validate estimates of net primary productivity, evaporation, and energy absorption using remotely sensed radiance data to provide data to validate new developments in micrometeorological theory for fluxes and air flows in complex terrain to provide high precision CO2 concentrations measurements (at Cape Grim) for use in regional, continental and global atmospheric inverse studies of the carbon cycle. Data from the OzFlux network of flux towers is available from this portal.  The data is organised into collections with each collection representing at least one site. The virtual resources will be used to host this web data portal."
                                }
                            ],
                            "name": "0406"
                        }
                    ],
                    "name": "0406"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 14.4,
                                    "instanceQuota": 14.4,
                                    "institution": "csiro.au",
                                    "name": "TERN Australian Coastal Ecosystems Facility",
                                    "usagePatterns": "Please ignore the core hours field above - as far as I can tell, it does not really apply for how we plan to use these servers. We will be hosting 10 to 50 datasets, with most being regularly added to. About half will be file-based, and we hope to use object data storage for those: the initial space requirement is around 2TB, expected to grow by 100GB/month.  The remaining datasets will require databases: MySQL, PostgreSQL/PostGIS and MongoDB. These will need to be able to be backed up regularly. Open-source data-services and GUI applications will be used to make this data available to the general public. All data requests will be made via HTTP, routed to the appropriate services via Apache. Load will be highly variable, but should not be sustained at high levels. There will be a limited number of concurrent users, fewer than 10 with login rights.",
                                    "useCase": "This request is for infrastructure to be associated with several projects, including: * Australian Coastal Ecosystems Facility (ACEF),  which is part of TERN (see http://acef.tern.org.au/?q=data) * The South-east Queensland Integrated Terrestrial to Ocean Research (SEQITOR), which is funded by ANDS (see  http://blog.seqitor.org.au/)  * eReefs (http://ereefs.org.au/) These servers will store and serve a combination of spatial time-series data, geo-referenced video data, remote sensing imagery, and 4D model outputs.  The VM size specified above is not required for all 6 servers, but is the best choice the form would allow. The actual planned use and size requirement for each one is: Server 1. Map Rendering and tile cache (GeoServer / THREDDS). XXL VM please! Server 2. Real-time sensor storage and aggregation (SensorCloud). XL Server 3. Map Layer aggregation (GeoNetwork). XL Server 4. Web-GUI Applications. XL Server 5. Build and test environment. M. Server 6. Monitoring and control of other servers. M."
                                }
                            ],
                            "name": "040305"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "uq.edu.au",
                                    "name": "CMLR",
                                    "usagePatterns": "Right now less than 10 users with data sets constantly increased every 2 or 3 months.",
                                    "useCase": "CMLR needs to build up a centralised web-based database system for researchers of monitoring data (flora, water quality, sediment and etc...). Currently it contains more than 10 years of collected data from multiple projects. The data volume set is increasing every month as researchers collect new data from field. Some other small databases will also be integrated into the system as well, such as a research journal database."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "CMLR",
                                    "usagePatterns": "Right now less than 10 users with data sets constantly increased every 2 or 3 months.",
                                    "useCase": "CMLR needs to build up a centralised web-based database system for researchers of monitoring data (flora, water quality, sediment and etc...). Currently it contains more than 10 years of collected data from multiple projects. The data volume set is increasing every month as researchers collect new data from field. Some other small databases will also be integrated into the system as well, such as a research journal database."
                                }
                            ],
                            "name": "0403"
                        }
                    ],
                    "name": "0403"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 1.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "TERN-Adel",
                                    "usagePatterns": "The project provides a User Accessible portal - AEKOS that contains large amounts of ecological data, binary and images. The initial estimates for the amounts of data in the system have been revised and we now require extra data capacity. ",
                                    "useCase": "As per our initial request that established the TERN-Adel tenancy.  The issue we now face is that we are being requested to provide individual AEKOS environments to our data providers in order for them to test and verify their datasets inside AEKOS prior to publication in production. The data ingestion processes that we need to run to create AEKOS data is extremely memory and processor intensive and generates a significant amount of data as an output. Our existing allocation is no longer able to cope with the demand we are receiving.  "
                                }
                            ],
                            "name": "04"
                        }
                    ],
                    "name": "04"
                }
            ],
            "name": "04"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "anu.edu.au",
                                    "name": "Water and Energy Data Analysis",
                                    "usagePatterns": "Please see 'Use case'. Just some simple data purning that requires significant amounts of ram because I am using R. ",
                                    "useCase": "I am working on a project to analyse water and energy consumption for the ACT Gov. I run out of memory to run the sample dataset on my desktop, so I wish to use the large amount of memory available on Nectur. I dont require much computing time nor hard drive space as I just need to do some simple data merges and breakdowns so the data is in smaller usable parts that I can then analyse on my desktop"
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 2.0,
                                    "institution": "anu.edu.au",
                                    "name": "ANU_HuNI",
                                    "usagePatterns": "",
                                    "useCase": "Component of HUNI project work. "
                                }
                            ],
                            "name": "16"
                        }
                    ],
                    "name": "16"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "intersect.org.au",
                                    "name": "Intersect HCSvLab",
                                    "usagePatterns": "",
                                    "useCase": "This instance is to be integrated with the Intersect HCSvLab project: A virtual lab which brings together many existing tools targeted towards research in communication sciences. In particular, this VM will host three tools (the creators of which are collaborating with HCSvLab) which are to be made available to users online."
                                }
                            ],
                            "name": "169999"
                        }
                    ],
                    "name": "1699"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "jcu.edu.au",
                                    "name": "Smart Water PoC (Proof of Concept)",
                                    "usagePatterns": "Web access by individuals Database driven data from individual smart water meters",
                                    "useCase": "Townsville Water & Waste (Townsville Water) is working with researchers in the eResearch Centre at James Cook University in seeking ways to assist residents reduce the amount of water they need to use.  The rationale for the selection of Townsville as a test region is that it is uniquely dry for a tropical area  resulting in an average  household water use that is amongst the highest in Australia, some 3 times more than other major Australian cities. Part of this is because a significant proportion of this water is used outdoors (approximately 60% of the usage).  Compute resources are required as part of hosting services for a \"smart water meter\" pilot to demonstrate and validate what can be achieved; these meters are installed in certain Townsville residences and allow the capture of water flow and similar information, which can be fed back to residents via a web-based portal. Findings of this pilot project can then be scaled eventually into a whole-of-Townsville approach. This project is currently underway and compute resources are required for data storage, serving, and access."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "Smart Water PoC (Proof of Concept)",
                                    "usagePatterns": "",
                                    "useCase": "This is a storage extension to the existing project"
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 0.5,
                                    "institution": "jcu.edu.au",
                                    "name": "Smart Water PoC (Proof of Concept)",
                                    "usagePatterns": "Web access by individuals Database driven data from individual smart water meters",
                                    "useCase": "Townsville Water & Waste (Townsville Water) is working with researchers in the eResearch Centre at James Cook University in seeking ways to assist residents reduce the amount of water they need to use.  The rationale for the selection of Townsville as a test region is that it is uniquely dry for a tropical area  resulting in an average  household water use that is amongst the highest in Australia, some 3 times more than other major Australian cities. Part of this is because a significant proportion of this water is used outdoors (approximately 60% of the usage).  Compute resources are required as part of hosting services for a \"smart water meter\" pilot to demonstrate and validate what can be achieved; these meters are installed in certain Townsville residences and allow the capture of water flow and similar information, which can be fed back to residents via a web-based portal. Findings of this pilot project can then be scaled eventually into a whole-of-Townsville approach. This project is currently underway and compute resources are required for data storage, serving, and access."
                                },
                                {
                                    "coreQuota": 0.0,
                                    "instanceQuota": 0.0,
                                    "institution": "uq.edu.au",
                                    "name": "Smart Water PoC (Proof of Concept)",
                                    "usagePatterns": "",
                                    "useCase": "This is a storage extension to the existing project"
                                }
                            ],
                            "name": "160404"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "ballarat.edu.au",
                                    "name": "Sport and Recreation Spatial",
                                    "usagePatterns": "Reasonably large datasets, Small number of users. ",
                                    "useCase": "A spatial information infrastructure including Mapserver/Mapcache for the delivery of spatial information for the Sport and Recreation Spatial Project. http://www.sportandrecreationspatial.com.au/ Spatial Analysis of Sports Participation, Health and Well-being factors from Large National survey datasets"
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.5,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_TUGG",
                                    "usagePatterns": "The project will have many users interacting with an growing data set.  Contributions from those users will feed the data sets. The system is also a potential harvest end point for the HuNI VL.",
                                    "useCase": "The Ultimate Gig Guide (TUGG) tracks the history of the live music scene in Melbourne, as it evolved from organised dance hall events, to discos and the thriving pub music scene of today. The database consists of a variety of contemporary gig guides, capturing minute details of bands, performances and venues to create a unique dataset of Melbourne music history. The TUGG platform has the ability to manage content, expose it to researchers, students, and casual users, and eventually allow end users to contribute their own knowledge. It has been established through a university research grant and it represents a multi-institutional, interdisciplinary research effort. The database is built using lightweight, modern web frameworks: Django provides the database logic, while Twitter Bootstrap delivers a cross-browser, cross-device independent web interface. The Nectar Research Cloud has been a useful deployment host during the development phase, due to its quick allocation time. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.5,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_TUGG",
                                    "usagePatterns": "The project will have many users interacting with an growing data set.  Contributions from those users will feed the data sets. The system is also a potential harvest end point for the HuNI VL.",
                                    "useCase": "The Ultimate Gig Guide (TUGG) tracks the history of the live music scene in Melbourne, as it evolved from organised dance hall events, to discos and the thriving pub music scene of today. The database consists of a variety of contemporary gig guides, capturing minute details of bands, performances and venues to create a unique dataset of Melbourne music history. The TUGG platform has the ability to manage content, expose it to researchers, students, and casual users, and eventually allow end users to contribute their own knowledge. It has been established through a university research grant and it represents a multi-institutional, interdisciplinary research effort. The database is built using lightweight, modern web frameworks: Django provides the database logic, while Twitter Bootstrap delivers a cross-browser, cross-device independent web interface. The Nectar Research Cloud has been a useful deployment host during the development phase, due to its quick allocation time. "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.5,
                                    "institution": "deakin.edu.au",
                                    "name": "Deakin_TUGG",
                                    "usagePatterns": "The project will have many users interacting with an growing data set.  Contributions from those users will feed the data sets. The system is also a potential harvest end point for the HuNI VL.",
                                    "useCase": "The Ultimate Gig Guide (TUGG) tracks the history of the live music scene in Melbourne, as it evolved from organised dance hall events, to discos and the thriving pub music scene of today. The database consists of a variety of contemporary gig guides, capturing minute details of bands, performances and venues to create a unique dataset of Melbourne music history. The TUGG platform has the ability to manage content, expose it to researchers, students, and casual users, and eventually allow end users to contribute their own knowledge. It has been established through a university research grant and it represents a multi-institutional, interdisciplinary research effort. The database is built using lightweight, modern web frameworks: Django provides the database logic, while Twitter Bootstrap delivers a cross-browser, cross-device independent web interface. The Nectar Research Cloud has been a useful deployment host during the development phase, due to its quick allocation time. "
                                }
                            ],
                            "name": "160402"
                        }
                    ],
                    "name": "1604"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "adelaide.edu.au",
                                    "name": "International Dental Graduate Study",
                                    "usagePatterns": "The project is expected to have at least 6 core users, of which at least 2 expect to use the cloud intensively.  It will have both small data sets and a few large data sets. 1. Large datasets     a. two survey datasets with about 1000 records     b. one qualitative dataset with about 500,000 words 2. Small datasets     About 15-20 smaller datasets of about 200 records Can also hold other generic data from ABS - population, geographic data etc. This will depend on the capacity of the cloud, and what we got to offer. ",
                                    "useCase": "The IDG Study is a national study of all overseas qualified dentists in Australia. Team members are mainly based in the Universities of Adelaide (4 members) and Sydney (2 members). Other members of the team are outside the university enviornment and are based in state public dental services (2 in Adelaide), , dental councils (1 member in Melbourne) and associations (2 in Sydney).  The purpose of the cloud is to: 1. Communicate effectively with team members 2. Build an external mode (or something like a website) - through which we can disseminate information to the broader dentist community.      a. Where resources can be updated regularly     b. Where team members can share daily updates on some important issues 3. Data tables can be made available to the public and can be extracted from the cloud, but with some built in protocols.  4. Have the capacity to store large quantities of qualitative and quantitative data.. that can also be visualised externally to make sense.   "
                                }
                            ],
                            "name": "160508"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "Food Desert Index Computation For The Greater Melbourne Area",
                                    "usagePatterns": "For each case study dataset (e.g. the Greater Melbourne Areas / Supermarkets), the Mapquest API calling is the most time-consuming part, which will only be performed once. The food desert index computation and analysis could be carried out multiple times to produce a series of comparative results. ",
                                    "useCase": "These two instances will be deployed as parallel computation nodes using R SNOW package (Simple Network of Workstations). First, we plan to generate a travel information database by calling Mapquest Direction API to query the route information between 53771 census tracts (in ABS MeshBox spacial unit) and their nearby major supermarkets (700+ in total). Three different travel modes are concerned: walking, driving and public transport. Secondly, the analysis on the travelinfo db will be performed to generate \"food desert index\" for each tract area for each travel mode. Finally, a general index can be synthesised based on three mode index values."
                                }
                            ],
                            "name": "160514"
                        }
                    ],
                    "name": "1605"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "ballarat.edu.au",
                                    "name": "Sport and Recreation Spatial",
                                    "usagePatterns": "Reasonably large datasets, Small number of users. ",
                                    "useCase": "A spatial information infrastructure including Mapserver/Mapcache for the delivery of spatial information for the Sport and Recreation Spatial Project. http://www.sportandrecreationspatial.com.au/ Spatial Analysis of Sports Participation, Health and Well-being factors from Large National survey datasets"
                                }
                            ],
                            "name": "1603"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "HuNI_VL",
                                    "usagePatterns": "Small data sets and few users. Possible public access.",
                                    "useCase": "NeCTAR VL exemplar"
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "HuNI_VL",
                                    "usagePatterns": "Small data sets and few users. Possible public access.",
                                    "useCase": "NeCTAR VL exemplar"
                                }
                            ],
                            "name": "160303"
                        }
                    ],
                    "name": "1603"
                }
            ],
            "name": "16"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "anu.edu.au",
                                    "name": "ReDBox",
                                    "usagePatterns": "Multiple users will use this VM to test ReDBox workflows. I can assume that the high demand time would be during the business hours.",
                                    "useCase": "ANDS is collaborating with fourteen universities (e.g. RMIT, Newcastle, Sydney, etc.) to establish research metadata repositories based on the ReDBox software package. In order to assist universities to create the correct workflows, we need to create a demo ReDBox machine on the cloud and link it to the ANDS services and National Library of Australia (NLA) party infrastructure.  Adrian Burton suggested that we use NeCTAR research cloud for this purpose for two reasons: Firstly, Xiaobin and I have used ReDBox in a number of NeCTAR dojos, and we have received a strong interest from the community. Secondly, this will be a good opportunity for the research offices to gain benefit from the collaboration between NeCTAR and ANDS. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb Learning Analytics Research Group",
                                    "usagePatterns": "Detail not really known, but likely to be small number of users (<10).",
                                    "useCase": "Providing a MySQL DB for use by researchers to analyse the data generated from the Coursera online courses being run by the University."
                                }
                            ],
                            "name": "130199"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "ECAPS",
                                    "usagePatterns": "The site is currently used to access the video library and make annotations. A small number of users and intermittent use generating low levels of load. The dataset is approx. 2TB of video files.",
                                    "useCase": "The ECAPS project involves evidencing skills practice in the health sciences through video capture and annotation. A large number of videos of students performing clinical skills has been collected and is being analysed and annotated."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_Flipped_Classroom",
                                    "usagePatterns": "The instances will be used to capture interactions with a virtual learning environment of cohorts of students of up to 300. They will contribute files and resources, thus the need for significant storage usage. The dataset per course ",
                                    "useCase": "The deployment of active learnign and more precisely Flipped Classroom methodology requires the use of advanced personalisation techniques that shape a learning environment differently for every student. The project proposes the design of a platform to support personalisation in Flipped Classrooms."
                                }
                            ],
                            "name": "130103"
                        }
                    ],
                    "name": "1301"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.4,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_hmC_methylation",
                                    "usagePatterns": "The project will have large datasets but only a maximum of 5 users",
                                    "useCase": "We have a sequencing project between two departments that require a large amount of storage room for sequencing datasets. A student and myself will be using it to analyse DNA methylation states. The student is split between multiple campuses and has little experience with linux, so therefore an appropriately resourced virtual machine will help him immensely."
                                }
                            ],
                            "name": "139999"
                        }
                    ],
                    "name": "1399"
                }
            ],
            "name": "13"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "StatGenMCRI",
                                    "usagePatterns": "Small number of users (approx. 5), small data sets (mostly code).  Test data (small versions of our main datasets).  Main computation and data storage will use other resource we have access to.",
                                    "useCase": "Host source code repositories (using e.g. GIT) for access by reserch group and external collaborators.  Data sharing.  Running ad hoc web services for research tools (e.g. our custom tools).  We were not sure about how to calculate the number of core hours.  We have just assumes a full year of use of one core with occasional use of a second core."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Food Desert Index Computation For The Greater Melbourne Area",
                                    "usagePatterns": "For each case study dataset (e.g. the Greater Melbourne Areas / Supermarkets), the Mapquest API calling is the most time-consuming part, which will only be performed once. The food desert index computation and analysis could be carried out multiple times to produce a series of comparative results. ",
                                    "useCase": "These two instances will be deployed as parallel computation nodes using R SNOW package (Simple Network of Workstations). First, we plan to generate a travel information database by calling Mapquest Direction API to query the route information between 53771 census tracts (in ABS MeshBox spacial unit) and their nearby major supermarkets (700+ in total). Three different travel modes are concerned: walking, driving and public transport. Secondly, the analysis on the travelinfo db will be performed to generate \"food desert index\" for each tract area for each travel mode. Finally, a general index can be synthesised based on three mode index values."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_RTrack01",
                                    "usagePatterns": "Large data sets with a small number of users, tens of gigabytes and only 2-3 users for this proof-of-concept. ",
                                    "useCase": "Model runs of light-level geo-location estimation from marine animals (elephant seals, snow petrels) using  Bayesian methods. These methods are currently being updated and we are exploring parallelization options. This instance would be a small test bed to demonstrate a proof of concept. It could ultimately be an R server service to run these models for researchers at the AAD, IMAS and the ACE CRC. "
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.2,
                                    "institution": "utas.edu.au",
                                    "name": "UTas_RTrack01",
                                    "usagePatterns": "Large data sets with a small number of users, tens of gigabytes and only 2-3 users for this proof-of-concept. ",
                                    "useCase": "Model runs of light-level geo-location estimation from marine animals (elephant seals, snow petrels) using  Bayesian methods. These methods are currently being updated and we are exploring parallelization options. This instance would be a small test bed to demonstrate a proof of concept. It could ultimately be an R server service to run these models for researchers at the AAD, IMAS and the ACE CRC. "
                                }
                            ],
                            "name": "010401"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "versi.edu.au",
                                    "name": "VeRSI DTN S-DMZ service monitoring",
                                    "usagePatterns": "",
                                    "useCase": "To monitor DTN S-DMZ service nodes"
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_Peter_Mac_Research_Dropbox",
                                    "usagePatterns": "I anticipate predominantly large data sets but with not many users.",
                                    "useCase": "Peter Mac has many collaborations with other local, national and international institutes and we currently often use dropbox (www.dropbox.com) to share data and collaborate. The free offering is very limited however. A use case is that Tony Papenfuss has now joined Peter Mac and he requires an easy way to access data at multiple locations (home, WEHI and Peter Mac) while also sharing the data with his collaborators and labs at both Peter Mac and WEHI."
                                }
                            ],
                            "name": "0104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "monash.edu",
                                    "name": "Monash MCTS Server",
                                    "usagePatterns": "***UPDATED REQUEST*** The amount of data being produced by experiments for this project is far larger than anticipated at inception. As such, I have amended this request to add 2TB of volume storage for this project. ***END UPDATED REQUEST*** As this server will be running a batch of tasks, it is anticipated that this instance will be running at high levels of activity and I/O for processor, RAM and local storage on a fairly \"flat\" usage pattern for the duration of this time. ",
                                    "useCase": "***UPDATED REQUEST*** Positive results within the initial experiment runs has led to an expansion of test scenarios to incorporate a wider range of data. As such, a larger amount of test data is being produced. This instance is now being used to store and process test data using MongoDB. ***UPDATED REQUEST ENDS*** This instance will be used to implement and assess the efficacy of numerous ranking algorithms in the context of search across large, hierarchical datasets.  In this case, a 7 million plus corpus of emails taken from an open source (Apache Software Foundation) will be used as the corpus. Attempts to date using available computing hardware have intermittently failed due to RAM constraints (16GB per node) "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "monash.edu",
                                    "name": "Forensic Data Ranking and Analysis",
                                    "usagePatterns": "Very large dataset, one to two users typically (maximum three concurrent users).",
                                    "useCase": "Collection, indexing and analysis of geo-tagged multimedia.    Our goal is to develop a ranking and analysis tool, primarily focussed on acceleration of existing multimedia forensics analytics.   This research will result in a prototype tool intended for use by international law enforcement agencies for multi-jurisdictional victim identification."
                                }
                            ],
                            "name": "010403"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "utas.edu.au",
                                    "name": "UTas Climate Change and Health Adaptions",
                                    "usagePatterns": "At this stage intermitent 20 people accessing in course.",
                                    "useCase": "A/Prof  Paul Turner paul.turner@utas.edu.au A/Prof Erica Bell Erica.Bell@utas.edu.au Climate Change and Rural  Health Risk Assessment project  online portal including quiz tool."
                                }
                            ],
                            "name": "010499"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "griffith.edu.au",
                                    "name": "Genomics",
                                    "usagePatterns": "",
                                    "useCase": "Genomics and Bioinformatics Research - data management/access front end"
                                },
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "griffith.edu.au",
                                    "name": "Genomics",
                                    "usagePatterns": "",
                                    "useCase": "Genomics and Bioinformatics Research - data management/access front end"
                                }
                            ],
                            "name": "010402"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin_S&P500",
                                    "usagePatterns": "User: One Number of High Frequency data sets: 21 files each exceeding 1Gb after expanding. Number of Day-close data sets: Currently one file of size 5Mb for one month of day close data. There will be data from 1990 to 2013 coming soon, making a total of approximately 276 files for each of the months therein, making a total of 1.38Gb of day close data. Data usage pattern: All data will be loaded in parallel processors for partitioned computation abd subsequently the results concatenated for final processing. ",
                                    "useCase": "I have been part of the Curtin_IS pilot project and are using 3 x 16 CPU nodes as part of my PhD thesis project.  The data being analysed consists of high frequency equity options trading data. Currently one month worth of data is available for analysis immediately. It consists of 30Gb of data.  During the second phase of the numerical analysis, I envisage more data, possibly equity options data in S&P500 index traded on CBOE from 1996 to 2012 will be used for the analysis, requiring more storage space as well as more threads as currently available to be for parallel computation.  As it stands, the numerical estimation procedure employs differential evolution implemented in R as package DEoptim that utilize parallel implementation via MPI framework and has been up and running in the Curtin_IS pilot project framework. On 48 parallel cores, one set of simulation, based on one subset of data, took 24 hours to complete.    During the phase two of the computation, the number of equations to be solved simultaneously using MPI-based parallel R (package foreach and its related parallel engines) would be 4 to 5 times in the initial phase. In addition, the amount of data to be handled will be much larger. It will be immensely helpful if more parallel core and storage space would be made available to help me complete the numerical calculation necessary for my PhD thesis project. "
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin_S&P500",
                                    "usagePatterns": "User: One Number of High Frequency data sets: 21 files each exceeding 1Gb after expanding. Number of Day-close data sets: Currently one file of size 5Mb for one month of day close data. There will be data from 1990 to 2013 coming soon, making a total of approximately 276 files for each of the months therein, making a total of 1.38Gb of day close data. Data usage pattern: All data will be loaded in parallel processors for partitioned computation abd subsequently the results concatenated for final processing. ",
                                    "useCase": "I have been part of the Curtin_IS pilot project and are using 3 x 16 CPU nodes as part of my PhD thesis project.  The data being analysed consists of high frequency equity options trading data. Currently one month worth of data is available for analysis immediately. It consists of 30Gb of data.  During the second phase of the numerical analysis, I envisage more data, possibly equity options data in S&P500 index traded on CBOE from 1996 to 2012 will be used for the analysis, requiring more storage space as well as more threads as currently available to be for parallel computation.  As it stands, the numerical estimation procedure employs differential evolution implemented in R as package DEoptim that utilize parallel implementation via MPI framework and has been up and running in the Curtin_IS pilot project framework. On 48 parallel cores, one set of simulation, based on one subset of data, took 24 hours to complete.    During the phase two of the computation, the number of equations to be solved simultaneously using MPI-based parallel R (package foreach and its related parallel engines) would be 4 to 5 times in the initial phase. In addition, the amount of data to be handled will be much larger. It will be immensely helpful if more parallel core and storage space would be made available to help me complete the numerical calculation necessary for my PhD thesis project. "
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "qut.edu.au",
                                    "name": "Twitter Data Collection",
                                    "usagePatterns": "These machines will run a web accessible front-end for data collection specification, and a mySQL/php script backend responsible for data collection. While admin/ssh access will be restricted to two accounts, there will be a number of users across several institutions using the front end (25-50 users). The machines will run a mySQL database for live metrics and data collection, which will periodically be backed up to local storage.",
                                    "useCase": "These machines will be responsible for social media data collection for a variety of current projects, including but not limited to crisis communication, political conversation, scientific communication, dissemination of news and conversation around entertainment (to include television, movies, games and social networks). This will feed into ongoing research on the above subjects focused on creating metrics to measure audience engagement and participation. For more details see http://www.mappingonlinepublics.com and http://www.tvmetrics.net"
                                }
                            ],
                            "name": "010406"
                        }
                    ],
                    "name": "0104"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Cotton Bollworm Transcriptome",
                                    "usagePatterns": "One user, many small data sets.",
                                    "useCase": "I'm going to use for running tools for sequencing. Basically, top-hat, cufflink, and fastqc and stuff. I'll also use for running Linux based software for sequencing."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "griffith.edu.au",
                                    "name": "protein folding",
                                    "usagePatterns": "it have large data sets with a small number of users?",
                                    "useCase": "protein structure prediction, protein design, protein function prediction"
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "MUSP Scheduler",
                                    "usagePatterns": "Windows Server need Q3.large flavour",
                                    "useCase": "Longitudinal epidemiological study following up 8000 mothers who gave birth, and their children) at the Mater Hospital Brisbane between 1981 and 1984. Current follow up of children is the tenth since the first clinical visit. Current phase MUSP30 collects demographic, socio-economic, physical, medical and psychological measures as well as physical samples for genetic research. "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "protein folding",
                                    "usagePatterns": "it have large data sets with a small number of users?",
                                    "useCase": "protein structure prediction, protein design, protein function prediction"
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 2.4,
                                    "institution": "uq.edu.au",
                                    "name": "Neural Netetwork Complex Dynamics Parameter Search",
                                    "usagePatterns": "This project will most likely continually run one virtual machine with the requested persistent volume storage attached.  This virtual machine will be used for debugging test experiments and storing data.  For larger experiments, a number of virtual machines will be spawned for the duration of a simulation and terminated after the job has been run.  This dynamic allocation approach aims to minimize the amount of resources that are continually being used, while greatly increasing the computational capacity for simulating neural networks.",
                                    "useCase": "The Complex and Intelligent Systems group (led by Prof J Wiles in ITEE) simulates a range of computational models of biological phenomena. The group is seeking computational resources to support a number of these projects.  I am currently conducting neural network simulations to study complex network activity across wide ranges of input parameters.  I intend to use these resources for two aspects of my project.  1) to increase the number of parameter sets I am able to simulate simultaneously. I am currently limited to 8 simultaneous simulations on my work desktop.   and 2) to increase the size of simulated networks.  I am currently limited to ~250 neurons, while some complex network dynamics only emerge with greater than ~1000 neurons.  The brain is able to continually change its activity state in response to internal representations and external stimuli.  This body of work aims to elucidate how various neural architectures are able to self-regulate these changes in activity states. Specifically, we would like to study dynamical switching mechanisms in neural networks that exhibit complex self-sustained activity.  Some of the neural regions of interest have over a million neurons and we expect that simulations siees will continue to improve with the development of modelling techniques coupled to increases in computational power."
                                }
                            ],
                            "name": "010202"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin_S&P500",
                                    "usagePatterns": "User: One Number of High Frequency data sets: 21 files each exceeding 1Gb after expanding. Number of Day-close data sets: Currently one file of size 5Mb for one month of day close data. There will be data from 1990 to 2013 coming soon, making a total of approximately 276 files for each of the months therein, making a total of 1.38Gb of day close data. Data usage pattern: All data will be loaded in parallel processors for partitioned computation abd subsequently the results concatenated for final processing. ",
                                    "useCase": "I have been part of the Curtin_IS pilot project and are using 3 x 16 CPU nodes as part of my PhD thesis project.  The data being analysed consists of high frequency equity options trading data. Currently one month worth of data is available for analysis immediately. It consists of 30Gb of data.  During the second phase of the numerical analysis, I envisage more data, possibly equity options data in S&P500 index traded on CBOE from 1996 to 2012 will be used for the analysis, requiring more storage space as well as more threads as currently available to be for parallel computation.  As it stands, the numerical estimation procedure employs differential evolution implemented in R as package DEoptim that utilize parallel implementation via MPI framework and has been up and running in the Curtin_IS pilot project framework. On 48 parallel cores, one set of simulation, based on one subset of data, took 24 hours to complete.    During the phase two of the computation, the number of equations to be solved simultaneously using MPI-based parallel R (package foreach and its related parallel engines) would be 4 to 5 times in the initial phase. In addition, the amount of data to be handled will be much larger. It will be immensely helpful if more parallel core and storage space would be made available to help me complete the numerical calculation necessary for my PhD thesis project. "
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin_S&P500",
                                    "usagePatterns": "User: One Number of High Frequency data sets: 21 files each exceeding 1Gb after expanding. Number of Day-close data sets: Currently one file of size 5Mb for one month of day close data. There will be data from 1990 to 2013 coming soon, making a total of approximately 276 files for each of the months therein, making a total of 1.38Gb of day close data. Data usage pattern: All data will be loaded in parallel processors for partitioned computation abd subsequently the results concatenated for final processing. ",
                                    "useCase": "I have been part of the Curtin_IS pilot project and are using 3 x 16 CPU nodes as part of my PhD thesis project.  The data being analysed consists of high frequency equity options trading data. Currently one month worth of data is available for analysis immediately. It consists of 30Gb of data.  During the second phase of the numerical analysis, I envisage more data, possibly equity options data in S&P500 index traded on CBOE from 1996 to 2012 will be used for the analysis, requiring more storage space as well as more threads as currently available to be for parallel computation.  As it stands, the numerical estimation procedure employs differential evolution implemented in R as package DEoptim that utilize parallel implementation via MPI framework and has been up and running in the Curtin_IS pilot project framework. On 48 parallel cores, one set of simulation, based on one subset of data, took 24 hours to complete.    During the phase two of the computation, the number of equations to be solved simultaneously using MPI-based parallel R (package foreach and its related parallel engines) would be 4 to 5 times in the initial phase. In addition, the amount of data to be handled will be much larger. It will be immensely helpful if more parallel core and storage space would be made available to help me complete the numerical calculation necessary for my PhD thesis project. "
                                }
                            ],
                            "name": "010205"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "griffith.edu.au",
                                    "name": "3D WSSI modelling",
                                    "usagePatterns": "The project will have large data sets with a small number of users",
                                    "useCase": "we are modeling the couple effects of wave/currents and seabed and structure interactions in 3D."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "3D WSSI modelling",
                                    "usagePatterns": "The project will have large data sets with a small number of users",
                                    "useCase": "we are modeling the couple effects of wave/currents and seabed and structure interactions in 3D."
                                }
                            ],
                            "name": "010204"
                        }
                    ],
                    "name": "0102"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "griffith.edu.au",
                                    "name": "HPC Workflows",
                                    "usagePatterns": "It's a HPC workflows Project so it's more likely to have large data sets, and at least initially, a small number of users.",
                                    "useCase": "We (eResearch Services, Griffith Uni) are currently developing a user-friendly HPC job submission web portal for researchers. Currently we have NAMD (and shortly MatLab) jobs running from a web portal through NIMROD to our local cluster. Nimrod is able to use cloud bursting and we would like to have more than 2 CPU's available to test and improve this workflow."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "cqu.edu.au",
                                    "name": "Access Grid International Website Hosting",
                                    "usagePatterns": "Most of the content that will be accessed is in the simple form of webpages.  Additional to this, the site will also provide the Access Grid Software downloads (similar to http://www.accessgrid.org/software).  As an example, the latest Windows Bundle installer for the Access Grid Software is 75MB in size.",
                                    "useCase": "I would like to request a VM which will help support the Australian (and International) Access Grid Community. As you might not be aware, ANL withdrew support for the open source Access Grid project.  From this, the International community, as well as leveraging open services such as sourceforge, combined to provide resources to continue the Access Grid Project. One of these services included the International Access Grid Web Site (See http://www.accessgrid.org/ - which currently runs on Drupal).  WestGrid (Canada) currently provides this service on a temporary basis, until a new permanent location/website could be arranged.  The server infrastructure currently hosting the website is reaching end of life and WestGrid can no longer provide this service and host this website. Though the Access Grid is slowly losing support  particularly internationally, there is still significant usage (and even new uptake  especially with AMSI [Australian Mathematical Sciences Institute]) and it would be good if this Research and Teaching Tool could continue to be supported. There are a number of us within the international community who would be happy to setup and support the servers, we just need the infrastructure to host the international website. "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "cqu.edu.au",
                                    "name": "ReDBox 2",
                                    "usagePatterns": "One of our machines that we are migrating is dev.redboxresearchdata.com.au and that is currently hosting our jenkins build server (https://dev.redboxresearchdata.com.au/jenkins/) as well as a Nexus artifact repository that contains versions of our ReDBox and Mint institutional builds as well as caches key libraries we need to build (http://dev.redboxresearchdata.com.au/nexus/)  This machine will need to be relatively powerful and have a decent amount of storage space. I believe 4 cores will be enough for this machine but to storage will be a problem. The current machine has 100GB of disc space but we are down to our last 10GB and on current trends we would be out of space in a few months. Because of this I'm thinking we'd have one extra-large vm and the rest all we would need are small.",
                                    "useCase": "We have had some virtual machines hosted on QERN that we need to be migrated as they are no longer being supported. Additionally we have some projects starting up for RDSI and others that we need to run test and demonstration environments for."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Genomics QERN migrate",
                                    "usagePatterns": "",
                                    "useCase": "Genomics and Bioinformatics Want to migrate QERN data to this VM"
                                }
                            ],
                            "name": "01"
                        }
                    ],
                    "name": "01"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 11.2,
                                    "instanceQuota": 11.2,
                                    "institution": "sydney.edu.au",
                                    "name": "VL201 Industrial Ecology",
                                    "usagePatterns": "Initial usage is 15, with a maximum of 8 concurrent, final numbers could be up to 65 users nationally. Datasets are comprised of large files of statistical data from various sources e.g. ABS, Water usage, Waste data. The file sizes are significant with only 5 datasets  using the initial 10gb allocated to the project.",
                                    "useCase": "The Industrial Ecology project currently has a Medium environment as a pilot and there are insufficient RAM to run the processes to reach the initial datafeed milestones or consequent milestones that require further processing. Emails from the business sponsor and technical lead of the project can be forwarded as evidence of lack of RAM if required. Basic requirements from Technical lead are below: We need to operate a web server, a storage for a potentially large number of different MRIOs and the computing capability to run the analysis. If we run all of that stuff on the cloud, we need more crunch. If not, we should explore different solutions soon. For example that mass-storage facility at UQ. The homepage that we are running here for our global model has its own, dedicated machine with 8 cores, 1 TB of storage and 64 GB or RAM. I reckon for this project we would need at least the same. Kind regards Neal"
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "sydney.edu.au",
                                    "name": "SYD-IndEcologyVL",
                                    "usagePatterns": "Large data sets from a medium group of users which is growing on a weekly basis, ie, two new PHDs in May - June.",
                                    "useCase": "The team requires a storage allocation of 5tb to enable testign and work to continue as the existing 480gb has been consumed and it is estimated that at least 3tb will be in use by go-live December 2013. Hi Neal We're hitting constant disk-space problems - this problem has now become urgent - we need more space. Thanks Manfred [cid:E0E9A103-BEE5-4644-9E87-BDD1D77D24A7] Read our article in Nature: http://www.nature.com/nature/journal/v486/n7401/full/nature11145.html Begin forwarded message: From: Arne Geschke > Subject: disc full Date: May 31, 2013 6:50:12 AM GMT+10:00 To: Manfred Lenzen >, Arunima Malik >, Daniel Moran >, Joe Lane > Hi all, I've changed the upload code to adjust the rights of the uploaded file so that everybody can now delete their work directory. So you shouldn't need my help anymore to delete it. I've noticed that the disc is - again - completely full. Dan has got one job that is currently processing on scarlett, I don't know what will happen when that hits the upload queue. I can stop the uploader to avoid any problems. In that case, the data would remain on scarlett until the uploader is restarted. Dan, if you like, you can keep that data of this run on scarlett if that helps. Please advise/delete data from your directory. Thanks, Arne Dr Arne Geschke | Postdoctoral Fellow Integrated Sustainability Analysis School of Physics | Faculty of Science THE UNIVERSITY OF SYDNEY Rm 501, School of Physics A28 | Physics Road | The University of Sydney | NSW | 2006 T +61 2 9036 7505 | F +61 2 9351 7726 E arne.geschke@sydney.edu.au | W http://www.isa.org.usyd.edu.au "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "sydney.edu.au",
                                    "name": "SYD-IndEcologyVL",
                                    "usagePatterns": "Large data sets from a medium group of users which is growing on a   weekly basis, ie, two new PHDs in May - June.   ",
                                    "useCase": "The team requires a storage allocation of 5tb to enable testing and  > work to continue as the existing 480gb has been consumed and it is  > estimated that at least 3tb will be in use by go-live December 2013. >  > Hi Neal > We&#39;re hitting constant disk-space problems - this problem has now  > become urgent - we need more space. > Thanks > Manfred > [cid:E0E9A103-BEE5-4644-9E87-BDD1D77D24A7] > Read our article in Nature: > http://www.nature.com/nature/journal/v486/n7401/full/nature11145.html > Begin forwarded message: > From: Arne Geschke &gt; > Subject: disc full > Date: May 31, 2013 6:50:12 AM GMT+10:00 > To: Manfred Lenzen &gt;, Arunima Malik &gt;, Daniel Moran &gt;, Joe  > Lane &gt; Hi all, I&#39;ve changed the upload code to adjust the  > rights of the uploaded file so that everybody can now delete their  > work directory. So you shouldn&#39;t need my help anymore to delete it. > I&#39;ve noticed that the disc is - again - completely full. Dan has  > got one job that is currently processing on scarlett, I don&#39;t know  > what will happen when that hits the upload queue. I can stop the  > uploader to avoid any problems. In that case, the data would remain on  > scarlett until the uploader is restarted. Dan, if you like, you can  > keep that data of this run on scarlett if that helps. > Please advise/delete data from your directory. > Thanks, > Arne > Dr Arne Geschke | Postdoctoral Fellow > Integrated Sustainability Analysis > School of Physics | Faculty of Science THE UNIVERSITY OF SYDNEY Rm  > 501, School of Physics A28 | Physics Road | The University of Sydney |  > NSW | 2006 T +61 2 9036 7505 | F +61 2 9351 7726 E  > arne.geschke@sydney.edu.au | W http://www.isa.org.usyd.edu.au >  >  > The usage pattern is: > Large data sets from a medium group of users which is growing on a  > weekly basis, ie, two new PHDs in May - June. >  > And the geographic requirements are: > Can be any available node, however, proximity to Massive would be  > preferred. >  > The tenant has specified the breakdown of their Fields Of Research as: >  > 0103 NUMERICAL AND COMPUTATIONAL MATHEMATICS (30%) >  >  > 0502 ENVIRONMENTAL SCIENCE AND MANAGEMENT (40%) >  >  > 0806 INFORMATION SYSTEMS (30%) >  >  > Yours, > the rcDashBoard. "
                                },
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.3,
                                    "institution": "uq.edu.au",
                                    "name": "MatlabEAIT",
                                    "usagePatterns": "Usage pattern will change over time, but will envisage small data sets with just myself as the user.",
                                    "useCase": "Evaluating Windows Matlab performance and firewall issues including licensing and connectivity to UQ EAIT file server .  "
                                }
                            ],
                            "name": "0103"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "griffith.edu.au",
                                    "name": "Interactive Shape Optimisation",
                                    "usagePatterns": "About 30 simulations will be conducted over a small number of weeks. Each simulation will require roughly 300 CPU hours.  The instance will be used for a few hours at a time and shut down in between runs. User interaction will require some communication to an external server with a negligible amount of data being transmitted. ",
                                    "useCase": "The Instance will be used as part of a case study on using Interactive Multi-Objective Particle Swarm Optimisation on a Jet Engine Compressor Blade design problem. The instance will be used for the computational fluid dynamics simulation tool and the algorithm. The requested number of core hours represents an upper limit and will likely be less."
                                },
                                {
                                    "coreQuota": 120.0,
                                    "instanceQuota": 120.0,
                                    "institution": "monash.edu",
                                    "name": "Plexos-Nimrod",
                                    "usagePatterns": "",
                                    "useCase": "This research concerns the identification of optimal investment configurations for renewable energy resources in a large and complex power grid. It is tacked through analysis and simulation of electricity networks using a mixture of time-sequential simulations and global optimisation of the parameters of these simulations using Nimrod/O. The outcomes will provide a better understanding of how the topology of a power system affects the selection of technology types and the lowest cost investment solution. It can be used to inform policy makers on distributed generation market design and will form part of the research in the CSIRO's Future Grid flagship collaboration cluster: www.futuregrid.org.au"
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 0.5,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Timetable Optimisation",
                                    "usagePatterns": "Access to the VM 's web server is essential; ideally via a remote web browser. Eg, port forwarding.",
                                    "useCase": "The TRC (Matthew Britton) has been working with Paul Duldig (UoA VP services & resources) and Virginia Deegan to optimise and experiment with the University of Adelaide timetable, using open-source software called UniTime. Until now we had only been working with a fraction of the entire UoA student body and course offerings, but we are now ready for the whole database. However, the computing requirements (32++ GB of RAM) of the task running in UniTime exceeds anything that is available to us (16 GB)."
                                },
                                {
                                    "coreQuota": 120.0,
                                    "instanceQuota": 120.0,
                                    "institution": "monash.edu",
                                    "name": "Plexos-Nimrod",
                                    "usagePatterns": "",
                                    "useCase": "This research concerns the identification of optimal investment configurations for renewable energy resources in a large and complex power grid. It is tacked through analysis and simulation of electricity networks using a mixture of time-sequential simulations and global optimisation of the parameters of these simulations using Nimrod/O. The outcomes will provide a better understanding of how the topology of a power system affects the selection of technology types and the lowest cost investment solution. It can be used to inform policy makers on distributed generation market design and will form part of the research in the CSIRO's Future Grid flagship collaboration cluster: www.futuregrid.org.au"
                                },
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "monash.edu",
                                    "name": "Meta-Batch-Scheduler Testing",
                                    "usagePatterns": "Small number of Users with small data sets. one or two sql and/or ldap servers.",
                                    "useCase": "Monash eResearch Centre's HPC division is developing a new Nimrod meta-batch-scheduler, which will submit jobs to multiple batch-schedulers based on Network and Hardware Locality. We need to test the prototype on small cluster.  "
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 25.6,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin_S&P500",
                                    "usagePatterns": "User: One Number of High Frequency data sets: 21 files each exceeding 1Gb after expanding. Number of Day-close data sets: Currently one file of size 5Mb for one month of day close data. There will be data from 1990 to 2013 coming soon, making a total of approximately 276 files for each of the months therein, making a total of 1.38Gb of day close data. Data usage pattern: All data will be loaded in parallel processors for partitioned computation abd subsequently the results concatenated for final processing. ",
                                    "useCase": "I have been part of the Curtin_IS pilot project and are using 3 x 16 CPU nodes as part of my PhD thesis project.  The data being analysed consists of high frequency equity options trading data. Currently one month worth of data is available for analysis immediately. It consists of 30Gb of data.  During the second phase of the numerical analysis, I envisage more data, possibly equity options data in S&P500 index traded on CBOE from 1996 to 2012 will be used for the analysis, requiring more storage space as well as more threads as currently available to be for parallel computation.  As it stands, the numerical estimation procedure employs differential evolution implemented in R as package DEoptim that utilize parallel implementation via MPI framework and has been up and running in the Curtin_IS pilot project framework. On 48 parallel cores, one set of simulation, based on one subset of data, took 24 hours to complete.    During the phase two of the computation, the number of equations to be solved simultaneously using MPI-based parallel R (package foreach and its related parallel engines) would be 4 to 5 times in the initial phase. In addition, the amount of data to be handled will be much larger. It will be immensely helpful if more parallel core and storage space would be made available to help me complete the numerical calculation necessary for my PhD thesis project. "
                                },
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 25.6,
                                    "institution": "curtin.edu.au",
                                    "name": "Curtin_S&P500",
                                    "usagePatterns": "User: One Number of High Frequency data sets: 21 files each exceeding 1Gb after expanding. Number of Day-close data sets: Currently one file of size 5Mb for one month of day close data. There will be data from 1990 to 2013 coming soon, making a total of approximately 276 files for each of the months therein, making a total of 1.38Gb of day close data. Data usage pattern: All data will be loaded in parallel processors for partitioned computation abd subsequently the results concatenated for final processing. ",
                                    "useCase": "I have been part of the Curtin_IS pilot project and are using 3 x 16 CPU nodes as part of my PhD thesis project.  The data being analysed consists of high frequency equity options trading data. Currently one month worth of data is available for analysis immediately. It consists of 30Gb of data.  During the second phase of the numerical analysis, I envisage more data, possibly equity options data in S&P500 index traded on CBOE from 1996 to 2012 will be used for the analysis, requiring more storage space as well as more threads as currently available to be for parallel computation.  As it stands, the numerical estimation procedure employs differential evolution implemented in R as package DEoptim that utilize parallel implementation via MPI framework and has been up and running in the Curtin_IS pilot project framework. On 48 parallel cores, one set of simulation, based on one subset of data, took 24 hours to complete.    During the phase two of the computation, the number of equations to be solved simultaneously using MPI-based parallel R (package foreach and its related parallel engines) would be 4 to 5 times in the initial phase. In addition, the amount of data to be handled will be much larger. It will be immensely helpful if more parallel core and storage space would be made available to help me complete the numerical calculation necessary for my PhD thesis project. "
                                }
                            ],
                            "name": "010303"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "anu.edu.au",
                                    "name": "3dtransformation",
                                    "usagePatterns": "",
                                    "useCase": "To do a 3D homoemorphism transformation, which requirs parrellel computation to speed up the algorithm. The transformation algorithm will run on the cloud and the image will be displayed in real-time sense. So I need at least 4 CPUs to do the computation, while this option is not available. Hence I request an XL instance. "
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 1.0,
                                    "institution": "anu.edu.au",
                                    "name": "3dtransformation",
                                    "usagePatterns": "",
                                    "useCase": "To do a 3D homoemorphism transformation, which requirs parrellel computation to speed up the algorithm. The transformation algorithm will run on the cloud and the image will be displayed in real-time sense. So I need at least 4 CPUs to do the computation, while this option is not available. Hence I request an XL instance. "
                                }
                            ],
                            "name": "010399"
                        }
                    ],
                    "name": "0103"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "monash.edu",
                                    "name": "Monash Latin Squares",
                                    "usagePatterns": "One user.  Medium/small amount of data generated by the program - less than 3 million lines of plain text in total.",
                                    "useCase": "Run experiment via Monash interface.  Site contacts Philip Chan and Shahaan Ayyub.  This project is a continuation of my projects which were previously run on the Monash SPONGE facility.  "
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "SAT Competition",
                                    "usagePatterns": " small data sets with small number of users",
                                    "useCase": "Stochastic local search satisfiability solvers from the GSAT family suffer from the cycling problem seriously on both huge random instances and crafted ones. While the configuration checking strategy handles this prob- lem considerably, it still solves them poorly, in terms of both run times and numbers of flips. In this paper we have developed a GSAT-based solver based on the no- tions of penult age and k-distance variable that prevent cycles in a novel locking and unlocking mechanism, to- gether with a hash heuristic as an auxiliary. On random 3-SAT benchmarks our solver outstrips all others on most instances, and surpasses other GSAT-based solvers by orders of magnitude on huge instances. Moreover, on crafted benchmarks we have improved the solving ca- pabilities of local search solvers, and solved much more instances than others in some domains. So far our strate- gies have significantly improved a current best solver on random 5-SAT benchmarks. Usually our solver needs much less flips than all others, which implies that our heuristic is the most informative and closest to the in- sight of most instances, and it has solved the cycling problem to a much greater extent."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "SAT Competition",
                                    "usagePatterns": " small data sets with small number of users",
                                    "useCase": "Stochastic local search satisfiability solvers from the GSAT family suffer from the cycling problem seriously on both huge random instances and crafted ones. While the configuration checking strategy handles this prob- lem considerably, it still solves them poorly, in terms of both run times and numbers of flips. In this paper we have developed a GSAT-based solver based on the no- tions of penult age and k-distance variable that prevent cycles in a novel locking and unlocking mechanism, to- gether with a hash heuristic as an auxiliary. On random 3-SAT benchmarks our solver outstrips all others on most instances, and surpasses other GSAT-based solvers by orders of magnitude on huge instances. Moreover, on crafted benchmarks we have improved the solving ca- pabilities of local search solvers, and solved much more instances than others in some domains. So far our strate- gies have significantly improved a current best solver on random 5-SAT benchmarks. Usually our solver needs much less flips than all others, which implies that our heuristic is the most informative and closest to the in- sight of most instances, and it has solved the cycling problem to a much greater extent."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "SAT Competition",
                                    "usagePatterns": " small data sets with small number of users",
                                    "useCase": "Stochastic local search satisfiability solvers from the GSAT family suffer from the cycling problem seriously on both huge random instances and crafted ones. While the configuration checking strategy handles this prob- lem considerably, it still solves them poorly, in terms of both run times and numbers of flips. In this paper we have developed a GSAT-based solver based on the no- tions of penult age and k-distance variable that prevent cycles in a novel locking and unlocking mechanism, to- gether with a hash heuristic as an auxiliary. On random 3-SAT benchmarks our solver outstrips all others on most instances, and surpasses other GSAT-based solvers by orders of magnitude on huge instances. Moreover, on crafted benchmarks we have improved the solving ca- pabilities of local search solvers, and solved much more instances than others in some domains. So far our strate- gies have significantly improved a current best solver on random 5-SAT benchmarks. Usually our solver needs much less flips than all others, which implies that our heuristic is the most informative and closest to the in- sight of most instances, and it has solved the cycling problem to a much greater extent."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "SAT Competition",
                                    "usagePatterns": " small data sets with small number of users",
                                    "useCase": "Stochastic local search satisfiability solvers from the GSAT family suffer from the cycling problem seriously on both huge random instances and crafted ones. While the configuration checking strategy handles this prob- lem considerably, it still solves them poorly, in terms of both run times and numbers of flips. In this paper we have developed a GSAT-based solver based on the no- tions of penult age and k-distance variable that prevent cycles in a novel locking and unlocking mechanism, to- gether with a hash heuristic as an auxiliary. On random 3-SAT benchmarks our solver outstrips all others on most instances, and surpasses other GSAT-based solvers by orders of magnitude on huge instances. Moreover, on crafted benchmarks we have improved the solving ca- pabilities of local search solvers, and solved much more instances than others in some domains. So far our strate- gies have significantly improved a current best solver on random 5-SAT benchmarks. Usually our solver needs much less flips than all others, which implies that our heuristic is the most informative and closest to the in- sight of most instances, and it has solved the cycling problem to a much greater extent."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "SAT Competition",
                                    "usagePatterns": " small data sets with small number of users",
                                    "useCase": "Stochastic local search satisfiability solvers from the GSAT family suffer from the cycling problem seriously on both huge random instances and crafted ones. While the configuration checking strategy handles this prob- lem considerably, it still solves them poorly, in terms of both run times and numbers of flips. In this paper we have developed a GSAT-based solver based on the no- tions of penult age and k-distance variable that prevent cycles in a novel locking and unlocking mechanism, to- gether with a hash heuristic as an auxiliary. On random 3-SAT benchmarks our solver outstrips all others on most instances, and surpasses other GSAT-based solvers by orders of magnitude on huge instances. Moreover, on crafted benchmarks we have improved the solving ca- pabilities of local search solvers, and solved much more instances than others in some domains. So far our strate- gies have significantly improved a current best solver on random 5-SAT benchmarks. Usually our solver needs much less flips than all others, which implies that our heuristic is the most informative and closest to the in- sight of most instances, and it has solved the cycling problem to a much greater extent."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "SAT Competition",
                                    "usagePatterns": " small data sets with small number of users",
                                    "useCase": "Stochastic local search satisfiability solvers from the GSAT family suffer from the cycling problem seriously on both huge random instances and crafted ones. While the configuration checking strategy handles this prob- lem considerably, it still solves them poorly, in terms of both run times and numbers of flips. In this paper we have developed a GSAT-based solver based on the no- tions of penult age and k-distance variable that prevent cycles in a novel locking and unlocking mechanism, to- gether with a hash heuristic as an auxiliary. On random 3-SAT benchmarks our solver outstrips all others on most instances, and surpasses other GSAT-based solvers by orders of magnitude on huge instances. Moreover, on crafted benchmarks we have improved the solving ca- pabilities of local search solvers, and solved much more instances than others in some domains. So far our strate- gies have significantly improved a current best solver on random 5-SAT benchmarks. Usually our solver needs much less flips than all others, which implies that our heuristic is the most informative and closest to the in- sight of most instances, and it has solved the cycling problem to a much greater extent."
                                }
                            ],
                            "name": "010104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 2.4,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The project is now in the phase where parallel calculations are possible.  We use all available cores, scheduling done by GNU parallel. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The memory is the real bottleneck, so possibly we will not be able to use all cores. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The memory is the real bottleneck, so possibly we will not be able to use all cores. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uws.edu.au",
                                    "name": "Intersect_Transformation_Semigroup_Enumeration",
                                    "usagePatterns": "The project is now in the phase where parallel calculations are possible.  We use all available cores, scheduling done by GNU parallel. 1 user. The search produces a few hundreds of megabytes of data.",
                                    "useCase": "Using our software packages developed for the GAP computer algebra system (www.gap-system.org) we run a search algorithm that enumerates transformation semigroups (finite state automata)."
                                }
                            ],
                            "name": "010105"
                        }
                    ],
                    "name": "0101"
                }
            ],
            "name": "01"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "utas.edu.au",
                                    "name": "hull-study-pt-I",
                                    "usagePatterns": "Few users on a small data set. ",
                                    "useCase": "Hi there, this study requires the simulation of the free-surface flow around medium-speed catamarans to investigate appropriate hull form parameters for a fuel-efficient design. The simulations will be done using the open-source code OpenFOAM, usually running in a parallel environment using 8 cores at a time. One run is expected to take a week, A complex parameter variation is required at different speeds, which results in a large number of runs. 10 instances with 8 cores each would allow 10 simulations per week. "
                                }
                            ],
                            "name": "091102"
                        }
                    ],
                    "name": "0911"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 64.0,
                                    "instanceQuota": 64.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "3D Printing in the Cloud",
                                    "usagePatterns": "The master VM will ingest models from a few MB up to 30-40 MB from each user. The master will then provision a worker VM to perform the computational work required (meshing/slicing/etc).  The default set-up is one master with one worker The master serves as the file repository to the workers (over NFS), and external-facing web server. As the demand changes, the master will dynamically spin up/down more worker VMs (which automatically connect over NFS). Periodically the master may store data in Amazon S3 or some other backup store.",
                                    "useCase": "Provides cloud-based meshing and slicing capability to serve growing local demand for processing behind all types of 3D printing. The current design is to use one master node which will serve jobs dynamically to up to 3 XXL VMs ('workers VMs'). It is unlikely that all requested cores will be used concurrently for extended periods, but only meet peak demands. The estimated number of core hours is a rough estimate of running at full capacity for 6 months. Tests show that 2-4 cores per user is most efficient with current 3D slicer software, so we plan to support  up to 30 concurrent users (which is perfect for scenarios such as a University mechatronics class) across the worker VMs. The Mechatronics Society 3D printed car project is the first test case for this cloud-based slicing capability. Later, as GPU nodes come online at the Monash node, much of the worker VM computations can start to be be offloaded from these XXL CPU-only VM instances to smaller CPU+GPU VMs. This will reduce pressure on vCPUs and core hours. "
                                }
                            ],
                            "name": "099999"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 1.2,
                                    "institution": "usq.edu.au",
                                    "name": "USQ eResearch Services Sandbox",
                                    "usagePatterns": "Many users and small data sets as well as small number of users and large data sets. This will vary depending on the tests and the resesearch group.",
                                    "useCase": "The cloud instances will be used to set up quick demos for researchers at USQ to run test experiments, simulations, modelling and calculations. The cloud instances will also be used to bench test the service from USQ."
                                }
                            ],
                            "name": "099901"
                        }
                    ],
                    "name": "0999"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 4.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "MUtopia Research Platform",
                                    "usagePatterns": "The number of users will most likely vary between 4-10 on our prototype system. Usage will be mainly in business working hours. The datasets are relatively small (in the order of hundreds of megabytes at most).",
                                    "useCase": "Hosts experimental servers for user experience, API and client testing for third-party usage with our existing decision support system prototype. This can entail 3 separate systems running at once such that we can very rapidly separate different versions of the system and test each in isolation (the data hosted on each system must be separate). "
                                },
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 2.4,
                                    "institution": "unimelb.edu.au",
                                    "name": "MUtopia Research Platform",
                                    "usagePatterns": "The number of users will most likely vary between 4-10 on our prototype system. Usage will be mainly in business working hours. The datasets are relatively small (in the order of hundreds of megabytes at most).",
                                    "useCase": "Hosts experimental servers for user experience, API and client testing for third-party usage with our existing decision support system prototype. This can entail 3 separate systems running at once such that we can very rapidly separate different versions of the system and test each in isolation (the data hosted on each system must be separate). "
                                }
                            ],
                            "name": "0905"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 28.8,
                                    "instanceQuota": 28.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM MUtopia",
                                    "usagePatterns": "Currently, our system is hosting moderately large precinct models (thousands of buildings with hundreds of parameters each), plus some large assets (such as uploaded 3D models). The scale and number of these projects is expected to grow, potentially substantially, over the next year as we acquire more clients to work with and develop more sophisticated models. However CPU usage is typically light, with the occasional demanding simulation. We would have a primary instance as a production server, and a secondary, identical server for development staging. The final server would be split between hosting our development tools and smaller, temporary demo instances in VMs. Object storage would be utilised to store assets, uploaded/downloadable by the user, as well as for redundant database backup.",
                                    "useCase": "The MUtopia team is developing a large web application for the modelling, simulation and visualisation of urban development projects. The servers will host the web application and its databases for access by researchers and clients, public and private, current and future. They should improve performance over our current arrangement, and provide greater redundancy and data security. The project will be ongoing beyond this year, but this should be sufficient for that time. I'm not sure how to estimate core hours for a 24/7 web server. See http://mutopia.unimelb.edu.au/ for more information."
                                },
                                {
                                    "coreQuota": 28.8,
                                    "instanceQuota": 1.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM MUtopia",
                                    "usagePatterns": "Currently, our system is hosting moderately large precinct models (thousands of buildings with hundreds of parameters each), plus some large assets (such as uploaded 3D models). The scale and number of these projects is expected to grow, potentially substantially, over the next year as we acquire more clients to work with and develop more sophisticated models. However CPU usage is typically light, with the occasional demanding simulation. We would have a primary instance as a production server, and a secondary, identical server for development staging. The final server would be split between hosting our development tools and smaller, temporary demo instances in VMs. Object storage would be utilised to store assets, uploaded/downloadable by the user, as well as for redundant database backup.",
                                    "useCase": "The MUtopia team is developing a large web application for the modelling, simulation and visualisation of urban development projects. The servers will host the web application and its databases for access by researchers and clients, public and private, current and future. They should improve performance over our current arrangement, and provide greater redundancy and data security. The project will be ongoing beyond this year, but this should be sufficient for that time. I'm not sure how to estimate core hours for a 24/7 web server. See http://mutopia.unimelb.edu.au/ for more information."
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM__iDDSS",
                                    "usagePatterns": "The platform provides a small group of experienced users with the capability of analysing large geospatial datasets (both static and dynamic).",
                                    "useCase": "The architecture of iDDSS platform consists of one Postgres database instance (VM1), one Apache web server instance (VM2) and two GeoServer instances (VM3/4). The platform is loosely coupled, and six types of services running upon these instances form the application layer of iDDSS:  (1) VM1 hosts Data Gathering Service (DGS) and Data Integration Service (DIS). Aggregated datasets from DIS are stored in VM3/4. (2) VM2 hosts Simulation Service (SS) and Visualisation Service (VS), fetching data from VM3/4. (3) VM3/4 host Disaster Modelling Service (DMS) and Optimization Service (OS). "
                                },
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 4.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "UoM__iDDSS",
                                    "usagePatterns": "The platform provides a small group of experienced users with the capability of analysing large geospatial datasets (both static and dynamic).",
                                    "useCase": "The architecture of iDDSS platform consists of one Postgres database instance (VM1), one Apache web server instance (VM2) and two GeoServer instances (VM3/4). The platform is loosely coupled, and six types of services running upon these instances form the application layer of iDDSS:  (1) VM1 hosts Data Gathering Service (DGS) and Data Integration Service (DIS). Aggregated datasets from DIS are stored in VM3/4. (2) VM2 hosts Simulation Service (SS) and Visualisation Service (VS), fetching data from VM3/4. (3) VM3/4 host Disaster Modelling Service (DMS) and Optimization Service (OS). "
                                }
                            ],
                            "name": "090505"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 32.0,
                                    "institution": "monash.edu",
                                    "name": "Monash_Water_Quality-Gippsland_Lakes",
                                    "usagePatterns": "It will only have a couples of users for this project at the moment. The number of files can get up to a few hundreds. The sizes of the individual files can range from a few KB to 20GB.",
                                    "useCase": "I am working on a water quality modelling project at the Gippsland Lakes. The research project looks into the  dynamics of the toxic cyanobacterial blooms in the lakes. The software we use is called  MIKE developed by DHI (http://www.dhisoftware.com/). The model requires a lot computing power. As approaching towards the end this study, we will need to do many simulations for model calibration and scenario explorations in the next 10 months. Unfortunately, the software I am using can  only run on windows platform and we just have a 6-core windows machine in our lab which is far from being capable to handles all the computations. Therefore we hope to get access to more computing power to overcome this constraints."
                                },
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 1.0,
                                    "institution": "monash.edu",
                                    "name": "Monash_Water_Quality-Gippsland_Lakes",
                                    "usagePatterns": "It will only have a couples of users for this project at the moment. The number of files can get up to a few hundreds. The sizes of the individual files can range from a few KB to 20GB.",
                                    "useCase": "I am working on a water quality modelling project at the Gippsland Lakes. The research project looks into the  dynamics of the toxic cyanobacterial blooms in the lakes. The software we use is called  MIKE developed by DHI (http://www.dhisoftware.com/). The model requires a lot computing power. As approaching towards the end this study, we will need to do many simulations for model calibration and scenario explorations in the next 10 months. Unfortunately, the software I am using can  only run on windows platform and we just have a 6-core windows machine in our lab which is far from being capable to handles all the computations. Therefore we hope to get access to more computing power to overcome this constraints."
                                }
                            ],
                            "name": "090508"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "Translink",
                                    "usagePatterns": "We will have 5-6 users and this single large data set (2.5 GB).",
                                    "useCase": "We have been given a large database (2.5 GB) of Go Card records from a large public agency in Queensland (Translink, the public transport authority). These data will be mined for useful insights into transport service performance and for traveler interactions with the system. The need for us is to share this database and resulting queries on the data across several researchers. We will be using MySQL and other data query tools."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "uq.edu.au",
                                    "name": "Translink",
                                    "usagePatterns": "We will have 5-6 users and this single large data set (2.5 GB).",
                                    "useCase": "We have been given a large database (2.5 GB) of Go Card records from a large public agency in Queensland (Translink, the public transport authority). These data will be mined for useful insights into transport service performance and for traveler interactions with the system. The need for us is to share this database and resulting queries on the data across several researchers. We will be using MySQL and other data query tools."
                                }
                            ],
                            "name": "090507"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "jcu.edu.au",
                                    "name": "Smart Water PoC (Proof of Concept)",
                                    "usagePatterns": "Web access by individuals Database driven data from individual smart water meters",
                                    "useCase": "Townsville Water & Waste (Townsville Water) is working with researchers in the eResearch Centre at James Cook University in seeking ways to assist residents reduce the amount of water they need to use.  The rationale for the selection of Townsville as a test region is that it is uniquely dry for a tropical area  resulting in an average  household water use that is amongst the highest in Australia, some 3 times more than other major Australian cities. Part of this is because a significant proportion of this water is used outdoors (approximately 60% of the usage).  Compute resources are required as part of hosting services for a \"smart water meter\" pilot to demonstrate and validate what can be achieved; these meters are installed in certain Townsville residences and allow the capture of water flow and similar information, which can be fed back to residents via a web-based portal. Findings of this pilot project can then be scaled eventually into a whole-of-Townsville approach. This project is currently underway and compute resources are required for data storage, serving, and access."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 0.5,
                                    "institution": "jcu.edu.au",
                                    "name": "Smart Water PoC (Proof of Concept)",
                                    "usagePatterns": "Web access by individuals Database driven data from individual smart water meters",
                                    "useCase": "Townsville Water & Waste (Townsville Water) is working with researchers in the eResearch Centre at James Cook University in seeking ways to assist residents reduce the amount of water they need to use.  The rationale for the selection of Townsville as a test region is that it is uniquely dry for a tropical area  resulting in an average  household water use that is amongst the highest in Australia, some 3 times more than other major Australian cities. Part of this is because a significant proportion of this water is used outdoors (approximately 60% of the usage).  Compute resources are required as part of hosting services for a \"smart water meter\" pilot to demonstrate and validate what can be achieved; these meters are installed in certain Townsville residences and allow the capture of water flow and similar information, which can be fed back to residents via a web-based portal. Findings of this pilot project can then be scaled eventually into a whole-of-Townsville approach. This project is currently underway and compute resources are required for data storage, serving, and access."
                                }
                            ],
                            "name": "090509"
                        }
                    ],
                    "name": "0905"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 44.8,
                                    "instanceQuota": 44.8,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF CFD Dev",
                                    "usagePatterns": "We envisage a small number of users, about 3 presently. We will require access to a moderate amount of temporary data space, up to 1 TB. However, this does not need to be stored beyond the life of the VM instance.",
                                    "useCase": "Our research group develops computational fluid dynamics codes to simulate compressible flows. Our code scales well to 100s of cores in a traditional HPC environment. However, we face the difficulty of developing on local workstations of few cores and then making the leap to large-scale HPC infrastructures where we compete for scarce compute time. We can use the many-core environment of the cloud to bridge the gap when trying to develop, debug and test our code and simulations."
                                },
                                {
                                    "coreQuota": 2.8,
                                    "instanceQuota": 0.7,
                                    "institution": "adelaide.edu.au",
                                    "name": "Airfoil Noise: Wing in junction and porous",
                                    "usagePatterns": "On instance disk anticipated as sufficient for higher i/o tasks. Some long compute time, single i/o tasks may be run by mounting eRSA storage to which the user has access (ersa_mounthpchome).",
                                    "useCase": "The cloud instances will be used to run computation fluid dynamics (CFD) simulations, and noise modelling post-processing of the CFD."
                                },
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "MILD combustion jet in hot coflow",
                                    "usagePatterns": "Large data set, small number of users.",
                                    "useCase": "My students and I will be using a VM for running OpenFOAM (reactingFOAM and OpenSMOKE). Based on previous experience with similar jobs submitted to Tizard each case will take approximately 5000 CPU hours. We have a large parametric study to perform, involving 20 different cases."
                                },
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 0.5,
                                    "institution": "griffith.edu.au",
                                    "name": "Ocean model",
                                    "usagePatterns": "Large data sets with a small number of users",
                                    "useCase": "I need a VM to conduct the project listed below. The project implements high-resolution ocean model to investigate coastal processes for the South-east Queensland. The project requires a large volume of storage space and access speed to analyse the data. Although there are some activities regrading regional ocean modelling in Australia such as CSIRO and universities, this level of detailed numerical modelling has not been conducted in Australia and will demonstrate capacity of high-resolution modelling necessary to understand coastal ocean, which is currently missing."
                                }
                            ],
                            "name": "091501"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "usc.edu.au",
                                    "name": "USC-UPLOADS",
                                    "usagePatterns": "This will be a long term resource hosting a web server as part of an ARC funded research output",
                                    "useCase": "This is to host a web based front end for an ARC funded project. The goal of the UPLOADS (Understanding and Preventing Led Outdoor Accidents Data System) project is to develop a standardised, national approach to incident reporting and learning for the outdoor sector in Australia. The project is funded by the Australian Research Council and a diverse range of stakeholders in the outdoor sector, including outdoor education and recreation associations, outdoor activity providers and government departments. The project involves researchers from the University of the Sunshine Coast Accident Research (USCAR) team, the Monash University Accident Research Centre (MUARC), and the Centre for Healthy and Safe Sport (CHASS), University of Ballarat."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "usc.edu.au",
                                    "name": "USC-UPLOADS",
                                    "usagePatterns": "will be low use development purposes only",
                                    "useCase": "This is to develop a web based front end for an ARC funded project. The goal of the UPLOADS (Understanding and Preventing Led Outdoor Accidents Data System) project is to develop a standardised, national approach to incident reporting and learning for the outdoor sector in Australia. The project is funded by the Australian Research Council and a diverse range of stakeholders in the outdoor sector, including outdoor education and recreation associations, outdoor activity providers and government departments. The project involves researchers from the University of the Sunshine Coast Accident Research (USCAR) team, the Monash University Accident Research Centre (MUARC), and the Centre for Healthy and Safe Sport (CHASS), University of Ballarat."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "usc.edu.au",
                                    "name": "USC-UPLOADS",
                                    "usagePatterns": "This will be a long term resource hosting a web server as part of an ARC funded research output",
                                    "useCase": "This is to host a web based front end for an ARC funded project. The goal of the UPLOADS (Understanding and Preventing Led Outdoor Accidents Data System) project is to develop a standardised, national approach to incident reporting and learning for the outdoor sector in Australia. The project is funded by the Australian Research Council and a diverse range of stakeholders in the outdoor sector, including outdoor education and recreation associations, outdoor activity providers and government departments. The project involves researchers from the University of the Sunshine Coast Accident Research (USCAR) team, the Monash University Accident Research Centre (MUARC), and the Centre for Healthy and Safe Sport (CHASS), University of Ballarat."
                                }
                            ],
                            "name": "091507"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 0.5,
                                    "institution": "monash.edu",
                                    "name": "LTRAC-WebAndGitServer",
                                    "usagePatterns": "Standard web and git server access patterns. Lots of reading/writing of small files.",
                                    "useCase": "The Laboratory for Turbulence Research in Aerospace and Combustion (LTRAC) at Monash University undertakes research in the areas of turbulent flows, combustion, alternative and renewable energy, laser diagnostics and numerical simulation of turbulence. LTRAC has wide ranging capabilities in laser diagnostics including high resolution PIV, SPIV, Tomographic PIV (Tomo-PIV), Digital Holographic PIV, PLIF, Time-resolved PIV, SPIV and Tomo-PIV, high speed imaging including ultra-high-speed Schlieren capable of up to 1 Million frames per second imaging. The group has numerous links with both national and international research institutes, industries and universities. This NeCTAR project will allow us to host our group's web and git server to provide the public access to information about us and our technical reports as well as providing our group with a private git repository and wiki for internal policy and source code management."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "adelaide.edu.au",
                                    "name": "MILD combustion jet in hot coflow",
                                    "usagePatterns": "Large data set, small number of users.",
                                    "useCase": "My students and I will be using a VM for running OpenFOAM (reactingFOAM and OpenSMOKE). Based on previous experience with similar jobs submitted to Tizard each case will take approximately 5000 CPU hours. We have a large parametric study to perform, involving 20 different cases."
                                }
                            ],
                            "name": "091508"
                        }
                    ],
                    "name": "0915"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL EMAP",
                                    "usagePatterns": "edit 2014-01-06 * additional 5TB requested, for performing backups ========== edit 2013-10-14 * 3 instances requested * Galaxy instance will take up the majority of the cores * Number of core hours calculated for 3 years (average 27.4 core-hours per day) ========= medium sized datasets 100's MBs per dataset, plus any analysis files < 1GB",
                                    "useCase": "This will be used for the MyTardis and Galaxy storage for the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL_EMAP_DEV",
                                    "usagePatterns": "2014-01-07 need an additional test instance (1 VCPU)",
                                    "useCase": "Development environment for development and testing. Part of the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 3.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL EMAP",
                                    "usagePatterns": "edit 2013-10-14 * 3 instances requested * Galaxy instance will take up the majority of the cores * Number of core hours calculated for 3 years (average 27.4 core-hours per day) ========= medium sized datasets 100's MBs per dataset, plus any analysis files < 1GB",
                                    "useCase": "This will be used for the MyTardis and Galaxy storage for the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL_EMAP_DEV",
                                    "usagePatterns": "",
                                    "useCase": "Development environment for development and testing. Part of the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL EMAP",
                                    "usagePatterns": "edit 2013-10-14 * 3 instances requested * Galaxy instance will take up the majority of the cores * Number of core hours calculated for 3 years (average 27.4 core-hours per day) ========= medium sized datasets 100's MBs per dataset, plus any analysis files < 1GB",
                                    "useCase": "This will be used for the MyTardis and Galaxy storage for the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL EMAP",
                                    "usagePatterns": "edit 2014-01-06 * additional 5TB requested, for performing backups ========== edit 2013-10-14 * 3 instances requested * Galaxy instance will take up the majority of the cores * Number of core hours calculated for 3 years (average 27.4 core-hours per day) ========= medium sized datasets 100's MBs per dataset, plus any analysis files < 1GB",
                                    "useCase": "This will be used for the MyTardis and Galaxy storage for the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "sydney.edu.au",
                                    "name": "CVL_EMAP_DEV",
                                    "usagePatterns": "",
                                    "useCase": "Development environment for development and testing. Part of the Characterisation Virtual Laboratory - Energy Materials - Atom Probe subproject"
                                },
                                {
                                    "coreQuota": 12.0,
                                    "instanceQuota": 12.0,
                                    "institution": "csiro.au",
                                    "name": "CSIRO-CIAP",
                                    "usagePatterns": "At least initially we envision a small number or users (&lt; 1000 total, < 20 concurrent ) and relatively big data sets (e.g.28GB) We also need about 250 GB of volume storage.",
                                    "useCase": "This is an extension of the existing allocation for the NeCTAR Image Processing and Analysis toolkit. Some instances would be used to host online web interface and   database for the toolkit. (~ 3 instances ~ 4 cores) Other instances will be used as workers to preform actual computations. These my be provisioned on demand depending on the load. The worker instances may be either medium or large - this is still uncertain at the moment."
                                }
                            ],
                            "name": "0912"
                        }
                    ],
                    "name": "0912"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 0.5,
                                    "institution": "unsw.edu.au",
                                    "name": "PhDProj",
                                    "usagePatterns": "My project will be a single user/large data project. Ideally, I would like to run several instances of the program as I need to conduct numerous statistical studies, with one point representing one run.",
                                    "useCase": "PhD Research Project Optimal Flight Paths for Engine-Out Large Transport Aircraft over Mountainous Terrain The program is a java run time and usually takes 3-8 hours to run."
                                },
                                {
                                    "coreQuota": 0.5,
                                    "instanceQuota": 0.5,
                                    "institution": "unsw.edu.au",
                                    "name": "PhDProj",
                                    "usagePatterns": "My project will be a single user/large data project. Ideally, I would like to run several instances of the program as I need to conduct numerous statistical studies, with one point representing one run.",
                                    "useCase": "PhD Research Project Optimal Flight Paths for Engine-Out Large Transport Aircraft over Mountainous Terrain The program is a java run time and usually takes 3-8 hours to run."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "sydney.edu.au",
                                    "name": "Autonomous soaring 2",
                                    "usagePatterns": "This project will have one user running one instance generating large data sets. The data will be accessed by multiple users from different locations.",
                                    "useCase": "I intend to use the cloud instances to run further tests and simulations on machine learning methods for performing autonomous soaring in under powered unmanned aerial vehicles. This learning algorithm will require extensive validation, generating large data sets that will be accessed by multiple users."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.1,
                                    "institution": "sydney.edu.au",
                                    "name": "Autonomous soaring 2",
                                    "usagePatterns": "This project will have one user running one instance generating large data sets. The data will be accessed by multiple users from different locations.",
                                    "useCase": "I intend to use the cloud instances to run further tests and simulations on machine learning methods for performing autonomous soaring in under powered unmanned aerial vehicles. This learning algorithm will require extensive validation, generating large data sets that will be accessed by multiple users."
                                }
                            ],
                            "name": "090104"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "sydney.edu.au",
                                    "name": "Autonomous soaring",
                                    "usagePatterns": "",
                                    "useCase": "This project requires the generation, storage and processing of large amounts of simulated flight data to perform online machine learning. The goal of the project is to learn optimal policies for flight trajectory planning to perform autonomous soaring on an unmanned autonomous glider in an unknown wind field."
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "griffith.edu.au",
                                    "name": "Interactive Shape Optimisation",
                                    "usagePatterns": "About 30 simulations will be conducted over a small number of weeks. Each simulation will require roughly 300 CPU hours.  The instance will be used for a few hours at a time and shut down in between runs. User interaction will require some communication to an external server with a negligible amount of data being transmitted. ",
                                    "useCase": "The Instance will be used as part of a case study on using Interactive Multi-Objective Particle Swarm Optimisation on a Jet Engine Compressor Blade design problem. The instance will be used for the computational fluid dynamics simulation tool and the algorithm. The requested number of core hours represents an upper limit and will likely be less."
                                }
                            ],
                            "name": "090199"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 19.2,
                                    "instanceQuota": 19.2,
                                    "institution": "uq.edu.au",
                                    "name": "QCIF CFD Dev",
                                    "usagePatterns": "We envisage a small number of users, about 3 presently. We will require access to a moderate amount of temporary data space, up to 1 TB. However, this does not need to be stored beyond the life of the VM instance.",
                                    "useCase": "Our research group develops computational fluid dynamics codes to simulate compressible flows. Our code scales well to 100s of cores in a traditional HPC environment. However, we face the difficulty of developing on local workstations of few cores and then making the leap to large-scale HPC infrastructures where we compete for scarce compute time. We can use the many-core environment of the cloud to bridge the gap when trying to develop, debug and test our code and simulations."
                                }
                            ],
                            "name": "090107"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.2,
                                    "institution": "monash.edu",
                                    "name": "LTRAC-WebAndGitServer",
                                    "usagePatterns": "Standard web and git server access patterns. Lots of reading/writing of small files.",
                                    "useCase": "The Laboratory for Turbulence Research in Aerospace and Combustion (LTRAC) at Monash University undertakes research in the areas of turbulent flows, combustion, alternative and renewable energy, laser diagnostics and numerical simulation of turbulence. LTRAC has wide ranging capabilities in laser diagnostics including high resolution PIV, SPIV, Tomographic PIV (Tomo-PIV), Digital Holographic PIV, PLIF, Time-resolved PIV, SPIV and Tomo-PIV, high speed imaging including ultra-high-speed Schlieren capable of up to 1 Million frames per second imaging. The group has numerous links with both national and international research institutes, industries and universities. This NeCTAR project will allow us to host our group's web and git server to provide the public access to information about us and our technical reports as well as providing our group with a private git repository and wiki for internal policy and source code management."
                                }
                            ],
                            "name": "090101"
                        }
                    ],
                    "name": "0901"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 80.0,
                                    "instanceQuota": 40.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "Unimelb_ITS_Research_FEA",
                                    "usagePatterns": "",
                                    "useCase": "Develop FEA cloud projects for ITS reserach services"
                                }
                            ],
                            "name": "09"
                        }
                    ],
                    "name": "09"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 32.0,
                                    "instanceQuota": 2.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Flow and Noise Group",
                                    "usagePatterns": "",
                                    "useCase": "The resource is needed to support aeroacoustci research in the flow and noise group.  Supporting ARC and DSTO funded projects, 10 PhD students and 3 postdocs.  The use will mainly be for numerical simulation of fluid mechanics, acoustics and signal processing of experimental data."
                                }
                            ],
                            "name": "0913"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.3,
                                    "institution": "monash.edu",
                                    "name": "LTRAC-WebAndGitServer",
                                    "usagePatterns": "Standard web and git server access patterns. Lots of reading/writing of small files.",
                                    "useCase": "The Laboratory for Turbulence Research in Aerospace and Combustion (LTRAC) at Monash University undertakes research in the areas of turbulent flows, combustion, alternative and renewable energy, laser diagnostics and numerical simulation of turbulence. LTRAC has wide ranging capabilities in laser diagnostics including high resolution PIV, SPIV, Tomographic PIV (Tomo-PIV), Digital Holographic PIV, PLIF, Time-resolved PIV, SPIV and Tomo-PIV, high speed imaging including ultra-high-speed Schlieren capable of up to 1 Million frames per second imaging. The group has numerous links with both national and international research institutes, industries and universities. This NeCTAR project will allow us to host our group's web and git server to provide the public access to information about us and our technical reports as well as providing our group with a private git repository and wiki for internal policy and source code management."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 0.2,
                                    "institution": "adelaide.edu.au",
                                    "name": "MILD combustion jet in hot coflow",
                                    "usagePatterns": "Large data set, small number of users.",
                                    "useCase": "My students and I will be using a VM for running OpenFOAM (reactingFOAM and OpenSMOKE). Based on previous experience with similar jobs submitted to Tizard each case will take approximately 5000 CPU hours. We have a large parametric study to perform, involving 20 different cases."
                                }
                            ],
                            "name": "091305"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.3,
                                    "institution": "adelaide.edu.au",
                                    "name": "Airfoil Noise: Wing in junction and porous",
                                    "usagePatterns": "On instance disk anticipated as sufficient for higher i/o tasks. Some long compute time, single i/o tasks may be run by mounting eRSA storage to which the user has access (ersa_mounthpchome).",
                                    "useCase": "The cloud instances will be used to run computation fluid dynamics (CFD) simulations, and noise modelling post-processing of the CFD."
                                }
                            ],
                            "name": "091301"
                        }
                    ],
                    "name": "0913"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.4,
                                    "instanceQuota": 0.4,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF eSpaces",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate a custom webserve JCU's staff use to Qcloud, enabling others to use the same."
                                },
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 1.6,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CCDAM",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the primary digital asset management system used by JCU to enable others to do the same. "
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "jcu.edu.au",
                                    "name": "Exposing JCU research",
                                    "usagePatterns": "VM users will be small however, we would hope that web access to the portal would increase with time.",
                                    "useCase": "web portal for collaboration and dissemination of research outputs that will include reports, publications, datasets, databases, etc..  This will be maintained by JCU's eResearch Centre and we will assist researchers with development on this VM."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CLiMAS_1",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the web service side of a citizen science / communication site on the potential impacts of climate change on Australian fauna. This project was previously on QERN."
                                },
                                {
                                    "coreQuota": 3.2,
                                    "instanceQuota": 3.2,
                                    "institution": "jcu.edu.au",
                                    "name": "QCIF CLiMAS_2",
                                    "usagePatterns": "",
                                    "useCase": "This project is part of JCU's eResearch Centre allocation to migrate research tools to Qcloud. This part is to migrate the compute side of a citizen science / communication site on the potential impacts of climate change on Australian fauna. This project was previously on QERN. NOTE: this is associated with CLiMAS_1 VM request!!!"
                                }
                            ],
                            "name": "090703"
                        }
                    ],
                    "name": "0907"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 0.6,
                                    "institution": "sydney.edu.au",
                                    "name": "USyd_Velodyne_processing",
                                    "usagePatterns": "I have ~400 GB of datasets which would be read only (with one user). I cache aggressively to speed up data processing. This takes up to 1TB, although I can reduce this, and is write once, read many.   Finally I save results to analyse on my local machine. I'd need only a few GB for this. ",
                                    "useCase": "For my research I am processing large datasets of 3D laser and camera data. This requires a lot of hard drive space (we have terabytes of data), and also a lot of CPU time. For my thesis I am developing new processing algorithms, and this requires continual testing and analysis. Currently I am running algorithms over night on my desktop computer, but having access to more computing resources would greatly speed up my work. My research is in field robotics - more specifically developing algorithms for dynamic scene understanding - which is applicable to self-driving cars, and other safety and security uses. I have around 4 months to finish my thesis, and would like to be able to scale up the size of datasets I am working on. This extra processing power would allow me to do that. I can run many experiments at once meaning I would make full use of the cores.  The CPU time estimate above is only rough, I could use much more if it were available. "
                                }
                            ],
                            "name": "090602"
                        }
                    ],
                    "name": "0906"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "rmit.edu.au",
                                    "name": "\"Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory. "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "rmit.edu.au",
                                    "name": "Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "I've requested this allocation for another 12 months as we expect to be using this allocation  throughout this time. We expect to have a small number of users (~4) but some of us will be running large jobs that may take 30-50 core hours to run each. These may involve input and output data that take up to 5 Gig per machine (though many runs will be less then this). This will be for running simulations using the tzar framework for publications we are working on and also for testing and developing tzar further to be a component in the Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. ",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large Simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 4.0,
                                    "institution": "rmit.edu.au",
                                    "name": "Tzar-FW\" - Additional VMs",
                                    "usagePatterns": "I've requested this allocation for 6 months as we expect to be using this allocation sporadically throughout this time. We expect to have a small number of users (~4) but some of us will be running large jobs that may take 30-50 core hours to run each. These may involve input and output data that take up to 10 Gig per machine (though many runs will be less then this). This will be for running simulations using the tzar framework for publications we are working on and also for testing and developing tzar further to be a component in the Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. ",
                                    "useCase": "We would like to request the max number of cores in the \"Tzar framework\" allocation be increased from 7 to 20. There are two reasons for this. We have been testing and developing modelling software that runs on the Nectar VMs and it is now close to a stage where we can run some large simulation jobs for some papers we are preparing. Secondly, we are going to be contributing software to the  Biodiversity & Climate Change Virtual Laboratory that is funded by Nectar. Thus we will need more cores to test our software with larger data sets that will be used in the workflows for the Virtual Laboratory."
                                }
                            ],
                            "name": "090903"
                        }
                    ],
                    "name": "0909"
                }
            ],
            "name": "09"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 1.2,
                                    "institution": "sydney.edu.au",
                                    "name": "Solstice",
                                    "usagePatterns": "The project's main requirement is bandwidth in serving audio files to users mobile devices. Vivid runs from May 24th to June 10th with the project running from dusk to midnight each night. File serving needs to be relatively fast but we will handle synch issues by buffering pre-rendered audio on devices in good time. However, clearly there will be a capacity issue at the server or at specific mobile access points. We intend to gauge and handle this within the possible limits. Requests to download will be interrupted at capacity. An ultra minimal default file will be used as higher capacity is reached. ",
                                    "useCase": "Novel electronic media artwork. Audio files will be served from the VM to mobile devices and played in synch to create multi-device generative musical structures. A node.js websocket server on the VM will serve requests. Compressed audio files will be downloaded onto devices as needed. Client-side javascript / HTML5 audio tag will play audio in synch with a laser show being projected on the AMP building, Circular Quay, Sydney. This is a component of the Solstice large-scale electronic media artwork, hosted by the Vivid Festival and AMP's amplify festival. The project is developed by researchers in electronic arts and music based at the Faculty of Architecture, Design and Planning, and the Conservatorium of Music, University of Sydney. The work involves electronic media arts research that will result in HERDC countable research outputs and will constitute a major Non-Traditional Research Output. Usage patterns and audience response will be surveyed.  To be clear: multiple audience members distributed throughout the Circular Quay and Sydney Harbour area will be simultaneously accessing files on the server."
                                }
                            ],
                            "name": "120304"
                        }
                    ],
                    "name": "1203"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 40.0,
                                    "instanceQuota": 40.0,
                                    "institution": "unimelb.edu.au",
                                    "name": "AURIN",
                                    "usagePatterns": "Maintaining a worker pool for testing or workflow processing.",
                                    "useCase": "Bursting compute workflows and ephemeral testing."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "Indigenous Housing Knowledge Base",
                                    "usagePatterns": "Large datasets with a small number of researchers using the service.",
                                    "useCase": "This is an ARC Discovery Indigenous grant : IN140100033, ?Defining the Impact of Regionalism on Aboriginal Housing and Settlements. This project aims to improve our understanding of the regional factors that impact on Indigenous Housing by establishing an integrated knowledge-base that aggregates data from a number of sources into a single data model and providing web-based research services that enable the data to be queried, aggregated, analysed and visualized spatially. We will be using the instance to host these services and also for extracting structured data from publications, inferencing and for applying R statistical analysis tools to the data."
                                }
                            ],
                            "name": "12"
                        }
                    ],
                    "name": "12"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 2.0,
                                    "institution": "qut.edu.au",
                                    "name": "Jellyfish Migration Project",
                                    "usagePatterns": "It will be large data sets and a large number of users.",
                                    "useCase": "This is a a Transport and Mobility Portal for Research and Modelling Project funded by QCIF.  The project number is 00113. Objectives: (Aims of the project in terms of deliverables to the participants and future research interests) Limited institutional hosting capabilities are, at this point, preventing further growth of the user base. Accessing vast amounts of data requires more scalable resources that can accommodate fluctuations in demand to the portal and its service. A cloud based storage and hosting of the platform would allow access to the platform for a wider community.  While the portal is currently running as a standalone corporately firewalled system, a collaborative installation of the portal and its tools in QCloud would be beneficial, as it would allow for: improved breadth of data by collections from additional users, scope to include more sub-discipline data sets (operations/planning), wider geographical coverage of data (Brisbane vs. South East Queensland), and ability to easily share information among authorities and universities, creation of  a collaboration platform for transport research and projects. Such installation would overcome the above listed issues regarding transport data, and make it not only visible to researchers, but provide it in a standardised format that allows quick prototyping of ideas and will foster research ideas and help to identify data gaps. "
                                }
                            ],
                            "name": "120405"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.2,
                                    "instanceQuota": 0.3,
                                    "institution": "uq.edu.au",
                                    "name": "MatlabEAIT",
                                    "usagePatterns": "Usage pattern will change over time, but will envisage small data sets with just myself as the user.",
                                    "useCase": "Evaluating Windows Matlab performance and firewall issues including licensing and connectivity to UQ EAIT file server .  "
                                }
                            ],
                            "name": "1204"
                        }
                    ],
                    "name": "1204"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "Translink",
                                    "usagePatterns": "We will have 5-6 users and this single large data set (2.5 GB).",
                                    "useCase": "We have been given a large database (2.5 GB) of Go Card records from a large public agency in Queensland (Translink, the public transport authority). These data will be mined for useful insights into transport service performance and for traveler interactions with the system. The need for us is to share this database and resulting queries on the data across several researchers. We will be using MySQL and other data query tools."
                                },
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.2,
                                    "institution": "uq.edu.au",
                                    "name": "Translink",
                                    "usagePatterns": "We will have 5-6 users and this single large data set (2.5 GB).",
                                    "useCase": "We have been given a large database (2.5 GB) of Go Card records from a large public agency in Queensland (Translink, the public transport authority). These data will be mined for useful insights into transport service performance and for traveler interactions with the system. The need for us is to share this database and resulting queries on the data across several researchers. We will be using MySQL and other data query tools."
                                }
                            ],
                            "name": "120506"
                        }
                    ],
                    "name": "1205"
                }
            ],
            "name": "12"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "uq.edu.au",
                                    "name": "Automated Topology Builder Development",
                                    "usagePatterns": "Usage will be for testing and development as the pipeline is visualized. The load will be intermittent. ",
                                    "useCase": "Project Aim:  To migrate an automated topology builder http://compbio.biosci.uq.edu.au/atb/ on to the Queensland NeCTAR platform. Our system provides atomic interaction parameters for molecules that are used in simulations and drug design. Users submit the coordinates of the molecule in question, we then perform a series of back-end calculations and add the molecule to the database. The calculations vary depending on what is in the database and the nature of the molecule. In order to be able to handle this and a widely varying load we are re-organising our pipeline so that separate calculations can be farmed out onto different virtual machines that are generated as required and removed (or suspended) afterwards.  Currently the NeCTAR nodes in Brisbane do not have this capacity. To develop and test the application we need one VM to act as a head node and access to multiple dynamically allocated VM's with up to 4 cores each and about 1G mem per core (ideally 12-16 cores in total).  This project is funded in part by NeCTAR via the Queensland Cyber Infrastructure Foundation and UQ"
                                },
                                {
                                    "coreQuota": 150.0,
                                    "instanceQuota": 150.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQ_ATB",
                                    "usagePatterns": "The main database is managed by a set of core machines which handle job scheduling and user access. These spawn compute jobs as required. These compute jobs are independent, largely asynchronous and are not time critical. The initial quantum mechanical calculations required for each new molecule range from 10's of minutes to hundreds of CPU hours depending on the size of the system. Systems beyond a given threshold are directed to HPC systems. The validation jobs are in the order of 1-4 hours. Validation is, however, an iterative process with 20 to 50 jobs per molecule. A priority list of 700 molecules have been identified for initial validation. While the resources requested are considerable these will be primarily required in this expansion phase. The system is also designed to be robust and responsive expanding to fill any spare capacity. Equally jobs can be easily suspended if there are other demands on the system without affecting the database itself. The storage request is required during the validation phase.  The database itself is currently in the order of 10 Gbytes. ",
                                    "useCase": "The Automated Topology Builder and Repository is an ongoing project to generate and supply molecular interaction parameters for use in structure refinement and computational drug design. The site has approximately 1,200 registered users (growing by ~10 - 20 a week) and is accessed by several hundred researchers daily. The repository currently contains 4500 molecules.  When parameters for a molecule are requested that is not in the database as series of calculations are performed in order to generate the required parameters. These parameters are then tested and refined. The database is still in its development phase and is currently being expanded. Our aim is not only to host the site itself on the NECTAR system but to use the capacity of the Q-cloud to perform the initial generation of parameters of 12,000 commonly used molecules and validation of a proportion of these and exiting molecules in the database. Automated procedures for this have been developed. These involve dynamically creating and removing VMs as required and as resources are available. While expanding the database we fully scalable and have the capacity to use all available resources. We are working with QCIF and are well placed to ensure efficient utilization of resources in the testing and start-up phases of the Q-cloud with a high value project.          "
                                }
                            ],
                            "name": "030799"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "Molecular Dynamics Simulations",
                                    "usagePatterns": "small number of users and large data sets",
                                    "useCase": " Here is the research use case: The data consists mainly of coordinate trajectories from molecular dynamics simulations of viral glycoproteins, that are simulated to be used in order to more efficiently design drugs against infectious diseases such as malaria, influenza, parainfluenza, and rotavirus. The significance of this project lies in its capacity to use a novel approach to develop a better understanding of the glycobiology of viral infection. The uniqueness of the proposed project lies in the application of molecular dynamics (MD) as a tool to understand the underlying microscopic processes of carbohydrate-protein interaction responsible for a viral infection, making the design of more specific drugs feasible. Previous success with the development of the anti-influenza drug Relenza suggests that targeting proteins associated with carbohydrate recognition is a viable approach towards the development of new drugs. The outcomes of these studies will and have been published in peer-reviewed journals (JACS 2012, 134 (44), 18447) and are presented at international conferences. "
                                }
                            ],
                            "name": "030704"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 72.0,
                                    "instanceQuota": 9.0,
                                    "institution": "monash.edu",
                                    "name": "Ab initio calculations of physical properties of ionic and polymer materials",
                                    "usagePatterns": "The project will have large data sets with a small number of users (at the moment 3 users).",
                                    "useCase": "The main goal of the project is to establish reliable computational methodologies for the accurate prediction of physical properties of ionic and polymer materials, including liquid electrolytes and reaction rates of radical polymerisation in ionic liquids. The project addresses one of the five societal challenges suggested by the Chief Scientist Ian Chubb and endorsed by the Australian Government in June 2013 - \"living in a changing environment\". As a result, new sustainable and recyclable materials to be used as liquid electrolytes in renewable energies devices such as metal-ion batteries, solar and fuel cells need to be designed to allow us to have a fully sustainable future in Australia. The outcomes of our project will enable scientists to design novel electrolytes using quantum chemical methods, i.e. from first principles. In order to be cost-effective, these new electrolytes should be mixtures of existing organic solvents and ionic liquids. One of the goals of the project is to establish what combinations of ionic liquids and organic solvents will result in sustainable liquid electrolytes with enhanced transport properties and efficient performance in renewable energy devices. Hazardous solvents used to manufacture a vast number of polymers will need to be phased out in the future. Ionic liquids are considered the best environmentally friendly replacement  of organic solvents, thus making them the ideal medium for industrially important processes such as radical polymerisation. The project also aims to predict kinetic rates of radical polymerisation in ionic liquids with the view of designing task-specific ionic liquids for polymerisation reactions. The project is directly supported by two ARC DP grants, DP1095058 and DP140100036 (total funding of $670K), and an ARC Future Fellowship for the lead CI (total funding of $580K).  The project will perform the following calculations: 1) NMR calculations of ionic liquid clusters 2) NMR and IR calculations of metal ions complexed with ionic liquid ions and organic solvents 3) MP2 geometry optimisations of small radicals interacting with ionic liquid ions 4) IR frequency calculations of ionised nucleic bases Software used: GAUSSIAN09 and GAMESS-US"
                                }
                            ],
                            "name": "030701"
                        }
                    ],
                    "name": "0307"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "monash.edu",
                                    "name": "Synchrotron Monash Storage Service",
                                    "usagePatterns": "The 24/7 access to synchrotron data for browsing and download over HTTP. Users would be particularly interested in downloading data within the first month of collection, and the wider public later on via the same service.",
                                    "useCase": "UPDATE #2: There are several datasets that will be made openly accessible and discoverable in the near future. The data for these is seen as a strong candidate for RDSI merit allocation later on, and we'd like to transition this public data now from the Monash LaRDS store to a NeCTAR volume in preparation, and to demonstrate the mechanism for storing open data once RDSI storage is available. We're requesting a new, separate volume of 1TB for this tenancy. UPDATE: In addition to using LaRDS for raw data, the service desires volume storage from NeCTAR. 1TB of fast storage would be used to serve preview images in the web view. Fast image serving is crucial to a web site thats fast and responsive. This storage should live at Monash university for close proximity to the compute nodes, and the LaRDS store. *** The Australian Synchrotron (AS) doesnt guarantee the storage and availability users data 3 months after it has been collected. This is problematic when considering the longevity of initial data potentially associated with future scientific discovery when analytical techniques improve. A large quantity of users at the AS access their data via MyTardis (which the NeCTAR-Funded Bioscience Data Platform is built upon). Monash University has offered to take responsibility for the storage and access of all data on the ASs Macromolecular beamlines (MX1 / MX2) regardless of the institution of the user collecting the data. The Monash LaRDS multi-petabyte store will be used to host the raw data. Monash University are committed to operating and supporting the service (named the Synchrotron Monash Storage Service, store.synchrotron.org.au) in partnership with the AS until at least 31 Dec 2014. Initially, the service is expected to capture 10TB/month of data for sharing and serving. This service needs to provide reliable data access to users of the 24h facility and their collaborators. Such a service requires compute nodes to form a redundant, scalable architecture. Compute nodes are used to verify integrity of stored data, compress and store for long-term tape archival and stream zip/tar archives to users. Such an allocation from NeCTAR will help to ensure almost 100% uptime and support for potentially thousands of simultaneous users at a time."
                                },
                                {
                                    "coreQuota": 0.3,
                                    "instanceQuota": 1.2,
                                    "institution": "unimelb.edu.au",
                                    "name": "UniCarb_NeCTAR_Support",
                                    "usagePatterns": "One setup for test purposes.",
                                    "useCase": "Support for UniCarbKB project"
                                }
                            ],
                            "name": "030403"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 200.0,
                                    "instanceQuota": 200.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQ_ATB",
                                    "usagePatterns": "The main database is managed by a set of core machines which handle job scheduling and user access. These spawn compute jobs as required. These compute jobs are independent, largely asynchronous and are not time critical. The initial quantum mechanical calculations required for each new molecule range from 10's of minutes to hundreds of CPU hours depending on the size of the system. Systems beyond a given threshold are directed to HPC systems. The validation jobs are in the order of 1-4 hours. Validation is, however, an iterative process with 20 to 50 jobs per molecule. A priority list of 700 molecules have been identified for initial validation. While the resources requested are considerable these will be primarily required in this expansion phase. The system is also designed to be robust and responsive expanding to fill any spare capacity. Equally jobs can be easily suspended if there are other demands on the system without affecting the database itself. The storage request is required during the validation phase.  The database itself is currently in the order of 10 Gbytes. ",
                                    "useCase": "The Automated Topology Builder and Repository is an ongoing project to generate and supply molecular interaction parameters for use in structure refinement and computational drug design. The site has approximately 1,200 registered users (growing by ~10 - 20 a week) and is accessed by several hundred researchers daily. The repository currently contains 4500 molecules.  When parameters for a molecule are requested that is not in the database as series of calculations are performed in order to generate the required parameters. These parameters are then tested and refined. The database is still in its development phase and is currently being expanded. Our aim is not only to host the site itself on the NECTAR system but to use the capacity of the Q-cloud to perform the initial generation of parameters of 12,000 commonly used molecules and validation of a proportion of these and exiting molecules in the database. Automated procedures for this have been developed. These involve dynamically creating and removing VMs as required and as resources are available. While expanding the database we fully scalable and have the capacity to use all available resources. We are working with QCIF and are well placed to ensure efficient utilization of resources in the testing and start-up phases of the Q-cloud with a high value project.          "
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "uq.edu.au",
                                    "name": "Automated Topology Builder Development",
                                    "usagePatterns": "Usage will be for testing and development as the pipeline is visualized. The load will be intermittent. ",
                                    "useCase": "Project Aim:  To migrate an automated topology builder http://compbio.biosci.uq.edu.au/atb/ on to the Queensland NeCTAR platform. Our system provides atomic interaction parameters for molecules that are used in simulations and drug design. Users submit the coordinates of the molecule in question, we then perform a series of back-end calculations and add the molecule to the database. The calculations vary depending on what is in the database and the nature of the molecule. In order to be able to handle this and a widely varying load we are re-organising our pipeline so that separate calculations can be farmed out onto different virtual machines that are generated as required and removed (or suspended) afterwards.  Currently the NeCTAR nodes in Brisbane do not have this capacity. To develop and test the application we need one VM to act as a head node and access to multiple dynamically allocated VM's with up to 4 cores each and about 1G mem per core (ideally 12-16 cores in total).  This project is funded in part by NeCTAR via the Queensland Cyber Infrastructure Foundation and UQ"
                                }
                            ],
                            "name": "030402"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 20.0,
                                    "institution": "csiro.au",
                                    "name": "NeCTAR Image Processing and Analysis Toolkit",
                                    "usagePatterns": "",
                                    "useCase": "This is the storage for public installation of the image processing toolkit (CISIRO-CIAP) using Galaxy and Cloudman. For the time being we have sufficient allocation for compute, but we also need a  block storage for persistent storage of datasets."
                                },
                                {
                                    "coreQuota": 0.8,
                                    "instanceQuota": 0.8,
                                    "institution": "adelaide.edu.au",
                                    "name": "XAS",
                                    "usagePatterns": "Data with be accessible by many users and data sets are predicted to be relatively small ( < 1GB) However, data packs and measurements are expected to be numerous (~30-40 ) on each case.  ",
                                    "useCase": "XAS analysis using mainly the EXAFSPAK suite or anything else that might be proved more sufficient during the study  "
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "monash.edu",
                                    "name": "Monash_WilceLab-vec_hsqc",
                                    "usagePatterns": "These will be used for data warehousing, and in addition the accumulation of processed and testing data.",
                                    "useCase": "I am a Post-Doctoral Researcher with the Wilce group in the Department of Biochemistry.  This group is focused on protein structure and function, primarily using X-ray crystallography and SAXS data.  In addition to my work within the Wilce group I have a continuing collaboration with the Scanlon group at the Department of Medicinal Chemistry (Monash Parkville Campus).  Our shared interest is in the rapid assignment of two dimensional Nuclear Magnetic Resonance (NMR) data.  Such data is routinely used at multiple stages in Drug Discovery, for the screening of ligands, estimation of ligand binding affinity and also to infer binding site information.  I am building a machine learning algorithm to automate the assignment of NMR data.  I seek access to cluster facilities in order to obtain a performance advantage for the underlying relational database and large-scale linear algebra and statistical operations I require.  Multiple processors and access to large memory reserves could enhance rapid performance testing and code optimistation.  In addition, access to 30 GB of disk storage would be advantageous for storing of raw data and test data for the various stages of code optimisation. The primary aim is for this work is to be the central theme of a publication in a peer-reviewed journal.  A secondary aim is to create a repository of manually assigned NMR data for further data mining / feature extraction, improved accuracy in the estimation of statistical parameters, and tuning the predictive algorithm for protein subclasses.  In addition, the work will have industry relevance for commercial drug discovery.  "
                                }
                            ],
                            "name": "0304"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "monash.edu",
                                    "name": "Synchrotron Monash Storage Service",
                                    "usagePatterns": "The 24/7 access to synchrotron data for browsing and download over HTTP. Users would be particularly interested in downloading data within the first month of collection, and the wider public later on via the same service.",
                                    "useCase": "UPDATE #2: There are several datasets that will be made openly accessible and discoverable in the near future. The data for these is seen as a strong candidate for RDSI merit allocation later on, and we'd like to transition this public data now from the Monash LaRDS store to a NeCTAR volume in preparation, and to demonstrate the mechanism for storing open data once RDSI storage is available. We're requesting a new, separate volume of 1TB for this tenancy. UPDATE: In addition to using LaRDS for raw data, the service desires volume storage from NeCTAR. 1TB of fast storage would be used to serve preview images in the web view. Fast image serving is crucial to a web site thats fast and responsive. This storage should live at Monash university for close proximity to the compute nodes, and the LaRDS store. *** The Australian Synchrotron (AS) doesnt guarantee the storage and availability users data 3 months after it has been collected. This is problematic when considering the longevity of initial data potentially associated with future scientific discovery when analytical techniques improve. A large quantity of users at the AS access their data via MyTardis (which the NeCTAR-Funded Bioscience Data Platform is built upon).  Monash University has offered to take responsibility for the storage and access of all data on the ASs Macromolecular beamlines (MX1 / MX2) regardless of the institution of the user collecting the data. The Monash LaRDS multi-petabyte store will be used to host the raw data. Monash University are committed to operating and supporting the service (named the Synchrotron Monash Storage Service, store.synchrotron.org.au) in partnership with the AS until at least 31 Dec 2014. Initially, the service is expected to capture 10TB/month of data for sharing and serving. This service needs to provide reliable data access to users of the 24h facility and their collaborators. Such a service requires compute nodes to form a redundant, scalable architecture. Compute nodes are used to verify integrity of stored data, compress and store for long-term tape archival and stream zip/tar archives to users. Such an allocation from NeCTAR will help to ensure almost 100% uptime and support for potentially thousands of simultaneous users at a time."
                                }
                            ],
                            "name": "030405"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 150.0,
                                    "instanceQuota": 150.0,
                                    "institution": "uq.edu.au",
                                    "name": "UQ_ATB",
                                    "usagePatterns": "The main database is managed by a set of core machines which handle job scheduling and user access. These spawn compute jobs as required. These compute jobs are independent, largely asynchronous and are not time critical. The initial quantum mechanical calculations required for each new molecule range from 10's of minutes to hundreds of CPU hours depending on the size of the system. Systems beyond a given threshold are directed to HPC systems. The validation jobs are in the order of 1-4 hours. Validation is, however, an iterative process with 20 to 50 jobs per molecule. A priority list of 700 molecules have been identified for initial validation. While the resources requested are considerable these will be primarily required in this expansion phase. The system is also designed to be robust and responsive expanding to fill any spare capacity. Equally jobs can be easily suspended if there are other demands on the system without affecting the database itself. The storage request is required during the validation phase.  The database itself is currently in the order of 10 Gbytes. ",
                                    "useCase": "The Automated Topology Builder and Repository is an ongoing project to generate and supply molecular interaction parameters for use in structure refinement and computational drug design. The site has approximately 1,200 registered users (growing by ~10 - 20 a week) and is accessed by several hundred researchers daily. The repository currently contains 4500 molecules.  When parameters for a molecule are requested that is not in the database as series of calculations are performed in order to generate the required parameters. These parameters are then tested and refined. The database is still in its development phase and is currently being expanded. Our aim is not only to host the site itself on the NECTAR system but to use the capacity of the Q-cloud to perform the initial generation of parameters of 12,000 commonly used molecules and validation of a proportion of these and exiting molecules in the database. Automated procedures for this have been developed. These involve dynamically creating and removing VMs as required and as resources are available. While expanding the database we fully scalable and have the capacity to use all available resources. We are working with QCIF and are well placed to ensure efficient utilization of resources in the testing and start-up phases of the Q-cloud with a high value project.          "
                                }
                            ],
                            "name": "030401"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 9.6,
                                    "instanceQuota": 9.6,
                                    "institution": "monash.edu",
                                    "name": "Synchrotron Monash Storage Service",
                                    "usagePatterns": "The 24/7 access to synchrotron data for browsing and download over HTTP. Users would be particularly interested in downloading data within the first month of collection, and the wider public later on via the same service.",
                                    "useCase": "UPDATE #2: There are several datasets that will be made openly accessible and discoverable in the near future. The data for these is seen as a strong candidate for RDSI merit allocation later on, and we'd like to transition this public data now from the Monash LaRDS store to a NeCTAR volume in preparation, and to demonstrate the mechanism for storing open data once RDSI storage is available. We're requesting a new, separate volume of 1TB for this tenancy. UPDATE: In addition to using LaRDS for raw data, the service desires volume storage from NeCTAR. 1TB of fast storage would be used to serve preview images in the web view. Fast image serving is crucial to a web site thats fast and responsive. This storage should live at Monash university for close proximity to the compute nodes, and the LaRDS store. *** The Australian Synchrotron (AS) doesnt guarantee the storage and availability users data 3 months after it has been collected. This is problematic when considering the longevity of initial data potentially associated with future scientific discovery when analytical techniques improve. A large quantity of users at the AS access their data via MyTardis (which the NeCTAR-Funded Bioscience Data Platform is built upon).  Monash University has offered to take responsibility for the storage and access of all data on the ASs Macromolecular beamlines (MX1 / MX2) regardless of the institution of the user collecting the data. The Monash LaRDS multi-petabyte store will be used to host the raw data. Monash University are committed to operating and supporting the service (named the Synchrotron Monash Storage Service, store.synchrotron.org.au) in partnership with the AS until at least 31 Dec 2014. Initially, the service is expected to capture 10TB/month of data for sharing and serving. This service needs to provide reliable data access to users of the 24h facility and their collaborators. Such a service requires compute nodes to form a redundant, scalable architecture. Compute nodes are used to verify integrity of stored data, compress and store for long-term tape archival and stream zip/tar archives to users. Such an allocation from NeCTAR will help to ensure almost 100% uptime and support for potentially thousands of simultaneous users at a time."
                                }
                            ],
                            "name": "030406"
                        }
                    ],
                    "name": "0304"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "EPR and its Application to Structural Chemistry and Biology",
                                    "usagePatterns": "Queensland: Concurrent users: 10-12 CAI, SCMB, SMP, UQ and QUT Potential users include scientists at ANU, USyd, UWS, UoM and Heidelberg University where we have joint PhD students At present the data is small (kB to Mb). Once iResonanz is in production mode, we will have a database, which we expect to grow to TB. We also expect the database to be replicated around the world.",
                                    "useCase": "In conjunction with computational chemistry (ORCA), the analysis of continuous wave and pulsed EPR, ENDOR and ELDOR spectra is central to the determination of the geometric and electronic structure of paramagnetic molecules found in solid state materials, chemical, biological and biomedical systems.  We will utilize the XSophe-Sophe-XeprView, Molecular Sophe and iResonanz computer simulation software suites (developed within my group) for the analysis of continuous wave and pulsed EPR, ENDOR and ELDOR spectra. "
                                },
                                {
                                    "coreQuota": 60.0,
                                    "instanceQuota": 60.0,
                                    "institution": "monash.edu",
                                    "name": "Characterisation VL Development and Testing",
                                    "usagePatterns": "Mixed interactive and scheduled usage.",
                                    "useCase": "This will be a first development environment for the Characterisation VL (CVL). The numbers I gave are a first estimate. Happy to refine this as we learn more about the system and develop our requirements. I may put in further requests on behalf of users of the CVL. "
                                },
                                {
                                    "coreQuota": 28.8,
                                    "instanceQuota": 28.8,
                                    "institution": "monash.edu",
                                    "name": "CharacterisationVL - [Additional resource]",
                                    "usagePatterns": "Large data sets",
                                    "useCase": "Characterisation VM"
                                }
                            ],
                            "name": "030201"
                        }
                    ],
                    "name": "0302"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 10.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_Metal_Cluster_Catalysis ",
                                    "usagePatterns": "Gaussian09 calculations jobs have high transient/scratch data use but small (<200mb) persistent data use once the job is finished running. Four (maybe five) users but with very little or no overlap between data usage.",
                                    "useCase": "Recently, researchers have shown that the variable reactivity of nano-sized metal particles can be exploited to enhance catalytic activity of metallic systems. Metal clusters deposited onto surfaces, containing as few as several atoms, have been shown to induce catalysed activity at significantly lower temperatures compared with bulk metallic surfaces. The study and understanding of the underlying principles of these effects will provide a revolutionary methodology for developing next generation ultra-efficient and cheaper catalysts. Our research involves the theoretical study of the chemical and physical properties of metal clusters, their interactions with surfaces, and their chemical activity towards important molecules such as CO2 and H2O.  "
                                }
                            ],
                            "name": "030601"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "uq.edu.au",
                                    "name": "EPR and its Application to Structural Chemistry and Biology",
                                    "usagePatterns": "Queensland: Concurrent users: 10-12 CAI, SCMB, SMP, UQ and QUT Potential users include scientists at ANU, USyd, UWS, UoM and Heidelberg University where we have joint PhD students At present the data is small (kB to Mb). Once iResonanz is in production mode, we will have a database, which we expect to grow to TB. We also expect the database to be replicated around the world.",
                                    "useCase": "In conjunction with computational chemistry (ORCA), the analysis of continuous wave and pulsed EPR, ENDOR and ELDOR spectra is central to the determination of the geometric and electronic structure of paramagnetic molecules found in solid state materials, chemical, biological and biomedical systems.  We will utilize the XSophe-Sophe-XeprView, Molecular Sophe and iResonanz computer simulation software suites (developed within my group) for the analysis of continuous wave and pulsed EPR, ENDOR and ELDOR spectra. "
                                },
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 10.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "Adelaide_Metal_Cluster_Catalysis ",
                                    "usagePatterns": "Gaussian09 calculations jobs have high transient/scratch data use but small (<200mb) persistent data use once the job is finished running. Four (maybe five) users but with very little or no overlap between data usage.",
                                    "useCase": "Recently, researchers have shown that the variable reactivity of nano-sized metal particles can be exploited to enhance catalytic activity of metallic systems. Metal clusters deposited onto surfaces, containing as few as several atoms, have been shown to induce catalysed activity at significantly lower temperatures compared with bulk metallic surfaces. The study and understanding of the underlying principles of these effects will provide a revolutionary methodology for developing next generation ultra-efficient and cheaper catalysts. Our research involves the theoretical study of the chemical and physical properties of metal clusters, their interactions with surfaces, and their chemical activity towards important molecules such as CO2 and H2O.  "
                                },
                                {
                                    "coreQuota": 0.2,
                                    "instanceQuota": 0.8,
                                    "institution": "unimelb.edu.au",
                                    "name": "UniCarb_NeCTAR_Support",
                                    "usagePatterns": "One setup for test purposes.",
                                    "useCase": "Support for UniCarbKB project"
                                }
                            ],
                            "name": "030606"
                        }
                    ],
                    "name": "0306"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 2.0,
                                    "institution": "uq.edu.au",
                                    "name": "volume Calc",
                                    "usagePatterns": "1 user and small data sets.",
                                    "useCase": "I plan to use this to perform void volume calculations on large metallo-supramolecular assemblies using the voidoo-suite of software."
                                },
                                {
                                    "coreQuota": 2.0,
                                    "instanceQuota": 1.0,
                                    "institution": "uq.edu.au",
                                    "name": "volume Calc",
                                    "usagePatterns": "1 user and small data sets.",
                                    "useCase": "I plan to use this to perform void volume calculations on large metallo-supramolecular assemblies using the voidoo-suite of software."
                                }
                            ],
                            "name": "030302"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 48.0,
                                    "instanceQuota": 6.0,
                                    "institution": "monash.edu",
                                    "name": "Ab initio calculations of physical properties of ionic and polymer materials",
                                    "usagePatterns": "The project will have large data sets with a small number of users (at the moment 3 users).",
                                    "useCase": "The main goal of the project is to establish reliable computational methodologies for the accurate prediction of physical properties of ionic and polymer materials, including liquid electrolytes and reaction rates of radical polymerisation in ionic liquids. The project addresses one of the five societal challenges suggested by the Chief Scientist Ian Chubb and endorsed by the Australian Government in June 2013 - \"living in a changing environment\". As a result, new sustainable and recyclable materials to be used as liquid electrolytes in renewable energies devices such as metal-ion batteries, solar and fuel cells need to be designed to allow us to have a fully sustainable future in Australia. The outcomes of our project will enable scientists to design novel electrolytes using quantum chemical methods, i.e. from first principles. In order to be cost-effective, these new electrolytes should be mixtures of existing organic solvents and ionic liquids. One of the goals of the project is to establish what combinations of ionic liquids and organic solvents will result in sustainable liquid electrolytes with enhanced transport properties and efficient performance in renewable energy devices. Hazardous solvents used to manufacture a vast number of polymers will need to be phased out in the future. Ionic liquids are considered the best environmentally friendly replacement  of organic solvents, thus making them the ideal medium for industrially important processes such as radical polymerisation. The project also aims to predict kinetic rates of radical polymerisation in ionic liquids with the view of designing task-specific ionic liquids for polymerisation reactions. The project is directly supported by two ARC DP grants, DP1095058 and DP140100036 (total funding of $670K), and an ARC Future Fellowship for the lead CI (total funding of $580K).  The project will perform the following calculations: 1) NMR calculations of ionic liquid clusters 2) NMR and IR calculations of metal ions complexed with ionic liquid ions and organic solvents 3) MP2 geometry optimisations of small radicals interacting with ionic liquid ions 4) IR frequency calculations of ionised nucleic bases Software used: GAUSSIAN09 and GAMESS-US"
                                }
                            ],
                            "name": "030304"
                        }
                    ],
                    "name": "0303"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 12.8,
                                    "institution": "monash.edu",
                                    "name": "Monash_Water_Quality-Gippsland_Lakes",
                                    "usagePatterns": "It will only have a couples of users for this project at the moment. The number of files can get up to a few hundreds. The sizes of the individual files can range from a few KB to 20GB.",
                                    "useCase": "I am working on a water quality modelling project at the Gippsland Lakes. The research project looks into the  dynamics of the toxic cyanobacterial blooms in the lakes. The software we use is called  MIKE developed by DHI (http://www.dhisoftware.com/). The model requires a lot computing power. As approaching towards the end this study, we will need to do many simulations for model calibration and scenario explorations in the next 10 months. Unfortunately, the software I am using can  only run on windows platform and we just have a 6-core windows machine in our lab which is far from being capable to handles all the computations. Therefore we hope to get access to more computing power to overcome this constraints."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. Most of the resource (the extra we are requesting) will be for a trial deployment of a dynamic torque cluster on the SA node to augment shared HPC (cloudbursting). "
                                },
                                {
                                    "coreQuota": 12.8,
                                    "instanceQuota": 0.4,
                                    "institution": "monash.edu",
                                    "name": "Monash_Water_Quality-Gippsland_Lakes",
                                    "usagePatterns": "It will only have a couples of users for this project at the moment. The number of files can get up to a few hundreds. The sizes of the individual files can range from a few KB to 20GB.",
                                    "useCase": "I am working on a water quality modelling project at the Gippsland Lakes. The research project looks into the  dynamics of the toxic cyanobacterial blooms in the lakes. The software we use is called  MIKE developed by DHI (http://www.dhisoftware.com/). The model requires a lot computing power. As approaching towards the end this study, we will need to do many simulations for model calibration and scenario explorations in the next 10 months. Unfortunately, the software I am using can  only run on windows platform and we just have a 6-core windows machine in our lab which is far from being capable to handles all the computations. Therefore we hope to get access to more computing power to overcome this constraints."
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 2.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. We need some group resource to do this until the SA cloud node is in production. "
                                },
                                {
                                    "coreQuota": 16.0,
                                    "instanceQuota": 16.0,
                                    "institution": "adelaide.edu.au",
                                    "name": "eRSA Cloud Applications",
                                    "usagePatterns": "A modest number of users with mostly larger files, but some small files also",
                                    "useCase": "We are working with multiple user groups on trying out a variety of cloud applications as part of the SA Node project for Migrating Apps to the Cloud. This includes cluster in the cloud, developing different images for different application areas, web database applications, etc. We need some group resource to do this until the SA cloud node is in production. "
                                },
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 3.0,
                                    "institution": "griffith.edu.au",
                                    "name": "HPC Workflows",
                                    "usagePatterns": "It's a HPC workflows Project so it's more likely to have large data sets, and at least initially, a small number of users.",
                                    "useCase": "We (eResearch Services, Griffith Uni) are currently developing a user-friendly HPC job submission web portal for researchers. Currently we have NAMD (and shortly MatLab) jobs running from a web portal through NIMROD to our local cluster. Nimrod is able to use cloud bursting and we would like to have more than 2 CPU's available to test and improve this workflow."
                                },
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "adelaide.edu.au",
                                    "name": "XAS",
                                    "usagePatterns": "Data with be accessible by many users and data sets are predicted to be relatively small ( < 1GB) However, data packs and measurements are expected to be numerous (~30-40 ) on each case.  ",
                                    "useCase": "XAS analysis using mainly the EXAFSPAK suite or anything else that might be proved more sufficient during the study  "
                                }
                            ],
                            "name": "03"
                        }
                    ],
                    "name": "03"
                }
            ],
            "name": "03"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 96.0,
                                    "instanceQuota": 96.0,
                                    "institution": "intersect.org.au",
                                    "name": "HCSvLab",
                                    "usagePatterns": "The project will have a few users with large datasets. This usage pattern is relevant now, during development, but it will likely change once the virtual laboratory goes into full production. The INDRI cluster we want to setup will potentially consist of 10-15 simultaneously running instances. We need additional instances for running other tools.",
                                    "useCase": "This is for testing and development of the HCSvLab NeCTAR Virtual Laboratory Project. We want to setup instances to test tools (e.g. DeMoLib, HTK and INDRI) that will be run over data sourced from the virtual laboratory. One particular use case will be to setup a cluster for indexing very large datasets using INDRI. The dataset is called ClueWeb09 and is 25TB (http://lemurproject.org/clueweb09/). The clustering will potentially be done using Hadoop. "
                                },
                                {
                                    "coreQuota": 96.0,
                                    "instanceQuota": 6.0,
                                    "institution": "intersect.org.au",
                                    "name": "HCSvLab",
                                    "usagePatterns": "The project will have a few users with large datasets. This usage pattern is relevant now, during development, but it will likely change once the virtual laboratory goes into full production. The INDRI cluster we want to setup will potentially consist of 10-15 simultaneously running instances. We need additional instances for running other tools.",
                                    "useCase": "This is for testing and development of the HCSvLab NeCTAR Virtual Laboratory Project. We want to setup instances to test tools (e.g. DeMoLib, HTK and INDRI) that will be run over data sourced from the virtual laboratory. One particular use case will be to setup a cluster for indexing very large datasets using INDRI. The dataset is called ClueWeb09 and is 25TB (http://lemurproject.org/clueweb09/). The clustering will potentially be done using Hadoop. "
                                }
                            ],
                            "name": "1702"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 8.0,
                                    "institution": "griffith.edu.au",
                                    "name": "ANN Prototype",
                                    "usagePatterns": "Only a small number of users.  Reasonable data sets but cleared out frequently.  Many temporary files.",
                                    "useCase": "Using Artificial Neural Network to supplement a Multi-Objective Particle Swarm Optimisation on a lift/drag optimisation of lift/drag on a free form deformed airfoil as a proof of concept for a Jet Turbine Optimisation case. "
                                },
                                {
                                    "coreQuota": 8.0,
                                    "instanceQuota": 1.0,
                                    "institution": "griffith.edu.au",
                                    "name": "ANN Prototype",
                                    "usagePatterns": "Only a small number of users.  Reasonable data sets but cleared out frequently.  Many temporary files.",
                                    "useCase": "Using Artificial Neural Network to supplement a Multi-Objective Particle Swarm Optimisation on a lift/drag optimisation of lift/drag on a free form deformed airfoil as a proof of concept for a Jet Turbine Optimisation case. "
                                }
                            ],
                            "name": "170205"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "monash.edu",
                                    "name": "Monash_EFTPOS_mining1",
                                    "usagePatterns": "2-3 users running single multi-core VMs for development and testing",
                                    "useCase": "The project involves running data mining algorithms to find meaningful patterns from historical EFTPOS transaction data.  The data has been categorised commercial in confidence.  The research cloud is needed to satisfy the data security requirements outlined by the data owner, i.e.  a sandboxed environment which keeps other users (e.g., on a shared system) from accessing the data and which will allow the work to scale using distributed computing tools and methods (e.g. Hadoop)"
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "monash.edu",
                                    "name": "Monash_EFTPOS_mining1",
                                    "usagePatterns": "2-3 users running single multi-core VMs for development and testing",
                                    "useCase": "The project involves running data mining algorithms to find meaningful patterns from historical EFTPOS transaction data.  The data has been categorised commercial in confidence.  The research cloud is needed to satisfy the data security requirements outlined by the data owner, i.e.  a sandboxed environment which keeps other users (e.g., on a shared system) from accessing the data and which will allow the work to scale using distributed computing tools and methods (e.g. Hadoop)"
                                }
                            ],
                            "name": "170203"
                        }
                    ],
                    "name": "1702"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 3.0,
                                    "instanceQuota": 1.5,
                                    "institution": "anu.edu.au",
                                    "name": "ANU_Pro-ana_data_service",
                                    "usagePatterns": "The intention is to have 3 instances, each running 2 cores and 2 persistent volumes.  2 instances (preferably geographically separated) will run replicas of mongodb (each using one persistent volume). This will allow system maintenance/failure without interrupting data collection. The third instance will run a data collection script.",
                                    "useCase": "\"pro-ana\" (pro-anorexia) is a recent phenomena on various online social media platforms. We intend to collect data from the Twitter pro-ana community (possibly expanding to other media outlets). This application will provide 2 replicas of a mongodb and one instance for polling the Twitter api 24/7 and storing the data in mongo. Object storage will be used for database backups."
                                }
                            ],
                            "name": "170199"
                        }
                    ],
                    "name": "1701"
                }
            ],
            "name": "17"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 2.4,
                                    "instanceQuota": 2.4,
                                    "institution": "murdoch.edu.au",
                                    "name": "Murdoch_ACCWI",
                                    "usagePatterns": "This server will not store the main data sets of the centre, but may be a temporary storage point as we prepare datasets for submission to larger repositories.",
                                    "useCase": "This server will be used by members of the Australia-China Centre for Wheat Improvement (ACCWI) at Murdoch University for general purpose tasks., such as writing and testing code. It will function as the main server \"hub\" for the centre (which does not have any physical compute infrastructure of its own). Small analysis tasks may also be run by some users. It will also be used as the base for file exchange with collaborators."
                                },
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 0.5,
                                    "institution": "uq.edu.au",
                                    "name": "Crop Trait Mining Informatics Platform",
                                    "usagePatterns": "Initially a pilot project so will begin with 10s of users and moderate sized data sets (by plant improvement and genetic resources standards). Expected to gradually grow and either become the primary instance or a mirror of the primary instance of a global information system to facilitate the access to and use of germplasm in plant improvement.",
                                    "useCase": "Pilot phase will be a single instance for development and data aggregation purposes within Australia. It will involve installing Genesys I and uploading data from several GRDC funded research projects. Classes of data stored include plant genetic resources passport information together with associated characterization, phenotypic and environmental data. A GRDC funded use case to identify spring radiation frost tolerance in wheat will validate the value of a collaborative, global approach to sharing, storing and making these data classes publically available to facilitate their use in addressing food security/productivity, climate change and associated challenges. The project is anticipated to evolve into an integrated, easily accessible research data resource, together with tools to facilitate the identification and use of plant germplasm, and be part of a wider global system for plant improvement."
                                }
                            ],
                            "name": "070305"
                        }
                    ],
                    "name": "0703"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 25.6,
                                    "instanceQuota": 1.6,
                                    "institution": "uq.edu.au",
                                    "name": "Advanced methods for genomic analysis in animal breeding ",
                                    "usagePatterns": "It is expected that the project will include the analysis of a  small number of large data sets. These data sets will be from 1-5 GB although intermediate analysis may result in much larger temporary files. It is envisaged that each of these analysis will utilise the resource for between 4 hours and a few day. There will be a small number of users (<5).",
                                    "useCase": "Genomics has revolutionised many aspects of livestock breeding. The analysis of such data often requires Bayesian analysis which runs for long periods or analysis of large datasets that require large computing capacity.  Access to this resource will enable us to more effectively analyse such data, to develop new analysis methods and also to test existing methods and provide results to animal breeders."
                                }
                            ],
                            "name": "070201"
                        }
                    ],
                    "name": "0702"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.2,
                                    "institution": "murdoch.edu.au",
                                    "name": "Murdoch_Agri_Bio_Data_Integration",
                                    "usagePatterns": "This is a low level computing project will require the upload/process large datasets (500Mb per data source). There will be extensive testing (of random subsets of a data source) to optimise which (and how many data-sources) to be used before large process will occur.",
                                    "useCase": "As more datasets become available, researchers need to be smarter in how they integrate this information into a structure that will enable the evaluation of how cereal (wheat/barley) plants respond to changing environmental conditions to meet international grain markets. This research activity will utilise research in data ontology standards and statistical/bioinformatic tools to research data-pipelines for data mining of agricultural data."
                                }
                            ],
                            "name": "070103"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 1.6,
                                    "instanceQuota": 0.8,
                                    "institution": "usq.edu.au",
                                    "name": "USQ eResearch Services Sandbox",
                                    "usagePatterns": "Many users and small data sets as well as small number of users and large data sets. This will vary depending on the tests and the resesearch group.",
                                    "useCase": "The cloud instances will be used to set up quick demos for researchers at USQ to run test experiments, simulations, modelling and calculations. The cloud instances will also be used to bench test the service from USQ."
                                }
                            ],
                            "name": "070104"
                        }
                    ],
                    "name": "0701"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 0.6,
                                    "instanceQuota": 0.6,
                                    "institution": "utas.edu.au",
                                    "name": "NERP Web Fire Mapping",
                                    "usagePatterns": "A small number of users with moderate-sized data sets - initially one user managing a suite of spatial data, but more users may be added if necessary.  After initial map rendering, CPU requirements will be low.",
                                    "useCase": "Provide spatial data storage, map tile rendering and web mapping service serving for the National Environmental Research Program (Landscapes and Policy Hub)."
                                }
                            ],
                            "name": "070503"
                        }
                    ],
                    "name": "0705"
                }
            ],
            "name": "07"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.0,
                                    "instanceQuota": 2.0,
                                    "institution": "larosa.org.au",
                                    "name": "UoM Library",
                                    "usagePatterns": "",
                                    "useCase": "OJS and eprints service we run on behalf of the University Library"
                                }
                            ],
                            "name": "220207"
                        }
                    ],
                    "name": "2202"
                }
            ],
            "name": "22"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 1.8,
                                    "institution": "vu.edu.au",
                                    "name": "VU Phoenix Program",
                                    "usagePatterns": "Small number of users expected, essentially web and web service driven execution of processing.",
                                    "useCase": "The generic and core technologies of the project come from the Phoenix Program with the following application domains: 1) tourism and 2) supply chain and logistics.   We currently hold a category one research grant under Australia China Council in the domain of ICT application for tourism, and also holding an online real-time demonstration in the domain of supply chain and logistics through a research prototype (that is currently being upgraded in terms of concurrent processing and scaling up).   Applications in the above domains are to occur through cloud, a strategic direction for the project. Since we have industry partners of IBM and TIBCO who offered us commercial grade of products, some of those are to be used by the project and likely hosted on cloud, so we would like to apply reasonable resources that are adequate to support a private cloud scenario (e.g. hosting IBM Smart Cloud Technology, a private cloud environment). Depending on what is possible and available, I tentatively propose the specified resource requirements. -- Wei Dai, College of Business, Centre for Strategic and Economic Syudies. "
                                }
                            ],
                            "name": "1506"
                        }
                    ],
                    "name": "1506"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 4.8,
                                    "instanceQuota": 1.8,
                                    "institution": "vu.edu.au",
                                    "name": "VU Phoenix Program",
                                    "usagePatterns": "Small number of users expected, essentially web and web service driven execution of processing.",
                                    "useCase": "The generic and core technologies of the project come from the Phoenix Program with the following application domains: 1) tourism and 2) supply chain and logistics.   We currently hold a category one research grant under Australia China Council in the domain of ICT application for tourism, and also holding an online real-time demonstration in the domain of supply chain and logistics through a research prototype (that is currently being upgraded in terms of concurrent processing and scaling up).   Applications in the above domains are to occur through cloud, a strategic direction for the project. Since we have industry partners of IBM and TIBCO who offered us commercial grade of products, some of those are to be used by the project and likely hosted on cloud, so we would like to apply reasonable resources that are adequate to support a private cloud scenario (e.g. hosting IBM Smart Cloud Technology, a private cloud environment). Depending on what is possible and available, I tentatively propose the specified resource requirements. -- Wei Dai, College of Business, Centre for Strategic and Economic Syudies. "
                                }
                            ],
                            "name": "1507"
                        }
                    ],
                    "name": "1507"
                },
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "monash.edu",
                                    "name": "Monash_EFTPOS_mining1",
                                    "usagePatterns": "2-3 users running single multi-core VMs for development and testing",
                                    "useCase": "The project involves running data mining algorithms to find meaningful patterns from historical EFTPOS transaction data.  The data has been categorised commercial in confidence.  The research cloud is needed to satisfy the data security requirements outlined by the data owner, i.e.  a sandboxed environment which keeps other users (e.g., on a shared system) from accessing the data and which will allow the work to scale using distributed computing tools and methods (e.g. Hadoop)"
                                },
                                {
                                    "coreQuota": 6.4,
                                    "instanceQuota": 6.4,
                                    "institution": "monash.edu",
                                    "name": "Monash_EFTPOS_mining1",
                                    "usagePatterns": "2-3 users running single multi-core VMs for development and testing",
                                    "useCase": "The project involves running data mining algorithms to find meaningful patterns from historical EFTPOS transaction data.  The data has been categorised commercial in confidence.  The research cloud is needed to satisfy the data security requirements outlined by the data owner, i.e.  a sandboxed environment which keeps other users (e.g., on a shared system) from accessing the data and which will allow the work to scale using distributed computing tools and methods (e.g. Hadoop)"
                                }
                            ],
                            "name": "150301"
                        }
                    ],
                    "name": "1503"
                }
            ],
            "name": "15"
        },
        {
            "children": [
                {
                    "children": [
                        {
                            "children": [
                                {
                                    "coreQuota": 20.0,
                                    "instanceQuota": 5.0,
                                    "institution": "monash.edu",
                                    "name": "ClimateWorks_CART",
                                    "usagePatterns": "Expected users 100 per deployed instance. Database expected to grow at a rate of 1 GB per year, per deployed instance Object Storage expected to grow at a rate of 1 GB per year, per deployed instance.",
                                    "useCase": "ClimateWorks Australia is a non-profit collaboration hosted by Monash University in partnership with The Myer Foundation that aims to facilitate substantial reductions in Australia's greenhouse gas emissions over the next five years. CART, the Climate Abatement Research Tool, is used by Climate Works Australia to develop Low Carbon Growth Plans (LCGPs). It provides a scalable web-based data management platform to support data collection, data storage (warehouse), data processing (i.e. cost curve development) and to progress tracking efforts (reporting).  A number of groups (UN, Monash Finance, Tohoku University (Japan)) have expressed interest in customizing CART for their use.  We need a dynamic environment where we can provision such requests. "
                                }
                            ],
                            "name": "1402"
                        },
                        {
                            "children": [
                                {
                                    "coreQuota": 120.0,
                                    "instanceQuota": 120.0,
                                    "institution": "monash.edu",
                                    "name": "Plexos-Nimrod",
                                    "usagePatterns": "",
                                    "useCase": "This research concerns the identification of optimal investment configurations for renewable energy resources in a large and complex power grid. It is tacked through analysis and simulation of electricity networks using a mixture of time-sequential simulations and global optimisation of the parameters of these simulations using Nimrod/O. The outcomes will provide a better understanding of how the topology of a power system affects the selection of technology types and the lowest cost investment solution. It can be used to inform policy makers on distributed generation market design and will form part of the research in the CSIRO's Future Grid flagship collaboration cluster: www.futuregrid.org.au"
                                },
                                {
                                    "coreQuota": 120.0,
                                    "instanceQuota": 120.0,
                                    "institution": "monash.edu",
                                    "name": "Plexos-Nimrod",
                                    "usagePatterns": "",
                                    "useCase": "This research concerns the identification of optimal investment configurations for renewable energy resources in a large and complex power grid. It is tacked through analysis and simulation of electricity networks using a mixture of time-sequential simulations and global optimisation of the parameters of these simulations using Nimrod/O. The outcomes will provide a better understanding of how the topology of a power system affects the selection of technology types and the lowest cost investment solution. It can be used to inform policy makers on distributed generation market design and will form part of the research in the CSIRO's Future Grid flagship collaboration cluster: www.futuregrid.org.au"
                                }
                            ],
                            "name": "140205"
                        }
                    ],
                    "name": "1402"
                }
            ],
            "name": "14"
        }
]
